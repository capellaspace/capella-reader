pyproject.toml
---
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[project]
name = "capella-reader"
authors = [{ name = "Scott Staniewicz", email = "scott.stanie@gmail.com" }]
description = "Basic models and reader for Capella SLCs"
readme = { file = "README.md", content-type = "text/markdown" }
requires-python = ">=3.10"

classifiers = [
  "Intended Audience :: Developers",
  "Intended Audience :: Science/Research",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Programming Language :: Python :: 3.14",
  "Programming Language :: Python",
  "Topic :: Scientific/Engineering",
  "Typing :: Typed",
]
license = { file = "LICENSE.txt" }

# The version will be written into a version.py upon install, auto-generated
# https://hatch.pypa.io/latest/version/#configuration
dynamic = ["version"]

dependencies = ["numpy", "pydantic", "tifffile"]

[project.optional-dependencies]
test = [
  "pre-commit",
  "pytest",
  "pytest-cov",
  "pytest-randomly",
  "ruff",
  "ipython",
]
docs = [
  "mkdocs",
  "mkdocs-gen-files",
  "mkdocs-jupyter",
  "mkdocs-literate-nav",
  "mkdocs-material",
  "pymdown-extensions",
  "matplotlib",
  "mkdocstrings[python]",
  "mkdocs-section-index",
]

[project.urls]
Homepage = "https://github.com/scottstanie/capella-reader"
"Bug Tracker" = "https://github.com/scottstanie/capella-reader/issues"
Discussions = "https://github.com/scottstanie/capella-reader/discussions"
Changelog = "https://github.com/scottstanie/capella-reader/releases"


[tool.pixi.workspace]
channels = ["conda-forge"]
platforms = ["osx-arm64", "linux-64"]

[tool.pixi.pypi-dependencies]
capella-reader = { path = ".", editable = true }

[tool.pixi.dependencies]
numpy = ">=2.3.5,<3"
pydantic = ">=2.8.0"
tifffile = ">=2025.5.0"

[tool.pixi.feature.isce3.dependencies]
isce3 = "*"

# pixi install -e test
[tool.pixi.feature.test.dependencies]
pre-commit = "*"
pytest = "*"
pytest-cov = "*"
pytest-randomly = "*"
ruff = "*"
ipython = "*"

[tool.pixi.feature.py311.dependencies]
python = "3.11.*"
[tool.pixi.feature.py313.dependencies]
python = "3.13.*"
[tool.pixi.feature.py314.dependencies]
python = "3.14.*"

# pixi install -e docs
[tool.pixi.feature.docs.dependencies]
mkdocs = "*"
mkdocs-gen-files = "*"
mkdocs-jupyter = "*"
mkdocs-literate-nav = "*"
mkdocs-material = "*"
pymdown-extensions = "*"
matplotlib = "*"

[tool.pixi.feature.docs.pypi-dependencies]
mkdocstrings = { extras = ["python"] }
mkdocs-section-index = "*"


[tool.pixi.environments]
test = ["test"]
docs = ["docs"]
# Combine the testing depdencies with python pins:
test-py311 = ["test", "py311", "isce3"]
test-py313 = ["test", "py313", "isce3"]
test-py314 = ["test", "py314"]          # isce3 fail for 3.14
all = ["test", "docs", "isce3"]

[tool.pixi.tasks]
check = { cmd = "pre-commit run -a", description = "Run pre-commit checks on all files" }
test = { cmd = "pytest  --cov=capella_reader tests/", description = "Run pytest" }
docs = { cmd = "mkdocs serve", description = "Run mkdocs server" }
download-test-data = { cmd = """
mkdir -p tests/data && \
curl -L -o tests/data/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif https://capella-open-data.s3.amazonaws.com/data/2024/6/26/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif && \
curl -L -o tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif https://capella-open-data.s3.amazonaws.com/data/2024/6/29/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif
""", description = "Download Capella SLC test data files" }

[tool.hatch.version]
source = "vcs"
[tool.hatch.version.raw-options]
version_scheme = "no-guess-dev"
[tool.hatch.build.hooks.vcs]
version-file = "src/capella_reader/_version.py"

[tool.ruff]
src = ["src"]
unsafe-fixes = true

[tool.ruff.lint]
select = [
  "A",      # flake8-builtins
  "ARG",    # flake8-unused-arguments
  "B",      # flake8-bugbear
  "C4",     # flake8-comprehensions
  "D",      # pydocstyle
  "E",      # pycodestyle (errors)
  "W",      # pycodestyle (warnings)
  "EM",     # flake8-errmsg
  "EXE",    # flake8-executable
  "F",      # Pyflakes
  "I",      # isort
  "ISC",    # flake8-implicit-str-concat
  "N",      # pep8-naming
  "NPY201", # numpy 2.0 deprecations
  "PGH",    # pygrep-hooks
  "PIE",    # flake8-pie
  "PL",     # Pylint
  "PT",     # flake8-pytest-style
  "PTH",    # flake8-use-pathlib
  "PYI",    # flake8-pyi
  "RUF",    # Ruff-specific rules
  "SIM",    # flake8-simplify
  "TRY",    # tryceratops
  "UP",     # pyupgrade
  "YTT",    # flake8-2020
]

ignore = [
  "PTH123", # `open()` should be replaced by `Path.open()`
  "PLR",    # Pylint Refactor
  "D100",   # Missing docstring in public module
  "D102",   # Missing docstring in public method (properties)
  "D104",   # Missing docstring in public package
  "D105",   # Missing docstring in magic method
  "D213",   # D213: Multi-line docstring summary should start at the second line
  "D203",   # 1 blank line required before class docstring
  "N806",   # Variable _ in function should be lowercase
  "SIM108", # Use ternary operator
  "TRY003", # Avoid specifying long messages outside the exception
]

[tool.ruff.lint.per-file-ignores]
"**/__init__.py" = ["F401", "F403"]
"tests/**" = ["D", "N", "PTH", "ARG001", "PLR2004", "E501", "PLC0415"]
"docs/**" = ["D", "E501", "PLC0415", "RUF059"]                         # Example scripts
"src/capella_reader/_version.py" = ["UP007"]                           # Auto-generated file

[tool.black]
target-version = ["py311", "py312", "py313", "py314"]
preview = true

[tool.mypy]
python_version = "3.11"
ignore_missing_imports = true
plugins = ["pydantic.mypy"]


[tool.pytest.ini_options]
addopts = ""
filterwarnings = ["error"]
markers = [
  "integration: marks tests that require real test data files (deselect with '-m \"not integration\"')",
]


---
README.md
---
# capella-reader

[![Actions Status][actions-badge]][actions-link]
[![PyPI version][pypi-version]][pypi-link]

[![Conda-Forge][conda-badge]][conda-link]
[![GitHub Discussion][github-discussions-badge]][github-discussions-link]

Package for creating typed Python models from Capella Single Look Complex (SLC) GeoTIFF images and JSON metadata.

Capella embeds product metadata in the TIFF tags of its SLC GeoTIFF files[^1]; this package provides utilities for reading and working with that metadata based on the [Capella Product Specification version 1.8](https://support.capellaspace.com/hubfs/Capella_Space_SAR_Products_Format_Specification_v1.8.pdf?hsLang=en). Note that the JSON can be parsed with Python's built-in `json` module; however, this package provides typed models with validation and descriptions for each field for easier integration with other SAR processing libraries.
Additionally, this package includes an ISCE3 adapter via the `adapters.isce3` subpackage for working with the [ISCE3](https://github.com/isce-framework/isce3) processing library.

Example datasets can be found on the [Capella Open Data catalog](https://www.capellaspace.com/earth-observation/gallery).

[^1]: To work with other formats, such as CPHD and SICD, the [sarpy](https://github.com/ngageoint/sarpy) library is recommended.

## Installation

```bash
pip install capella-reader
```

## Quick Start

```python
from capella_reader import CapellaSLC

# Load a Capella SLC GeoTIFF
slc = CapellaSLC.from_file("tests/data/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif")

# Access metadata
print(slc.shape)                              # (rows, cols)
print(slc.meta.collect.platform)              # 'capella-14'
print(slc.meta.collect.image.center_pixel.incidence_angle)
```

### Loading Metadata from JSON

You can also load metadata directly from extended metadata JSON files:

```python
from pathlib import Path
from capella_reader.metadata import CapellaSLC, CapellaSLCMetadata

md_file = "CAPELLA_C13_SP_SLC_HH_20241126045307_20241126045346_extended.json"
slc = CapellaSLC.from_file(md_file)

# Or from a Python dict
import json
data = json.loads(Path(md_file).read_text())
meta = CapellaSLCMetadata.model_validate(data)

# Round-trip: dump back to JSON
json_str = meta.model_dump_json(indent=2)
```

## Examples

The `docs/examples/` directory contains visualization scripts demonstrating orbit and image footprint analysis:

### Orbit Visualization

```bash
# Combined map with orbit track and image footprint
python examples/orbit_footprint_map.py
```

See [docs/examples/README.md](docs/examples/README.md) for installation requirements and details.

### ISCE3 Integration

For users working with ISCE3, conversion utilities are available in the `adapters.isce3` subpackage:

```python
from capella_reader import CapellaSLC
from capella_reader import adapters

slc = CapellaSLC.from_file("path/to/slc.tif")

# Convert to ISCE3 data structures
radar_grid = adapters.isce3.get_radar_grid(slc)
orbit = adapters.isce3.get_orbit(slc)
doppler_poly = adapters.isce3.get_doppler_poly(slc)
doppler_lut = adapters.isce3.get_doppler_lut2d(slc)
```

These utilities require [ISCE3 to be installed separately](https://isce-framework.github.io/isce3/buildinstall/). They are not required for basic metadata parsing.

## Example slc model

```python

CapellaSLC(
    path=PosixPath('tests/data/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif'),
    meta=CapellaSLCMetadata(
        software_version='3.2.1',
        software_revision='f8884c31ba9ba129f4d838d6d02dea2c45a114ba-dirty',
        processing_time=datetime.datetime(2025, 5, 8, 17, 38, 5, 669807, tzinfo=TzInfo(UTC)),
        processing_deployment='production',
        product_version='1.10',
        product_type='SLC',
        collect=Collect(
            start_timestamp=datetime.datetime(2024, 6, 26, 15, 0, 51, 295975, tzinfo=TzInfo(UTC)),
            stop_timestamp=datetime.datetime(2024, 6, 26, 15, 0, 55, 793597, tzinfo=TzInfo(UTC)),
            local_datetime=datetime.datetime(2024, 6, 26, 9, 0, 53, 544786, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800))),
            local_timezone='America/Mexico_City',
            platform='capella-14',
            mode='stripmap',
            collect_id='0141157d-3cef-4db0-9239-66c8478e09e6',
            image=ImageMetadata(
                data_type='CInt16',
                length=20395.09163067352,
                width=10071.72523110632,
                rows=19191,
                columns=10631,
                pixel_spacing_row=1.0627985769047086,
                pixel_spacing_column=0.9474459172784211,
                algorithm='backprojection',
                scale_factor=0.004915546693498318,
                range_autofocus=None,
                azimuth_autofocus=None,
                range_window=Window(name='rectangular', parameters={}, broadening_factor=0.8844848400382688),
                processed_range_bandwidth=200000000.0,
                azimuth_window=Window(name='antenna-taper', parameters={'proc_beamwidth': 0.011835009078692163}, broadening_factor=0.9743063402100489),
                processed_azimuth_bandwidth=5022.581708747391,
                image_geometry=ImageGeometry(
                    type='slant_plane',
                    doppler_centroid_polynomial=Poly2D(degree=(3, 3), coefficients=array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]])),
                    first_line_time=datetime.datetime(2024, 6, 26, 15, 0, 51, 889124, tzinfo=TzInfo(UTC)),
                    delta_line_time=0.00016105733333333333,
                    range_to_first_sample=761275.5576171188,
                    delta_range_sample=0.6171875
                ),
                center_pixel=CenterPixel(
                    incidence_angle=40.93041272468091,
                    look_angle=36.797539101417286,
                    squint_angle=-0.05586304095603156,
                    layover_angle=0.05014323818914027,
                    target_position=ECEFPosition(x=-940324.8666604777, y=-5946543.280362384, z=2098893.4900432685),
                    center_time=datetime.datetime(2024, 6, 26, 15, 0, 53, 544786, tzinfo=TzInfo(UTC))
                ),
                range_resolution=0.6629047106470235,
                ground_range_resolution=1.011849005778887,
                azimuth_resolution=1.280051064876099,
                ground_azimuth_resolution=1.2800517167779046,
                azimuth_looks=1.0,
                range_looks=1.0,
                enl=1.0,
                reference_antenna_position=ECEFPosition(x=-684525.5142771972, y=-6655343.817559726, z=1969607.6195497077),
                reference_target_position=ECEFPosition(x=-942416.8362992472, y=-5947750.607837195, z=2101298.743469007),
                azimuth_beam_pattern_corrected=True,
                elevation_beam_pattern_corrected=True,
                radiometry='beta_nought',
                calibration='full',
                calibration_id='calibration_bundle/61b7b4ca-81a8-45e2-932e-11e1489c02c6',
                nesz_polynomial=Poly1D(degree=3, coefficients=array([ 2.10754181e+05, -5.51826980e-01,  3.61176889e-07,  0.00000000e+00])),
                nesz_peak=-24.100213130335916,
                terrain_models=TerrainModels(
                    focusing=TerrainModelRef(link='ExplicitInflatedWGS84[2234.0048828125]+https://en.wikipedia.org/wiki/World_Geodetic_System', name='ExplicitInflatedWGS84[2234.0048828125]')
                ),
                reference_doppler_centroid=0.0,
                frequency_doppler_centroid_polynomial=Poly2D(
                    degree=(3, 3),
                    coefficients=array([[-1.44553946e-20, -7.48708712e-15, -2.85415191e-09,
         2.73100984e-15],
       [-1.74404613e-18,  3.05520758e-17,  1.18415007e-11,
        -7.73815716e-18],
       [ 4.66622572e-23,  2.51364523e-17,  1.01345640e-11,
        -2.31147587e-17],
       [-3.83941864e-23, -1.36857307e-17, -2.95147334e-12,
         5.47845610e-18]])
                ),
                quantization=Quantization(type='block_adaptive_quantization', block_sample_size=64, mean_bits=0, std_bits=0, sample_bits=6)
            ),
            radar=Radar(
                rank=50,
                center_frequency=9649999872.0,
                pointing=<LookSide.LEFT: 'left'>,
                sampling_frequency=750000000.0,
                transmit_polarization='H',
                receive_polarization='H',
                time_varying_parameters=[
                    RadarTimeVaryingParams(
                        start_timestamps=[datetime.datetime(2024, 6, 26, 15, 0, 51, 301024, tzinfo=TzInfo(UTC))],
                        prf=9901.251518191899,
                        pulse_bandwidth=200000000.0,
                        pulse_duration=2.0197333333333334e-05,
                        rank=50
                    ),
                    RadarTimeVaryingParams(
                        start_timestamps=[
                            datetime.datetime(2024, 6, 26, 15, 0, 51, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 52, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 52, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 53, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 53, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 54, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 54, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 55, 301024, tzinfo=TzInfo(UTC))
                        ],
                        prf=9900.72869363185,
                        pulse_bandwidth=200000000.0,
                        pulse_duration=2.0197333333333334e-05,
                        rank=50
                    )
                ],
                prf=[
                    PRFEntry(start_timestamps=[datetime.datetime(2024, 6, 26, 15, 0, 51, 301024, tzinfo=TzInfo(UTC))], prf=9901.251518191899),
                    PRFEntry(
                        start_timestamps=[
                            datetime.datetime(2024, 6, 26, 15, 0, 51, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 52, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 52, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 53, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 53, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 54, 301024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 54, 801024, tzinfo=TzInfo(UTC)),
                            datetime.datetime(2024, 6, 26, 15, 0, 55, 301024, tzinfo=TzInfo(UTC))
                        ],
                        prf=9900.72869363185
                    )
                ]
            ),
            state=State(
                coordinate_system=CoordinateSystem(type='ecef'),
                direction='ascending',
                state_vectors=[
                    StateVector(
                        time=datetime.datetime(2024, 6, 26, 15, 0, 21, 199958, tzinfo=TzInfo(UTC)),
                        position=ECEFPosition(x=-850598.581307973, y=-6682238.839189964, z=1807837.287228866),
                        velocity=ECEFVelocity(vx=5117.104280428376, vy=717.1797056134662, vz=5036.528855949608)
                    ),
                    StateVector(
                        time=datetime.datetime(2024, 6, 26, 15, 0, 21, 799957, tzinfo=TzInfo(UTC)),
                        position=ECEFPosition(x=-847528.1256471712, y=-6681807.258344243, z=1810858.8158753526),
                        velocity=ECEFVelocity(vx=5117.763610991626, vy=721.4251640291432, vz=5035.248748046506)
                    ),
...
                    StateVector(
                        time=datetime.datetime(2024, 6, 26, 15, 1, 24, 799857, tzinfo=TzInfo(UTC)),
                        position=ECEFPosition(x=-523139.33015611547, y=-6622363.191185433, z=2123595.7839055755),
                        velocity=ECEFVelocity(vx=5177.000593219978, vy=1164.868056941853, vz=4889.0522055202055)
                    ),
                    StateVector(
                        time=datetime.datetime(2024, 6, 26, 15, 1, 25, 399856, tzinfo=TzInfo(UTC)),
                        position=ECEFPosition(x=-520032.9939516515, y=-6621663.012097265, z=2126528.759803662),
                        velocity=ECEFVelocity(vx=5177.469063184616, vy=1169.066041486259, vz=4887.548741599158)
                    )
                ],
                source='precise_determination'
            ),
            pointing=[
                PointingSample(
                    time=datetime.datetime(2024, 6, 26, 15, 0, 21, 199958, tzinfo=TzInfo(UTC)),
                    attitude=AttitudeQuaternion(q0=0.7514354558972666, q1=0.539089174592113, q2=0.33782447576219665, q3=0.17493464855872806)
                ),
                PointingSample(
                    time=datetime.datetime(2024, 6, 26, 15, 0, 21, 799957, tzinfo=TzInfo(UTC)),
                    attitude=AttitudeQuaternion(q0=0.7513778110356235, q1=0.5391998219192001, q2=0.3379087735187681, q3=0.1746782481659656)
                ),
                ...
                PointingSample(
                    time=datetime.datetime(2024, 6, 26, 15, 1, 25, 399856, tzinfo=TzInfo(UTC)),
                    attitude=AttitudeQuaternion(q0=0.7464363921729759, q1=0.549879633720447, q2=0.3455829675461274, q3=0.14504242371384654)
                )
            ],
            transmit_antenna=Antenna(
                azimuth_beamwidth=0.01211729820681252,
                elevation_beamwidth=0.01219859298030783,
                gain=49.58,
                beam_pattern=Poly2D(
                    degree=(7, 7),
                    coefficients=array([[ 4.96288054e+01,  3.54236916e-04, -8.23751862e+04,
        -5.46972016e+00, -3.85558272e+07,  3.86615092e+04,
         5.23604124e+11,  2.89923661e+08],
       [-3.48671932e-04,  2.20787614e+00, -5.08022213e+00,
        -9.12656092e+03,  6.91149686e+04,  5.63776331e+06,
        ...
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00],
       [-7.85159183e+07,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00]])
                )
            ),
            receive_antenna=Antenna(
                azimuth_beamwidth=0.01211729820681252,
                elevation_beamwidth=0.01219859298030783,
                gain=49.58,
                beam_pattern=Poly2D(
                    degree=(7, 7),
                    coefficients=array([[ 4.96288054e+01,  3.54236916e-04, -8.23751862e+04,
        -5.46972016e+00, -3.85558272e+07,  3.86615092e+04,
         5.23604124e+11,  2.89923661e+08],
       [-3.48671932e-04,  2.20787614e+00, -5.08022213e+00,
        -9.12656092e+03,  6.91149686e+04,  5.63776331e+06,
         ...
       [-7.85159183e+07,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00]])
                )
            )
        )
    )
)
```

## Development

Using [pixi](https://pixi.sh/) for pacakage management and task running:

```bash
# after installing pixi: curl -fsSL https://pixi.sh/install.sh | sh
pixi install
```

To create a shell for development
```bash
pixi shell -e test
pre-commit install
```

You can see all pixi tasks with:

```bash
pixi task list
```

Download test data (required for integration tests):
```bash
pixi run download-test-data        # Get the sample data images
pixi run test                      # Run all tests (requires test data)
```

Run linting and formatting:
```bash
pixi run --environment test check
pixi run --environment test test
```

## References

[Capella Product Specification version 1.8](https://support.capellaspace.com/hubfs/Capella_Space_SAR_Products_Format_Specification_v1.8.pdf?hsLang=en)

## License

See LICENSE.txt

<!-- prettier-ignore-start -->
[actions-badge]:            https://github.com/scottstanie/capella-reader/workflows/CI/badge.svg
[actions-link]:             https://github.com/scottstanie/capella-reader/actions
[conda-badge]:              https://img.shields.io/conda/vn/conda-forge/capella-reader
[conda-link]:               https://github.com/conda-forge/capella-reader-feedstock
[github-discussions-badge]: https://img.shields.io/static/v1?label=Discussions&message=Ask&color=blue&logo=github
[github-discussions-link]:  https://github.com/scottstanie/capella-reader/discussions
[pypi-link]:                https://pypi.org/project/capella-reader/
[pypi-version]:             https://img.shields.io/pypi/v/capella-reader

<!-- prettier-ignore-end -->


---
docs/examples/isce3_get_doppler.py
---
"""Example of estimating Doppler from Capella attitude data.

Functions started from ISCE3 example:
https://github.com/isce-framework/isce3/blob/5b2d6f6057a706d3ff5cfe7342ced8a7933fb302/python/packages/isce3/geometry/doppler.py

Uses the `get_attitude` in `capella_reader.adapters.isce3`.
"""

from __future__ import annotations

import logging
from pathlib import Path

import isce3
import matplotlib.pyplot as plt
import numpy as np
from isce3.geometry import DEMInterpolator
from numpy.typing import ArrayLike

from capella_reader import CapellaSLC
from capella_reader.adapters.isce3 import get_attitude, get_doppler_lut2d, get_orbit


def los2doppler(look, v, wvl):
    """
    Compute Doppler given line-of-sight vector

    Parameters
    ----------
    look : array_like
        ECEF line of sight vector in m
    v : array_like
        ECEF velocity vector in m/s
    wvl : float
        Radar wavelength in m

    Returns
    -------
    float
        Doppler frequency in Hz
    """
    return 2 / wvl * np.asarray(v).dot(look) / np.linalg.norm(look)


def make_doppler_lut_from_attitude(
    az_time: ArrayLike,
    slant_range: ArrayLike,
    orbit: isce3.core.Orbit,
    attitude: isce3.core.Attitude,
    wavelength: float,
    *,
    dem: DEMInterpolator | None = None,
    epoch: isce3.core.DateTime | None = None,
    az_angle: float = 0.0,
    interp_method: isce3.core.DataInterpMethod | str = "bilinear",
    bounds_error: bool = True,
) -> isce3.core.LUT2d:
    """
    Estimate a 2-D lookup table (LUT) of Doppler centroid values from antenna attitude.

    Parameters
    ----------
    az_time : array_like
        Azimuth time coordinates of the output LUT, in seconds. Values must be uniformly
        spaced. If `epoch` is not None, these coordinates should be specified relative
        to that datetime. Otherwise, they should be relative to the reference epoch of
        `orbit` and `attitude` (which must have the same reference epoch in this case).
    slant_range : array_like
        Slant range coordinates of the output LUT, in meters. Values must be uniformly
        spaced.
    orbit : isce3.core.Orbit
        The path of the antenna phase center over a period spanning the azimuth time
        bounds of the output LUT. If `epoch` is None, this must have the same reference
        epoch as `attitude`.
    attitude : isce3.core.Attitude
        The orientation of the antenna over a period spanning the azimuth time bounds of
        the output LUT. Represents the rotation from the radar antenna coordinate system
        to ECEF coordinates as a function of time. The antenna coordinate system is a
        Cartesian coordinate system with +Z axis pointing along the mechanical boresight
        of the antenna, +X axis pointing in the direction of increasing elevation angle,
        and +Y axis pointing in the direction of increasing azimuth angle. If `epoch` is
        None, this must have the same reference epoch as `orbit`.
    wavelength : float
        The radar central wavelength, in meters.
    dem : DEMInterpolator or None, optional
        Digital elevation model specifying the height of the scene in meters above the
        WGS 84 ellipsoid. If None, a zero-height DEM is used. Will calculate stats
        (modifying input object) if they haven't already been calculated. Defaults to
        None.
    epoch : isce3.core.DateTime or None, optional
        Reference epoch for the azimuth time coordinates of the output LUT. If None,
        defaults to the reference epoch of `orbit` and `attitude` (which must have the
        same reference epoch in this case). Defaults to None.
    az_angle : float, optional
        Complement of the angle between the along-track axis of the antenna and its
        electrical boresight, in radians.  Zero for non-scanned, flush-mounted antennas
        like ALOS-1. Defaults to 0.
    interp_method : isce3.core.DataInterpMethod or str, optional
        Interpolation method used by the output LUT. Defaults to 'bilinear'.
    bounds_error : bool, optional
        Whether to raise an exception when attempting to evaluate the output LUT outside
        of its valid domain. Defaults to True.

    Returns
    -------
    isce3.core.LUT2d
        A 2-D LUT that may be used to compute the estimated Doppler centroid of the SAR
        acquisition, in hertz, as a function of azimuth time and slant range.
    """
    log = logging.getLogger("isce3.geometry.doppler")

    if epoch is None:
        # If `orbit` and `attitude` have different reference epochs, it could cause
        # confusion about which epoch the output LUT is referencing. It seems best to
        # raise an exception in this case, rather than update one reference epoch to
        # match the other.
        if orbit.reference_epoch != attitude.reference_epoch:
            msg = (
                f"orbit reference epoch ({orbit.reference_epoch}) must match attitude"
                f" reference epoch ({attitude.reference_epoch})"
            )
            raise ValueError(msg)
    else:
        # If `epoch` was specified, ensure that `orbit` and `attitude` each have that
        # reference epoch. If necessary, create a temporary copy of the orbit and/or
        # attitude data and update its time tags.
        if orbit.reference_epoch != epoch:
            orbit = orbit.copy()
            orbit.update_reference_epoch(epoch)
        if attitude.reference_epoch != epoch:
            attitude = attitude.copy()
            attitude.update_reference_epoch(epoch)

    az_time = np.asarray(az_time)
    slant_range = np.asarray(slant_range)

    # If a DEM wasn't provided, default to a zero-height DEM.
    if dem is None:
        dem = DEMInterpolator()

    # Compute height statistics of the input DEM, including the min & max heights. These
    # are needed by `get_approx_el_bounds` below. Statistics are stored internally in
    # the `DEMInterpolator` object. This is a no-op if height statistics were previously
    # computed.
    dem.compute_min_max_mean_height()

    dop = np.zeros((len(az_time), len(slant_range)))

    # Using the default EL bounds [-45, 45] deg can cause trouble when looking
    # near nadir, as this large interval can span both sides of the left-right
    # ambiguity.  So solve the problem on the sphere a few times using bounding
    # cases.
    log.info("Attempting to find reasonable EL search bounds.")
    ti = az_time[len(az_time) // 2]
    rdr_xyz, _ = orbit.interpolate(ti)
    qi = attitude.interpolate(ti)
    rmin = slant_range[0]
    rmax = slant_range[-1]
    el0, el1 = isce3.antenna.get_approx_el_bounds(rmin, az_angle, rdr_xyz, qi, dem)
    el2, el3 = isce3.antenna.get_approx_el_bounds(rmax, az_angle, rdr_xyz, qi, dem)
    el_min, el_max = min(el0, el2), max(el1, el3)
    log.info(
        f"Preliminary EL bounds are [{np.rad2deg(el_min) :.3f}, "
        f"{np.rad2deg(el_max) :.3f}] deg"
    )

    for i, ti in enumerate(az_time):
        rdr_xyz, v = orbit.interpolate(ti)
        qi = attitude.interpolate(ti)
        for j, rj in enumerate(slant_range):
            # For very long observations the geometry may change enough that
            # the bounds become invalid.  If that happens, recalculate.
            try:
                tgt_xyz = isce3.antenna.range_az_to_xyz(
                    rj, az_angle, rdr_xyz, qi, dem, el_min=el_min, el_max=el_max
                )
            except RuntimeError:
                el0, el1 = isce3.antenna.get_approx_el_bounds(
                    rj, az_angle, rdr_xyz, qi, dem
                )
                el_min, el_max = min(el0, el_min), max(el1, el_max)
                log.info(
                    f"Updating EL bounds to [{np.rad2deg(el_min) :.3f}, "
                    f"{np.rad2deg(el_max) :.3f}] deg"
                )
                tgt_xyz = isce3.antenna.range_az_to_xyz(
                    rj, az_angle, rdr_xyz, qi, dem, el_min=el_min, el_max=el_max
                )
            dop[i, j] = los2doppler(tgt_xyz - rdr_xyz, v, wavelength)

    return isce3.core.LUT2d(slant_range, az_time, dop, interp_method, bounds_error)


def main():
    """Demonstrate using attitude to compute Doppler centroid for a Capella SLC."""
    slc_path = Path(
        "tests/data/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif"
    )
    slc = CapellaSLC.from_file(slc_path)

    print(f"Loaded SLC: {slc_path.name}")
    print(f"Shape: {slc.shape}")
    print(f"Wavelength: {slc.wavelength:.4f} m")
    print(f"Number of state vectors: {len(slc.collect.state.state_vectors)}")
    print(f"Number of pointing samples: {len(slc.collect.pointing)}")

    orbit = get_orbit(slc.collect.state.state_vectors, ref_epoch=slc.ref_epoch)
    attitude = get_attitude(slc.collect.pointing, ref_epoch=slc.ref_epoch)

    print("\nComputing Doppler LUT from attitude...")

    sensing_start = (slc.sensing_start - slc.ref_epoch).total_seconds()
    sensing_end = sensing_start + slc.shape[0] * slc.delta_line_time
    n_az = 20
    az_times = np.linspace(sensing_start, sensing_end, n_az)

    n_rg = 10
    slant_ranges = np.linspace(
        slc.starting_range,
        slc.starting_range + slc.shape[1] * slc.delta_range_sample,
        n_rg,
    )

    doppler_from_attitude = make_doppler_lut_from_attitude(
        az_time=az_times,
        slant_range=slant_ranges,
        orbit=orbit,
        attitude=attitude,
        wavelength=slc.wavelength,
        epoch=isce3.core.DateTime(str(slc.ref_epoch).strip("Z")),
        bounds_error=False,
    )

    doppler_from_metadata = get_doppler_lut2d(slc, n_az=n_az)

    print("Computing differences...")
    az_eval = az_times[1:-1]
    rg_eval = slant_ranges[1:-1]

    Y, X = np.meshgrid(az_eval, rg_eval, indexing="ij")
    dop_att = np.zeros_like(Y)
    dop_meta = np.zeros_like(Y)

    for i, t in enumerate(az_eval):
        for j, r in enumerate(rg_eval):
            dop_att[i, j] = doppler_from_attitude.eval(t, r)
            dop_meta[i, j] = doppler_from_metadata.eval(t, r)

    diff = dop_att - dop_meta

    print(
        f"\nDoppler from attitude - mean: {dop_att.mean():.2f} Hz, std:"
        f" {dop_att.std():.2f} Hz"
    )
    print(
        f"Doppler from metadata - mean: {dop_meta.mean():.2f} Hz, std:"
        f" {dop_meta.std():.2f} Hz"
    )
    print(
        f"Difference - mean: {diff.mean():.2f} Hz, std: {diff.std():.2f} Hz, max:"
        f" {np.abs(diff).max():.2f} Hz"
    )

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    im0 = axes[0].imshow(dop_att, aspect="auto", cmap="RdBu_r")
    axes[0].set_title("Doppler from Attitude")
    axes[0].set_xlabel("Slant Range Index")
    axes[0].set_ylabel("Azimuth Time Index")
    plt.colorbar(im0, ax=axes[0], label="Doppler (Hz)")

    im1 = axes[1].imshow(dop_meta, aspect="auto", cmap="RdBu_r")
    axes[1].set_title("Doppler from Metadata")
    axes[1].set_xlabel("Slant Range Index")
    axes[1].set_ylabel("Azimuth Time Index")
    plt.colorbar(im1, ax=axes[1], label="Doppler (Hz)")

    im2 = axes[2].imshow(diff, aspect="auto", cmap="RdBu_r")
    axes[2].set_title("Difference (Attitude - Metadata)")
    axes[2].set_xlabel("Slant Range Index")
    axes[2].set_ylabel("Azimuth Time Index")
    plt.colorbar(im2, ax=axes[2], label="Doppler (Hz)")

    plt.tight_layout()
    plt.savefig("doppler_comparison.png", dpi=150)
    print("\nSaved comparison plot to doppler_comparison.png")
    plt.show()


if __name__ == "__main__":
    main()


---
docs/examples/orbit_ground_track.py
---
#!/usr/bin/env python
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "cartopy",
#     "numpy",
#     "pyproj",
#     "shapely",
# ]
# ///
"""Visualize satellite ground track and image footprint on a 2D map.

This script demonstrates how to:
- Load Capella metadata from JSON
- Extract orbit state vectors
- Convert ECEF coordinates to lat/lon
- Calculate image footprint corners
- Plot the ground track and footprint on a map
"""

from pathlib import Path

import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import numpy as np
import pyproj
from shapely.geometry import Polygon

from capella_reader.metadata import CapellaSLCMetadata
from capella_reader.slc import CapellaSLC


def ecef_to_latlon(positions):
    """Convert ECEF positions to latitude/longitude.

    Parameters
    ----------
    positions : np.ndarray
        Nx3 array of ECEF positions [x, y, z] in meters

    Returns
    -------
    lats : np.ndarray
        Latitudes in degrees
    lons : np.ndarray
        Longitudes in degrees
    alts : np.ndarray
        Altitudes in meters above WGS84 ellipsoid

    """
    ecef = pyproj.Proj(proj="geocent", ellps="WGS84", datum="WGS84")
    lla = pyproj.Proj(proj="latlong", ellps="WGS84", datum="WGS84")
    transformer = pyproj.Transformer.from_proj(ecef, lla, always_xy=True)

    lons, lats, alts = transformer.transform(
        positions[:, 0], positions[:, 1], positions[:, 2]
    )

    return np.array(lats), np.array(lons), np.array(alts)


def calculate_image_corners_ecef(meta):
    """Calculate the ECEF coordinates of the image corners.

    Parameters
    ----------
    meta : CapellaSLCMetadata
        Capella metadata object

    Returns
    -------
    corners : np.ndarray
        4x3 array of corner positions [x, y, z] in ECEF meters
        Order: top-left, top-right, bottom-right, bottom-left

    """

    def _get_pfa_corners(image, image_geometry):
        rows = image.rows
        cols = image.columns

        ref_row, ref_col = image_geometry.scene_reference_point_row_col
        ref_ecef = image_geometry.scene_reference_point_ecef.as_array()

        row_dir = np.array(image_geometry.row_direction)
        col_dir = np.array(image_geometry.col_direction)

        row_spacing = image.pixel_spacing_row
        col_spacing = image.pixel_spacing_column

        return np.array(
            [
                ref_ecef
                - (ref_row * row_spacing * row_dir)
                - (ref_col * col_spacing * col_dir),
                ref_ecef
                - (ref_row * row_spacing * row_dir)
                + ((cols - ref_col) * col_spacing * col_dir),
                ref_ecef
                + ((rows - ref_row) * row_spacing * row_dir)
                + ((cols - ref_col) * col_spacing * col_dir),
                ref_ecef
                + ((rows - ref_row) * row_spacing * row_dir)
                - (ref_col * col_spacing * col_dir),
            ]
        )

    def _get_slant_plane_corners(image, state_vectors, pointing):
        center_ecef = np.array(image.center_pixel.target_position.as_array())
        center_lat, center_lon, _ = ecef_to_latlon(center_ecef.reshape(1, -1))
        lat_rad = np.deg2rad(center_lat[0])
        lon_rad = np.deg2rad(center_lon[0])

        east = np.array([-np.sin(lon_rad), np.cos(lon_rad), 0.0])
        north = np.array(
            [
                -np.sin(lat_rad) * np.cos(lon_rad),
                -np.sin(lat_rad) * np.sin(lon_rad),
                np.cos(lat_rad),
            ]
        )

        center_time = image.center_pixel.center_time
        closest_sv = min(
            state_vectors,
            key=lambda sv: abs((sv.time - center_time).total_seconds()),
        )
        sat_vel = np.array(closest_sv.velocity.as_array())

        along_enu = (np.dot(sat_vel, east) * east) + (np.dot(sat_vel, north) * north)
        along_dir = along_enu / np.linalg.norm(along_enu)

        along_e = np.dot(along_dir, east)
        along_n = np.dot(along_dir, north)
        col_enu = (
            (along_n * east - along_e * north)
            if pointing == "right"
            else (-along_n * east + along_e * north)
        )
        col_dir = col_enu / np.linalg.norm(col_enu)

        row_extent = 0.5 * (image.rows - 1) * image.pixel_spacing_row
        col_extent = 0.5 * (image.columns - 1) * image.pixel_spacing_column
        row_dir = along_dir

        return np.array(
            [
                center_ecef - row_extent * row_dir - col_extent * col_dir,
                center_ecef - row_extent * row_dir + col_extent * col_dir,
                center_ecef + row_extent * row_dir + col_extent * col_dir,
                center_ecef + row_extent * row_dir - col_extent * col_dir,
            ]
        )

    image = meta.collect.image
    image_geometry = image.image_geometry

    if image_geometry.type == "pfa":
        return _get_pfa_corners(image, image_geometry)

    return _get_slant_plane_corners(
        image, meta.collect.state.state_vectors, meta.collect.radar.pointing
    )


def main(filename: Path | str, /):
    """Plot the ground track and image footprint for a Capella SLC file.

    Parameters
    ----------
    filename : Path | str
        Path to the Capella SLC file (JSON metadata or .slc file)

    """
    path = Path(filename)
    if path.suffix == ".json":
        print(f"Loading metadata from {path.name}...")
        json_str = path.read_text()
        meta = CapellaSLCMetadata.model_validate_json(json_str)
    else:
        print(f"Loading SLC file from {path.name}...")
        meta = CapellaSLC.from_file(path).meta

    state_vectors = meta.collect.state.state_vectors
    print(f"Found {len(state_vectors)} state vectors")

    positions = np.array([sv.position.as_array() for sv in state_vectors])
    sat_lats, sat_lons, sat_alts = ecef_to_latlon(positions)
    print(f"Satellite altitude: {sat_alts.mean() / 1000:.1f} km")
    print(f"Orbit direction: {meta.collect.state.direction}")

    geometry_type = meta.collect.image.image_geometry.type
    print(f"Geometry type: {geometry_type}")

    if geometry_type == "slant_plane":
        image_corners_ecef = calculate_image_corners_ecef(meta)
        corner_lats, corner_lons, _ = ecef_to_latlon(image_corners_ecef)
    else:
        print("Skipping footprint calculation for PFA geometry (not yet supported)")
        corner_lats = corner_lons = None

    center_ecef = np.array(meta.collect.image.center_pixel.target_position.as_array())
    center_lats, center_lons, _ = ecef_to_latlon(center_ecef.reshape(1, -1))

    fig = plt.figure(figsize=(9, 6))
    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())

    lat_margin = 1.0
    lon_margin = 1.0

    if corner_lats is not None:
        min_lon = min(sat_lons.min(), corner_lons.min())
        max_lon = max(sat_lons.max(), corner_lons.max())
        min_lat = min(sat_lats.min(), corner_lats.min())
        max_lat = max(sat_lats.max(), corner_lats.max())
    else:
        min_lon = min(sat_lons.min(), center_lons[0])
        max_lon = max(sat_lons.max(), center_lons[0])
        min_lat = min(sat_lats.min(), center_lats[0])
        max_lat = max(sat_lats.max(), center_lats[0])

    ax.set_extent(
        [
            min_lon - lon_margin,
            max_lon + lon_margin,
            min_lat - lat_margin,
            max_lat + lat_margin,
        ],
        crs=ccrs.PlateCarree(),
    )

    ax.add_feature(cfeature.LAND, facecolor="lightgray", alpha=0.3)
    ax.add_feature(cfeature.OCEAN, facecolor="lightblue", alpha=0.3)
    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
    ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle=":")
    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)

    ax.plot(
        sat_lons,
        sat_lats,
        "r-",
        linewidth=2,
        label=f"Satellite Ground Track ({meta.collect.state.direction})",
        transform=ccrs.Geodetic(),
    )

    ax.scatter(
        sat_lons[0],
        sat_lats[0],
        c="green",
        s=100,
        marker="o",
        label="Start",
        transform=ccrs.Geodetic(),
        zorder=5,
    )
    ax.scatter(
        sat_lons[-1],
        sat_lats[-1],
        c="red",
        s=100,
        marker="s",
        label="End",
        transform=ccrs.Geodetic(),
        zorder=5,
    )

    if corner_lats is not None:
        footprint_polygon = Polygon(zip(corner_lons, corner_lats, strict=False))
        ax.add_geometries(
            [footprint_polygon],
            crs=ccrs.PlateCarree(),
            facecolor="blue",
            alpha=0.2,
            edgecolor="blue",
            linewidth=2,
            label="Image Footprint",
        )

    ax.scatter(
        center_lons[0],
        center_lats[0],
        c="blue",
        s=150,
        marker="*",
        label="Scene Center",
        transform=ccrs.Geodetic(),
        zorder=5,
    )

    ax.legend(loc="upper right")

    center_time_str = meta.collect.image.center_pixel.center_time.as_datetime()
    ax.set_title(
        "Capella SAR Ground Track and Image Footprint\n"
        f"{center_time_str.strftime('%Y-%m-%d %H:%M:%S')} UTC",
        fontsize=14,
        fontweight="bold",
    )

    plt.tight_layout()
    output_file = Path(__file__).parent / "ground_track.png"
    plt.savefig(output_file, dpi=150, bbox_inches="tight")
    print(f"\nSaved figure to {output_file}")

    plt.show()


if __name__ == "__main__":
    import tyro

    tyro.cli(main)


---
docs/examples/orbit_quality_check copy.py
---
#!/usr/bin/env python3
"""Orbit Quality Diagnostic Tool.

This script checks whether orbit state vectors have velocity consistent with position,
which determines whether Hermite interpolation is appropriate.

Hermite interpolation constrains velocity to be the derivative of position.
If the input orbit has velocity that doesn't match dP/dt (common with some sensors like
Radarsat-2, Kompsat), Hermite will produce worse results than Lagrange.

Usage:

Run the script directly to see diagnostic plots:
    python orbit_quality_check.py --slc tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif

Or import and use with your own data:
    from orbit_quality_check import OrbitQualityChecker
    checker = OrbitQualityChecker(times, positions, velocities)
    checker.run_diagnostics()

"""

from dataclasses import dataclass

import matplotlib.pyplot as plt
import numpy as np
from numpy.typing import DTypeLike, NDArray

from capella_reader import CapellaSLC

# =============================================================================
# Interpolators
# =============================================================================


def hermite4_orbit_np(
    tt: NDArray, xx: NDArray, vv: NDArray, t: float, dtype: DTypeLike = np.float64
) -> tuple[NDArray, NDArray]:
    """4-node Hermite interpolation using position and velocity at each node.

    Parameters
    ----------
    tt : (4,) float64
        Node times (must be distinct).
    xx : (4,3) float64
        Position vectors at tt (ECEF meters).
    vv : (4,3) float64
        Velocity vectors at tt (ECEF m/s).
    t : float
        Query time.
    dtype : dtype_like
        Data type for internal calculations.
        Default is np.float64.

    Returns
    -------
    xout : (3,) float
        Interpolated position at time t.
    vout : (3,) float
        Interpolated velocity at time t (derivative of interpolant).

    """
    dtype = np.dtype(dtype)
    tt = np.asarray(tt, dtype=dtype)
    xx = np.asarray(xx, dtype=dtype)
    vv = np.asarray(vv, dtype=dtype)

    # if tt.shape != (4,) or xx.shape != (4, 3) or vv.shape != (4, 3):
    #     msg = "Expected tt:(4,), xx:(4,3), vv:(4,3)"
    #     raise ValueError(msg)
    # # TODO: this should move to a wrapper...
    # if np.any(~np.isfinite(tt)) or np.any(np.diff(np.sort(tt)) == 0):
    #     msg = "tt must be finite and distinct"
    #     raise ValueError(msg)

    # n = 4
    n = len(tt)

    # Precompute pairwise differences Δ_ij = t_i - t_j
    # diff[i,j] = tt[i] - tt[j]
    diff = tt[:, None] - tt[None, :]

    # For i != j: invdiff[i,j] = 1/(t_i - t_j), else 0
    invdiff = np.where(np.eye(n, dtype=bool), 0.0, 1.0 / diff)

    # dl[i] = L_i'(t_i) = Σ_{j≠i} 1/(t_i - t_j)
    dl = invdiff.sum(axis=1)

    # Compute Lagrange basis values L_i(t)
    # L_i(t) = Π_{j≠i} (t - t_j)/(t_i - t_j)
    L = np.ones(n, dtype=dtype)
    for i in range(n):
        for j in range(n):
            if j == i:
                continue
            L[i] *= (t - tt[j]) / (tt[i] - tt[j])

    # Compute L_i'(t) at the query time t (called hdot in ISCE/ROIPAC)
    # L_i'(t) = Σ_{j≠i} [ 1/(t_i - t_j) * Π_{k≠i,j} (t - t_k)/(t_i - t_k) ]
    Ldot = np.zeros(n, dtype=dtype)
    for i in range(n):
        s = 0.0
        for j in range(n):
            if j == i:
                continue
            prod = 1.0 / (tt[i] - tt[j])
            for k in range(n):
                if k in (i, j):
                    continue
                prod *= (t - tt[k]) / (tt[i] - tt[k])
            s += prod
        Ldot[i] = s

    dt = t - tt
    L2 = L * L

    # Hermite basis functions
    # α_i(t) = (1 - 2 (t - t_i) L_i'(t_i)) * L_i(t)^2
    # β_i(t) = (t - t_i) * L_i(t)^2
    alpha = (1.0 - 2.0 * dt * dl) * L2
    beta = dt * L2

    # Derivatives of basis functions (w.r.t. t)
    # α'_i(t) and β'_i(t) using d/dt L_i(t)^2 = 2 L_i(t) L_i'(t)
    dL2_dt = 2.0 * L * Ldot

    alpha_dot = (-2.0 * dl) * L2 + (1.0 - 2.0 * dt * dl) * dL2_dt
    beta_dot = 1.0 * L2 + dt * dL2_dt

    # Interpolate vector-valued position and velocity
    # p(t)  = Σ [ α_i x_i + β_i v_i ]
    # p'(t) = Σ [ α'_i x_i + β'_i v_i ]
    xout = alpha @ xx + beta @ vv
    vout = alpha_dot @ xx + beta_dot @ vv

    return xout, vout


@dataclass
class HermiteInterpolator:
    """Piecewise cubic Hermite interpolation.

    Uses position AND velocity at nodes, guaranteeing that the interpolated
    velocity equals the derivative of interpolated position.
    """

    """times: (N,) array of times in seconds"""
    times: np.ndarray
    """positions: (N, 3) array of positions in meters"""
    positions: np.ndarray
    """velocities: (N, 3) array of velocities in m/s"""
    velocities: np.ndarray

    def _find_interval(self, t: float) -> int:
        """Find the interval index for time t."""
        idx = np.searchsorted(self.times, t) - 1
        return np.clip(idx, 0, len(self.times) - 2)

    def interpolate(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate position and velocity at time t.

        Returns: (position, velocity) arrays
        """
        idx = self._find_interval(t)
        subset = slice(idx - 2, idx + 2)
        tt = self.times[subset]
        if tt.shape != (4,):
            raise ValueError(f"{t = } must have +/-2 elements in {self.times}")
        xx = self.positions[subset]
        vv = self.velocities[subset]

        return hermite4_orbit(tt, xx, vv, t)


@dataclass
class LagrangeInterpolator:
    """Lagrange polynomial interpolation (what ISCE calls "Legendre").

    Interpolates position and velocity independently using sliding windows.
    No constraint that velocity equals derivative of position.
    """

    """times: (N,) array of times in seconds"""
    times: np.ndarray
    """positions: (N, 3) array of positions in meters"""
    positions: np.ndarray
    """velocities: (N, 3) array of velocities in m/s"""
    velocities: np.ndarray
    """Number of points to use for interpolation (default 9 for degree-8)"""
    n_points: int = 9

    def _select_window(self, t: float) -> tuple[np.ndarray, int]:
        """Select n_points centered around time t, return times and start index."""
        idx = np.searchsorted(self.times, t)
        half = self.n_points // 2
        start = max(0, min(idx - half, len(self.times) - self.n_points))
        return self.times[start : start + self.n_points], start

    def _lagrange_interp(
        self, t: float, t_nodes: np.ndarray, values: np.ndarray
    ) -> np.ndarray:
        """Lagrange interpolation for vector values."""
        n = len(t_nodes)
        result = np.zeros(3)

        for k in range(n):
            # Compute Lagrange basis L_k(t)
            Lk = 1.0
            for j in range(n):
                if j != k:
                    Lk *= (t - t_nodes[j]) / (t_nodes[k] - t_nodes[j])
            result += values[k] * Lk

        return result

    def interpolate(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate position and velocity at time t.

        Note: Position and velocity are interpolated independently.
        """
        t_win, start = self._select_window(t)
        pos_win = self.positions[start : start + self.n_points]
        vel_win = self.velocities[start : start + self.n_points]

        position = self._lagrange_interp(t, t_win, pos_win)
        velocity = self._lagrange_interp(t, t_win, vel_win)

        return position, velocity

    def interpolate_with_pos_derivative(
        self, t: float, dt: float = 1e-3
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Interpolate and also compute velocity from position derivative.

        Returns: (position, velocity_from_fit, velocity_from_derivative)
        """
        position, velocity_fit = self.interpolate(t)

        # Numerical derivative of position
        pos_plus, _ = self.interpolate(t + dt / 2)
        pos_minus, _ = self.interpolate(t - dt / 2)
        velocity_deriv = (pos_plus - pos_minus) / dt

        return position, velocity_fit, velocity_deriv


# =============================================================================
# Orbit Quality Checker
# =============================================================================


def run_trade_study(
    t: np.ndarray,
    pos_hi: np.ndarray,
    vel_hi: np.ndarray,
    coarse_dt: float = 10.0,
    lagrange_n_points: int = 9,
) -> dict:
    """Trade study: downsample a high-rate 'truth' orbit, interpolate back.

    Compares Hermite vs Lagrange to the high-rate truth.

    Parameters
    ----------
    t : (N,) array
        High-rate times (in seconds)
    pos_hi : (N, 3) array
        High-rate positions (meters).
    vel_hi : (N, 3) array
        High-rate velocities (m/s).
    coarse_dt : float, default 10.0
        Target coarse sampling interval in seconds (e.g. 10 s).
    lagrange_n_points : int, default 9
        Number of points for Lagrange interpolation (degree = n_points - 1).

    Returns
    -------
    results : dict
        Contains per-method position/velocity errors vs the high-rate truth.

    """
    pos_hi = np.asarray(pos_hi)
    vel_hi = np.asarray(vel_hi)

    # --- Choose coarse sampling stride from desired coarse_dt ---
    dt_hi = np.median(np.diff(t))
    stride = max(1, round(coarse_dt / dt_hi))

    t_coarse = t[::stride]
    pos_coarse = pos_hi[::stride]
    vel_coarse = vel_hi[::stride]

    # Build interpolators from coarse orbit
    hermite = HermiteInterpolator(t_coarse, pos_coarse, vel_coarse)
    lagrange = LagrangeInterpolator(
        t_coarse, pos_coarse, vel_coarse, n_points=lagrange_n_points
    )

    # Restrict evaluation to the interior where Lagrange has a full window
    margin_coarse = lagrange_n_points // 2
    margin = margin_coarse * stride
    t_eval = t[margin:-margin]
    pos_truth = pos_hi[margin:-margin]
    vel_truth = vel_hi[margin:-margin]

    pos_err_herm = []
    vel_err_herm = []
    pos_err_lagr = []
    vel_err_lagr = []

    for ti, p_true, v_true in zip(t_eval, pos_truth, vel_truth, strict=True):
        p_h, v_h = hermite.interpolate(ti)
        p_l, v_l = lagrange.interpolate(ti)

        pos_err_herm.append(np.linalg.norm(p_h - p_true))
        vel_err_herm.append(np.linalg.norm(v_h - v_true))

        pos_err_lagr.append(np.linalg.norm(p_l - p_true))
        vel_err_lagr.append(np.linalg.norm(v_l - v_true))

    pos_err_herm = np.array(pos_err_herm)
    vel_err_herm = np.array(vel_err_herm)
    pos_err_lagr = np.array(pos_err_lagr)
    vel_err_lagr = np.array(vel_err_lagr)

    return {
        "t_eval": t_eval,
        "pos_err_hermite": pos_err_herm,
        "vel_err_hermite": vel_err_herm,
        "pos_err_lagrange": pos_err_lagr,
        "vel_err_lagrange": vel_err_lagr,
        "summary": {
            "hermite_pos_rms": np.sqrt(np.mean(pos_err_herm**2)),
            "hermite_pos_max": np.max(pos_err_herm),
            "hermite_vel_rms": np.sqrt(np.mean(vel_err_herm**2)),
            "hermite_vel_max": np.max(vel_err_herm),
            "lagrange_pos_rms": np.sqrt(np.mean(pos_err_lagr**2)),
            "lagrange_pos_max": np.max(pos_err_lagr),
            "lagrange_vel_rms": np.sqrt(np.mean(vel_err_lagr**2)),
            "lagrange_vel_max": np.max(vel_err_lagr),
            "coarse_stride": stride,
            "coarse_dt_est": dt_hi * stride,
            "dt_hi": dt_hi,
        },
    }


def plot_trade_study(
    results: dict, title: str | None = None, save_path: str | None = None
):
    """
    Plot summary of a single trade-study run.

    Parameters
    ----------
    results : dict
        Output of run_trade_study().
    title : str, optional
        Suptitle for the figure.
    save_path : str, optional
        If given, save figure to this path.
    """
    t = np.asarray(results["t_eval"])
    # Normalize time axis so we can read the x-axis more easily
    t0 = t[0]
    t_rel = t - t0

    pos_h = np.asarray(results["pos_err_hermite"]) * 1000.0  # mm
    pos_l = np.asarray(results["pos_err_lagrange"]) * 1000.0  # mm
    vel_h = np.asarray(results["vel_err_hermite"]) * 1000.0  # mm/s
    vel_l = np.asarray(results["vel_err_lagrange"]) * 1000.0  # mm/s

    summary = results["summary"]

    fig, axes = plt.subplots(2, 2, figsize=(11, 7), constrained_layout=True)

    # 1) Position error vs time
    ax = axes[0, 0]
    ax.plot(t_rel, pos_h, label="Hermite", linewidth=1.5)
    ax.plot(t_rel, pos_l, label="Lagrange", linewidth=1.5, linestyle="--")
    ax.set_ylabel("Position error [mm]")
    ax.set_xlabel("Relative time [s]")
    ax.set_title("Position error vs time")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 2) Velocity error vs time
    ax = axes[0, 1]
    ax.plot(t_rel, vel_h, label="Hermite", linewidth=1.5)
    ax.plot(t_rel, vel_l, label="Lagrange", linewidth=1.5, linestyle="--")
    ax.set_ylabel("Velocity error [mm/s]")
    ax.set_xlabel("Relative time [s]")
    ax.set_title("Velocity error vs time")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 3) Histograms of position error
    ax = axes[1, 0]
    bins = 30
    ax.hist(pos_h, bins=bins, alpha=0.5, label="Hermite")
    ax.hist(pos_l, bins=bins, alpha=0.5, label="Lagrange")
    ax.set_xlabel("Position error [mm]")
    ax.set_ylabel("Count")
    ax.set_title("Position error distribution")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 4) Text summary
    ax = axes[1, 1]
    ax.axis("off")

    text = f"""
Coarse sampling:
  dt_hi      = {summary['dt_hi']:.6f} s
  stride     = {summary['coarse_stride']}
  dt_coarse  = {summary['coarse_dt_est']:.6f} s

Position error (RMS / max):
  Hermite    = {summary['hermite_pos_rms']*1000:.3f} / {summary['hermite_pos_max']*1000:.3f} mm
  Lagrange   = {summary['lagrange_pos_rms']*1000:.3f} / {summary['lagrange_pos_max']*1000:.3f} mm

Velocity error (RMS / max):
  Hermite    = {summary['hermite_vel_rms']*1000:.3f} / {summary['hermite_vel_max']*1000:.3f} mm/s
  Lagrange   = {summary['lagrange_vel_rms']*1000:.3f} / {summary['lagrange_vel_max']*1000:.3f} mm/s
"""

    ax.text(
        0.02,
        0.98,
        text,
        va="top",
        ha="left",
        fontsize=9,
        family="monospace",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.4),
    )

    if title is not None:
        fig.suptitle(title, fontsize=14)

    if save_path is not None:
        fig.savefig(save_path, dpi=150, bbox_inches="tight")

    return fig


# =============================================================================
# Main
# =============================================================================


def main():
    """Run diagnostic on test orbits."""
    import argparse

    parser = argparse.ArgumentParser(description="Orbit quality diagnostic tool")
    parser.add_argument(
        "--slc",
        type=str,
        default="tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif",
        help="Path to SLC file",
    )
    parser.add_argument(
        "--coarse-dt", type=float, default=6, help="Coarse sampling interval in seconds"
    )
    parser.add_argument(
        "--save", type=str, default=None, help="Path to save diagnostic plot"
    )
    parser.add_argument("--no-show", action="store_true", help="Do not display plot")

    args = parser.parse_args()
    slc = CapellaSLC.from_file(args.slc)
    times, positions, velocities = slc.meta.collect.state.get_state()

    trade_result = run_trade_study(
        times, positions, velocities, coarse_dt=args.coarse_dt
    )

    print("Trade study results:")
    print(trade_result)
    plot_trade_study(trade_result, save_path="trade_study_results.png")

    return trade_result


if __name__ == "__main__":
    main()


---
docs/examples/orbit_quality_check.py
---
#!/usr/bin/env python3
"""Orbit Quality Diagnostic Tool.

This script checks whether orbit state vectors have velocity consistent with position,
which determines whether Hermite interpolation is appropriate.

Hermite interpolation constrains velocity to be the derivative of position.
If the input orbit has velocity that doesn't match dP/dt (common with some sensors like
Radarsat-2, Kompsat), Hermite will produce worse results than Lagrange.

Usage:

Run the script directly to see diagnostic plots:
    python orbit_quality_check.py --slc tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif

Or import and use with your own data:
    from orbit_quality_check import OrbitQualityChecker
    checker = OrbitQualityChecker(times, positions, velocities)
    checker.run_diagnostics()

"""

from dataclasses import dataclass

# =============================================================================
# Interpolators
# =============================================================================
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as np
from jax import lax
from numpy.typing import DTypeLike, NDArray

from capella_reader import CapellaSLC

# If you care about ~1e-9-ish tolerances, you probably want x64.
jax.config.update("jax_enable_x64", True)


def hermite_orbit_jax(tt, xx, vv, t):
    """
    tt: (n,)
    xx: (n,3)
    vv: (n,3)
    t:  scalar
    """
    tt = jnp.asarray(tt)
    xx = jnp.asarray(xx)
    vv = jnp.asarray(vv)
    t = jnp.asarray(t, dtype=tt.dtype)

    n = tt.shape[0]
    I = jnp.eye(n, dtype=bool)

    # Precompute pairwise differences Δ_ij = t_i - t_j
    # diff[i,j] = tt[i] - tt[j]
    diff = tt[:, None] - tt[None, :]  # (n,n)
    invdiff = jnp.where(I, 0.0, 1.0 / diff)  # (n,n)
    dl = jnp.sum(invdiff, axis=1)  # (n,)

    # Compute Lagrange basis values L_i(t)
    # L_i(t) = Π_{j≠i} (t - t_j)/(t_i - t_j)
    num = (t - tt)[None, :]  # (1,n)
    ratio = jnp.where(I, 1.0, num / diff)  # (n,n)
    L = jnp.prod(ratio, axis=1)  # (n,)

    # Compute L_i'(t) at the query time t (called hdot in ISCE/ROIPAC)
    # L_i'(t) = Σ_{j≠i} [ 1/(t_i - t_j) * Π_{k≠i,j} (t - t_k)/(t_i - t_k) ]
    dt_all = t - tt  # (n,)
    inv_dt = jnp.where(dt_all == 0, 0.0, 1.0 / dt_all)
    sum_inv_dt = jnp.sum(inv_dt)
    Ldot = L * (sum_inv_dt - inv_dt)  # (n,)

    dt = t - tt
    L2 = L * L

    # Hermite basis functions
    # α_i(t) = (1 - 2 (t - t_i) L_i'(t_i)) * L_i(t)^2
    # β_i(t) = (t - t_i) * L_i(t)^2
    alpha = (1.0 - 2.0 * dt * dl) * L2
    beta = dt * L2

    # Derivatives of basis functions (w.r.t. t)
    # α'_i(t) and β'_i(t) using d/dt L_i(t)^2 = 2 L_i(t) L_i'(t)
    dL2_dt = 2.0 * L * Ldot
    alpha_dot = (-2.0 * dl) * L2 + (1.0 - 2.0 * dt * dl) * dL2_dt
    beta_dot = L2 + dt * dL2_dt

    # Interpolate vector-valued position and velocity
    # p(t)  = Σ [ α_i x_i + β_i v_i ]
    # p'(t) = Σ [ α'_i x_i + β'_i v_i ]
    xout = alpha @ xx + beta @ vv
    vout = alpha_dot @ xx + beta_dot @ vv
    return xout, vout


def intp_orbit_jax(tt, xx, vv, t, *, win: int = 4):
    tt = jnp.asarray(tt)
    xx = jnp.asarray(xx)
    vv = jnp.asarray(vv)
    t = jnp.asarray(t, dtype=tt.dtype)

    n = tt.shape[0]

    # center window around t
    i = jnp.searchsorted(tt, t) - (win // 2)
    i = jnp.clip(i, 0, n - win)

    ttw = lax.dynamic_slice_in_dim(tt, i, win, axis=0)  # (win,)
    xxw = lax.dynamic_slice(xx, (i, jnp.int32(0)), (win, jnp.int32(3)))  # (win,3)
    vvw = lax.dynamic_slice(vv, (i, jnp.int32(0)), (win, jnp.int32(3)))  # (win,3)

    return hermite_orbit_jax(ttw, xxw, vvw, t)


hermite_orbit_jit = jax.jit(hermite_orbit_jax)
intp_orbit_jit = jax.jit(intp_orbit_jax, static_argnames=("win",))


hermite4_many_t = jax.vmap(hermite_orbit_jax, in_axes=(None, None, None, 0))


def hermite4_orbit_np(
    tt: NDArray, xx: NDArray, vv: NDArray, t: float, dtype: DTypeLike = np.float64
) -> tuple[NDArray, NDArray]:
    """4-node Hermite interpolation using position and velocity at each node.

    Parameters
    ----------
    tt : (4,) float64
        Node times (must be distinct).
    xx : (4,3) float64
        Position vectors at tt (ECEF meters).
    vv : (4,3) float64
        Velocity vectors at tt (ECEF m/s).
    t : float
        Query time.
    dtype : dtype_like
        Data type for internal calculations.
        Default is np.float64.

    Returns
    -------
    xout : (3,) float
        Interpolated position at time t.
    vout : (3,) float
        Interpolated velocity at time t (derivative of interpolant).

    """
    dtype = np.dtype(dtype)
    tt = np.asarray(tt, dtype=dtype)
    xx = np.asarray(xx, dtype=dtype)
    vv = np.asarray(vv, dtype=dtype)

    # if tt.shape != (4,) or xx.shape != (4, 3) or vv.shape != (4, 3):
    #     msg = "Expected tt:(4,), xx:(4,3), vv:(4,3)"
    #     raise ValueError(msg)
    # # TODO: this should move to a wrapper...
    # if np.any(~np.isfinite(tt)) or np.any(np.diff(np.sort(tt)) == 0):
    #     msg = "tt must be finite and distinct"
    #     raise ValueError(msg)

    # n = 4
    n = len(tt)

    # Precompute pairwise differences Δ_ij = t_i - t_j
    # diff[i,j] = tt[i] - tt[j]
    diff = tt[:, None] - tt[None, :]

    # For i != j: invdiff[i,j] = 1/(t_i - t_j), else 0
    invdiff = np.where(np.eye(n, dtype=bool), 0.0, 1.0 / diff)

    # dl[i] = L_i'(t_i) = Σ_{j≠i} 1/(t_i - t_j)
    dl = invdiff.sum(axis=1)

    # Compute Lagrange basis values L_i(t)
    # L_i(t) = Π_{j≠i} (t - t_j)/(t_i - t_j)
    L = np.ones(n, dtype=dtype)
    for i in range(n):
        for j in range(n):
            if j == i:
                continue
            L[i] *= (t - tt[j]) / (tt[i] - tt[j])

    # Compute L_i'(t) at the query time t (called hdot in ISCE/ROIPAC)
    # L_i'(t) = Σ_{j≠i} [ 1/(t_i - t_j) * Π_{k≠i,j} (t - t_k)/(t_i - t_k) ]
    Ldot = np.zeros(n, dtype=dtype)
    for i in range(n):
        s = 0.0
        for j in range(n):
            if j == i:
                continue
            prod = 1.0 / (tt[i] - tt[j])
            for k in range(n):
                if k in (i, j):
                    continue
                prod *= (t - tt[k]) / (tt[i] - tt[k])
            s += prod
        Ldot[i] = s

    dt = t - tt
    L2 = L * L

    # Hermite basis functions
    # α_i(t) = (1 - 2 (t - t_i) L_i'(t_i)) * L_i(t)^2
    # β_i(t) = (t - t_i) * L_i(t)^2
    alpha = (1.0 - 2.0 * dt * dl) * L2
    beta = dt * L2

    # Derivatives of basis functions (w.r.t. t)
    # α'_i(t) and β'_i(t) using d/dt L_i(t)^2 = 2 L_i(t) L_i'(t)
    dL2_dt = 2.0 * L * Ldot

    alpha_dot = (-2.0 * dl) * L2 + (1.0 - 2.0 * dt * dl) * dL2_dt
    beta_dot = 1.0 * L2 + dt * dL2_dt

    # Interpolate vector-valued position and velocity
    # p(t)  = Σ [ α_i x_i + β_i v_i ]
    # p'(t) = Σ [ α'_i x_i + β'_i v_i ]
    xout = alpha @ xx + beta @ vv
    vout = alpha_dot @ xx + beta_dot @ vv

    return xout, vout


@dataclass
class HermiteInterpolator:
    """Piecewise cubic Hermite interpolation.

    Uses position AND velocity at nodes, guaranteeing that the interpolated
    velocity equals the derivative of interpolated position.
    """

    """times: (N,) array of times in seconds"""
    times: np.ndarray
    """positions: (N, 3) array of positions in meters"""
    positions: np.ndarray
    """velocities: (N, 3) array of velocities in m/s"""
    velocities: np.ndarray
    """window size for interpolation"""
    win: int = 4

    def _find_interval(self, t: float) -> int:
        """Find the interval index for time t."""
        idx = np.searchsorted(self.times, t) - 1
        return np.clip(idx, 0, len(self.times) - 2)

    def interpolate(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate position and velocity at time t.

        Returns: (position, velocity) arrays
        """
        # idx = self._find_interval(t)
        # subset = slice(idx - 2, idx + 2)
        # tt = self.times[subset]
        # if tt.shape != (4,):
        #     raise ValueError(f"{t = } must have +/-2 elements in {self.times}")
        # xx = self.positions[subset]
        # vv = self.velocities[subset]
        # return hermite4_orbit(tt, xx, vv, t)
        return intp_orbit_jax(
            self.times, self.positions, self.velocities, t, win=self.win
        )

    def interpolate_many(self, t_arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate for a range of times."""

    def interpolate2(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate position and velocity at time t.

        Returns: (position, velocity) arrays
        """
        import orbit

        return orbit.intp_orbit(
            self.times, self.positions, self.velocities, t, win=self.win
        )


@dataclass
class LagrangeInterpolator:
    """Lagrange polynomial interpolation (what ISCE calls "Legendre").

    Interpolates position and velocity independently using sliding windows.
    No constraint that velocity equals derivative of position.
    """

    """times: (N,) array of times in seconds"""
    times: np.ndarray
    """positions: (N, 3) array of positions in meters"""
    positions: np.ndarray
    """velocities: (N, 3) array of velocities in m/s"""
    velocities: np.ndarray
    """Number of points to use for interpolation (default 9 for degree-8)"""
    n_points: int = 9

    def _select_window(self, t: float) -> tuple[np.ndarray, int]:
        """Select n_points centered around time t, return times and start index."""
        idx = np.searchsorted(self.times, t)
        half = self.n_points // 2
        start = max(0, min(idx - half, len(self.times) - self.n_points))
        return self.times[start : start + self.n_points], start

    def _lagrange_interp(
        self, t: float, t_nodes: np.ndarray, values: np.ndarray
    ) -> np.ndarray:
        """Lagrange interpolation for vector values."""
        n = len(t_nodes)
        result = np.zeros(3)

        for k in range(n):
            # Compute Lagrange basis L_k(t)
            Lk = 1.0
            for j in range(n):
                if j != k:
                    Lk *= (t - t_nodes[j]) / (t_nodes[k] - t_nodes[j])
            result += values[k] * Lk

        return result

    def interpolate(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """Interpolate position and velocity at time t.

        Note: Position and velocity are interpolated independently.
        """
        t_win, start = self._select_window(t)
        pos_win = self.positions[start : start + self.n_points]
        vel_win = self.velocities[start : start + self.n_points]

        position = self._lagrange_interp(t, t_win, pos_win)
        velocity = self._lagrange_interp(t, t_win, vel_win)

        return position, velocity

    def interpolate_with_pos_derivative(
        self, t: float, dt: float = 1e-3
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Interpolate and also compute velocity from position derivative.

        Returns: (position, velocity_from_fit, velocity_from_derivative)
        """
        position, velocity_fit = self.interpolate(t)

        # Numerical derivative of position
        pos_plus, _ = self.interpolate(t + dt / 2)
        pos_minus, _ = self.interpolate(t - dt / 2)
        velocity_deriv = (pos_plus - pos_minus) / dt

        return position, velocity_fit, velocity_deriv


# =============================================================================
# Orbit Quality Checker
# =============================================================================


def run_trade_study(
    t: np.ndarray,
    pos_hi: np.ndarray,
    vel_hi: np.ndarray,
    coarse_dt: float = 10.0,
    lagrange_n_points: int = 9,
) -> dict:
    """Trade study: downsample a high-rate 'truth' orbit, interpolate back.

    Compares Hermite vs Lagrange to the high-rate truth.

    Parameters
    ----------
    t : (N,) array
        High-rate times (in seconds)
    pos_hi : (N, 3) array
        High-rate positions (meters).
    vel_hi : (N, 3) array
        High-rate velocities (m/s).
    coarse_dt : float, default 10.0
        Target coarse sampling interval in seconds (e.g. 10 s).
    lagrange_n_points : int, default 9
        Number of points for Lagrange interpolation (degree = n_points - 1).

    Returns
    -------
    results : dict
        Contains per-method position/velocity errors vs the high-rate truth.

    """
    pos_hi = np.asarray(pos_hi)
    vel_hi = np.asarray(vel_hi)

    # --- Choose coarse sampling stride from desired coarse_dt ---
    dt_hi = np.median(np.diff(t))
    stride = max(1, round(coarse_dt / dt_hi))

    t_coarse = t[::stride]
    pos_coarse = pos_hi[::stride]
    vel_coarse = vel_hi[::stride]

    # Build interpolators from coarse orbit
    hermite = HermiteInterpolator(t_coarse, pos_coarse, vel_coarse, win=4)
    lagrange = LagrangeInterpolator(
        t_coarse, pos_coarse, vel_coarse, n_points=lagrange_n_points
    )

    # Restrict evaluation to the interior where Lagrange has a full window
    margin_coarse = max(lagrange_n_points // 2, hermite.win // 2)
    print(f"{margin_coarse = }")
    margin = margin_coarse * stride
    t_eval = t[margin:-margin]
    pos_truth = pos_hi[margin:-margin]
    vel_truth = vel_hi[margin:-margin]

    pos_err_herm = []
    vel_err_herm = []
    pos_err_lagr = []
    vel_err_lagr = []

    for ti, p_true, v_true in zip(t_eval, pos_truth, vel_truth, strict=True):
        p_h, v_h = hermite.interpolate(ti)
        p_l, v_l = lagrange.interpolate(ti)

        pos_err_herm.append(np.linalg.norm(p_h - p_true))
        vel_err_herm.append(np.linalg.norm(v_h - v_true))

        pos_err_lagr.append(np.linalg.norm(p_l - p_true))
        vel_err_lagr.append(np.linalg.norm(v_l - v_true))

    pos_err_herm = np.array(pos_err_herm)
    vel_err_herm = np.array(vel_err_herm)
    pos_err_lagr = np.array(pos_err_lagr)
    vel_err_lagr = np.array(vel_err_lagr)

    return {
        "t_eval": t_eval,
        "pos_err_hermite": pos_err_herm,
        "vel_err_hermite": vel_err_herm,
        "pos_err_lagrange": pos_err_lagr,
        "vel_err_lagrange": vel_err_lagr,
        "summary": {
            "hermite_pos_rms": np.sqrt(np.mean(pos_err_herm**2)),
            "hermite_pos_max": np.max(pos_err_herm),
            "hermite_vel_rms": np.sqrt(np.mean(vel_err_herm**2)),
            "hermite_vel_max": np.max(vel_err_herm),
            "lagrange_pos_rms": np.sqrt(np.mean(pos_err_lagr**2)),
            "lagrange_pos_max": np.max(pos_err_lagr),
            "lagrange_vel_rms": np.sqrt(np.mean(vel_err_lagr**2)),
            "lagrange_vel_max": np.max(vel_err_lagr),
            "coarse_stride": stride,
            "coarse_dt_est": dt_hi * stride,
            "dt_hi": dt_hi,
        },
    }


def plot_trade_study(
    results: dict, title: str | None = None, save_path: str | None = None
):
    """
    Plot summary of a single trade-study run.

    Parameters
    ----------
    results : dict
        Output of run_trade_study().
    title : str, optional
        Suptitle for the figure.
    save_path : str, optional
        If given, save figure to this path.
    """
    t = np.asarray(results["t_eval"])
    # Normalize time axis so we can read the x-axis more easily
    t0 = t[0]
    t_rel = t - t0

    pos_h = np.asarray(results["pos_err_hermite"]) * 1000.0  # mm
    pos_l = np.asarray(results["pos_err_lagrange"]) * 1000.0  # mm
    vel_h = np.asarray(results["vel_err_hermite"]) * 1000.0  # mm/s
    vel_l = np.asarray(results["vel_err_lagrange"]) * 1000.0  # mm/s

    summary = results["summary"]

    fig, axes = plt.subplots(2, 2, figsize=(11, 7), constrained_layout=True)

    # 1) Position error vs time
    ax = axes[0, 0]
    ax.plot(t_rel, pos_h, label="Hermite", linewidth=1.5)
    ax.plot(t_rel, pos_l, label="Lagrange", linewidth=1.5, linestyle="--")
    ax.set_ylabel("Position error [mm]")
    ax.set_xlabel("Relative time [s]")
    ax.set_title("Position error vs time")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 2) Velocity error vs time
    ax = axes[0, 1]
    ax.plot(t_rel, vel_h, label="Hermite", linewidth=1.5)
    ax.plot(t_rel, vel_l, label="Lagrange", linewidth=1.5, linestyle="--")
    ax.set_ylabel("Velocity error [mm/s]")
    ax.set_xlabel("Relative time [s]")
    ax.set_title("Velocity error vs time")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 3) Histograms of position error
    ax = axes[1, 0]
    bins = 30
    ax.hist(pos_h, bins=bins, alpha=0.5, label="Hermite")
    ax.hist(pos_l, bins=bins, alpha=0.5, label="Lagrange")
    ax.set_xlabel("Position error [mm]")
    ax.set_ylabel("Count")
    ax.set_title("Position error distribution")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # 4) Text summary
    ax = axes[1, 1]
    ax.axis("off")

    text = f"""
Coarse sampling:
  dt_hi      = {summary['dt_hi']:.6f} s
  stride     = {summary['coarse_stride']}
  dt_coarse  = {summary['coarse_dt_est']:.6f} s

Position error (RMS / max):
  Hermite    = {summary['hermite_pos_rms']*1000:.3f} / {summary['hermite_pos_max']*1000:.3f} mm
  Lagrange   = {summary['lagrange_pos_rms']*1000:.3f} / {summary['lagrange_pos_max']*1000:.3f} mm

Velocity error (RMS / max):
  Hermite    = {summary['hermite_vel_rms']*1000:.3f} / {summary['hermite_vel_max']*1000:.3f} mm/s
  Lagrange   = {summary['lagrange_vel_rms']*1000:.3f} / {summary['lagrange_vel_max']*1000:.3f} mm/s
"""

    ax.text(
        0.02,
        0.98,
        text,
        va="top",
        ha="left",
        fontsize=9,
        family="monospace",
        bbox={"boxstyle": "round", "facecolor": "wheat", "alpha": 0.4},
    )

    if title is not None:
        fig.suptitle(title, fontsize=14)

    if save_path is not None:
        fig.savefig(save_path, dpi=150, bbox_inches="tight")

    return fig


# =============================================================================
# Main
# =============================================================================


def main():
    """Run diagnostic on test orbits."""
    import argparse

    parser = argparse.ArgumentParser(description="Orbit quality diagnostic tool")
    parser.add_argument(
        "--slc",
        type=str,
        default="tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif",
        help="Path to SLC file",
    )
    parser.add_argument(
        "--coarse-dt", type=float, default=6, help="Coarse sampling interval in seconds"
    )
    parser.add_argument(
        "--save", type=str, default=None, help="Path to save diagnostic plot"
    )
    parser.add_argument("--no-show", action="store_true", help="Do not display plot")

    args = parser.parse_args()
    slc = CapellaSLC.from_file(args.slc)
    times, positions, velocities = slc.meta.collect.state.get_state()

    trade_result = run_trade_study(
        times, positions, velocities, coarse_dt=args.coarse_dt
    )

    print("Trade study results:")
    # print(trade_result)
    plot_trade_study(trade_result, save_path="trade_study_results.png")

    return trade_result


if __name__ == "__main__":
    main()


---
docs/examples/scene_footprint_with_orbit.py
---
"""Combined visualization of satellite orbit track and image footprint.

This script creates a comprehensive map showing:
- Satellite ground track with time annotations
- Image footprint polygon
- Scene center and imaging geometry
- Geographic context with coastlines and borders
"""

from pathlib import Path

import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import numpy as np
import pyproj
from shapely import Polygon
from shapely.plotting import plot_polygon

from capella_reader.slc import CapellaSLC


def ecef_to_latlon(positions):
    """Convert ECEF positions to latitude/longitude.

    Parameters
    ----------
    positions : np.ndarray
        Nx3 array of ECEF positions [x, y, z] in meters

    Returns
    -------
    lats : np.ndarray
        Latitudes in degrees
    lons : np.ndarray
        Longitudes in degrees
    alts : np.ndarray
        Altitudes in meters above WGS84 ellipsoid

    """
    ecef = pyproj.Proj(proj="geocent", ellps="WGS84", datum="WGS84")
    lla = pyproj.Proj(proj="latlong", ellps="WGS84", datum="WGS84")
    transformer = pyproj.Transformer.from_proj(ecef, lla, always_xy=True)

    lons, lats, alts = transformer.transform(
        positions[:, 0], positions[:, 1], positions[:, 2]
    )

    return np.array(lats), np.array(lons), np.array(alts)


def main():
    metadata_file = (
        Path(__file__).parent.parent.parent
        / "tests"
        / "data"
        / "CAPELLA_C13_SP_SLC_HH_20241126045307_20241126045346_extended.json"
        # / "CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif"
    )

    assert metadata_file.exists(), f"Test data not found: {metadata_file}"

    print(f"Loading metadata from {metadata_file.name}...")
    slc = CapellaSLC.from_file(metadata_file)
    times, positions, velocities = slc.meta.collect.state.get_state()
    times_python = slc.meta.collect.state.get_state(time_as_float=False)[0]

    meta = slc.meta

    print(f"Found {len(times)} state vectors")
    print(f"Time span: {times[0]} to {times[-1]}")

    sat_lats, sat_lons, sat_alts = ecef_to_latlon(positions)

    slc_gcps = slc.gcps
    lon = np.array([gcp.x for gcp in slc_gcps])
    lat = np.array([gcp.y for gcp in slc_gcps])

    # image_corners_ecef = calculate_image_corners_ecef(meta)
    # corner_lats, corner_lons, _ = ecef_to_latlon(image_corners_ecef)

    center_ecef = np.array(meta.collect.image.center_pixel.target_position.as_array())
    center_lats, center_lons, _ = ecef_to_latlon(center_ecef.reshape(1, -1))

    image_center_time = meta.collect.image.center_pixel.center_time
    center_idx = len(times) // 2

    print("\nOrbit Information:")
    print(f"  Direction: {meta.collect.state.direction}")
    print(f"  Mean altitude: {sat_alts.mean() / 1000:.1f} km")
    print(f"  Mean velocity: {np.linalg.norm(velocities, axis=1).mean():.0f} m/s")
    print("\nImage Information:")
    print(f"  Size: {meta.collect.image.rows} x {meta.collect.image.columns} pixels")
    print(
        f"  Dimensions: {meta.collect.image.length:.1f} x"
        f" {meta.collect.image.width:.1f} m"
    )
    print(
        f"  Incidence angle: {meta.collect.image.center_pixel.incidence_angle:.1f} deg"
    )
    print(f"  Center time: {image_center_time}")

    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())

    lat_margin = 1.0
    lon_margin = 1.0
    ax.set_extent(
        [
            min(sat_lons.min(), lon.min()) - lon_margin,
            max(sat_lons.max(), lon.max()) + lon_margin,
            min(sat_lats.min(), lat.min()) - lat_margin,
            max(sat_lats.max(), lat.max()) + lat_margin,
        ],
        crs=ccrs.PlateCarree(),
    )

    ax.add_feature(cfeature.LAND, facecolor="wheat", alpha=0.4)
    ax.add_feature(cfeature.OCEAN, facecolor="lightblue", alpha=0.4)
    ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor="black")
    ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle=":", edgecolor="gray")
    ax.add_feature(cfeature.LAKES, facecolor="lightblue", alpha=0.3)

    gl = ax.gridlines(
        draw_labels=True, dms=True, x_inline=False, y_inline=False, alpha=0.5
    )
    gl.top_labels = False
    gl.right_labels = False

    ax.plot(
        sat_lons,
        sat_lats,
        "r-",
        linewidth=2.5,
        label=f"Ground Track ({meta.collect.state.direction})",
        transform=ccrs.Geodetic(),
        zorder=3,
    )

    annotation_stride = len(sat_lons) // 5
    for i in range(0, len(sat_lons), annotation_stride):
        time_str = str(times_python[i])
        ax.annotate(
            time_str,
            xy=(sat_lons[i], sat_lats[i]),
            xytext=(5, 5),
            textcoords="offset points",
            fontsize=8,
            bbox={"boxstyle": "round,pad=0.3", "facecolor": "yellow", "alpha": 0.7},
            transform=ccrs.PlateCarree(),
            zorder=4,
        )

    ax.scatter(
        sat_lons[0],
        sat_lats[0],
        c="green",
        s=150,
        marker="o",
        edgecolors="black",
        linewidths=1,
        label="Orbit Start",
        transform=ccrs.Geodetic(),
        zorder=5,
    )
    ax.scatter(
        sat_lons[-1],
        sat_lats[-1],
        c="darkred",
        s=150,
        marker="s",
        edgecolors="black",
        linewidths=1,
        label="Orbit End",
        transform=ccrs.Geodetic(),
        zorder=5,
    )

    ax.scatter(
        sat_lons[center_idx],
        sat_lats[center_idx],
        c="orange",
        s=200,
        marker="D",
        edgecolors="black",
        linewidths=1,
        label="Satellite at Image Center Time",
        transform=ccrs.Geodetic(),
        zorder=5,
    )

    polygon = Polygon(zip(lon, lat, strict=False))
    footprint = polygon.convex_hull
    plot_polygon(footprint, transform=ccrs.Geodetic())

    ax.scatter(
        center_lons[0],
        center_lats[0],
        c="blue",
        s=250,
        marker="*",
        edgecolors="black",
        linewidths=1,
        label="Scene Center",
        transform=ccrs.Geodetic(),
        zorder=5,
    )

    ax.plot(
        [sat_lons[center_idx], center_lons[0]],
        [sat_lats[center_idx], center_lats[0]],
        "k--",
        linewidth=1.5,
        alpha=0.6,
        label="Line-of-Sight",
        transform=ccrs.Geodetic(),
        zorder=2,
    )

    ax.legend(loc="upper left", fontsize=10, framealpha=0.9)

    title_text = (
        "Capella SAR Orbit and Image Footprint\n"
        f"Platform: Capella-{meta.collect.platform} | "
        f"Mode: {meta.collect.mode} | "
        f"Polarization: {slc.polarization}\n"
        f"Collection: {image_center_time!s} UTC | "
        f"Altitude: {sat_alts.mean() / 1000:.1f} km"
    )
    ax.set_title(title_text, fontsize=13, fontweight="bold", pad=20)

    plt.tight_layout()
    output_file = Path(__file__).parent / "orbit_footprint_map.png"
    plt.savefig(output_file, dpi=150, bbox_inches="tight")
    print(f"\nSaved figure to {output_file}")

    plt.show()


if __name__ == "__main__":
    main()


---
src/capella_reader/__init__.py
---
"""Copyright (c) 2025 Scott Staniewicz. All rights reserved.

capella-reader: Basic models and io for Capella SLCs
"""

from __future__ import annotations

from capella_reader._time import Time, TimeDelta
from capella_reader._version import version as __version__
from capella_reader.collect import Collect
from capella_reader.geometry import (
    AttitudeQuaternion,
    ECEFPosition,
    ECEFVelocity,
)
from capella_reader.image import (
    CenterPixel,
    ImageGeometry,
    ImageMetadata,
    Quantization,
    TerrainModelRef,
    TerrainModels,
    Window,
)
from capella_reader.metadata import CapellaSLCMetadata
from capella_reader.orbit import (
    Antenna,
    CoordinateSystem,
    PointingSample,
    State,
    StateVector,
)
from capella_reader.polynomials import Poly1D, Poly2D
from capella_reader.radar import PRFEntry, Radar, RadarTimeVaryingParams
from capella_reader.slc import CapellaSLC

__all__ = [
    "Antenna",
    "AttitudeQuaternion",
    "CapellaSLC",
    "CapellaSLCMetadata",
    "CenterPixel",
    "Collect",
    "CoordinateSystem",
    "ECEFPosition",
    "ECEFVelocity",
    "ImageGeometry",
    "ImageMetadata",
    "PRFEntry",
    "PointingSample",
    "Poly1D",
    "Poly2D",
    "Quantization",
    "Radar",
    "RadarTimeVaryingParams",
    "State",
    "StateVector",
    "TerrainModelRef",
    "TerrainModels",
    "Time",
    "TimeDelta",
    "Window",
    "__version__",
]


---
src/capella_reader/_time.py
---
from __future__ import annotations

from typing import TYPE_CHECKING, Any

import numpy as np

if TYPE_CHECKING:
    from pydantic_core import core_schema


class TimeDelta:
    """Wrapper around np.timedelta64 for nanosecond-precision time differences."""

    __slots__ = ("_td",)

    def __init__(self, td: np.timedelta64) -> None:
        self._td = np.timedelta64(td, "ns")

    def total_seconds(self) -> float:
        """Return total seconds as float."""
        return self._td / np.timedelta64(1, "s")

    def __repr__(self) -> str:
        return f"TimeDelta({self._td})"

    def as_numpy(self) -> np.timedelta64:
        """Return underlying numpy timedelta64."""
        return self._td


class Time:
    """Wrapper around np.datetime64 for nanosecond-precision timestamps.

    Provides proper Pydantic serialization and arithmetic with nanosecond precision.
    """

    __slots__ = ("_dt",)

    def __init__(self, dt: str | np.datetime64) -> None:
        if isinstance(dt, str):
            dt = dt.rstrip("Z")
            self._dt = np.datetime64(dt, "ns")
        else:
            self._dt = np.datetime64(dt, "ns")  # type: ignore[call-overload]

    def __sub__(self, other: Time) -> TimeDelta:
        return TimeDelta(self._dt - other._dt)

    def __add__(self, delta: np.timedelta64) -> Time:
        return Time(self._dt + np.timedelta64(delta, "ns"))

    def __repr__(self) -> str:
        return f"Time({self._dt})"

    def __eq__(self, other: Time) -> bool:  # type: ignore[override]
        return self._dt == other._dt

    def __hash__(self) -> int:
        return hash(self._dt)

    def __lt__(self, other: Time) -> bool:
        return bool(self._dt < other._dt)

    def __str__(self) -> str:
        return np.datetime_as_string(self._dt, timezone="UTC").item()

    def as_numpy(self) -> np.datetime64:
        """Return underlying numpy datetime64."""
        return self._dt

    def as_datetime(self):
        """Return as Python datetime object.

        Returns
        -------
        datetime.datetime
            Python datetime object in UTC timezone.

        """
        import datetime

        # Convert to UTC timestamp in seconds
        timestamp_ns = (self._dt - np.datetime64("1970-01-01T00:00:00", "ns")).astype(
            int
        )
        timestamp_s = timestamp_ns / 1e9
        return datetime.datetime.fromtimestamp(timestamp_s, tz=datetime.timezone.utc)

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: Any
    ) -> core_schema.CoreSchema:
        from pydantic_core import core_schema

        def validate(value: str | np.datetime64 | Time) -> Time:
            if isinstance(value, Time):
                return value
            return Time(value)

        def serialize(value: Time) -> str:
            return np.datetime_as_string(value._dt, timezone="UTC").item()

        python_schema = core_schema.with_info_plain_validator_function(
            lambda v, _: validate(v)
        )

        return core_schema.json_or_python_schema(
            json_schema=core_schema.no_info_after_validator_function(
                validate, core_schema.str_schema()
            ),
            python_schema=python_schema,
            serialization=core_schema.plain_serializer_function_ser_schema(
                serialize, return_schema=core_schema.str_schema()
            ),
        )


---
src/capella_reader/_types.py
---
from __future__ import annotations

from types import EllipsisType
from typing import TypeAlias

Index: TypeAlias = int | slice | EllipsisType


---
src/capella_reader/_version.py
---
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = [
    "__version__",
    "__version_tuple__",
    "version",
    "version_tuple",
    "__commit_id__",
    "commit_id",
]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
    COMMIT_ID = Union[str, None]
else:
    VERSION_TUPLE = object
    COMMIT_ID = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE
commit_id: COMMIT_ID
__commit_id__: COMMIT_ID

__version__ = version = '0.0.post1.dev80+g7426bc8d5'
__version_tuple__ = version_tuple = (0, 0, 'post1', 'dev80', 'g7426bc8d5')

__commit_id__ = commit_id = None


---
src/capella_reader/collect.py
---
"""Collect metadata block."""

from __future__ import annotations

from pydantic import BaseModel, Field

from capella_reader._time import Time
from capella_reader.image import ImageMetadata
from capella_reader.orbit import Antenna, PointingSample, State
from capella_reader.radar import Radar


class Collect(BaseModel):
    """Collect-level metadata."""

    start_timestamp: Time = Field(..., description="Collect start time (UTC)")
    stop_timestamp: Time = Field(..., description="Collect stop time (UTC)")

    local_datetime: str = Field(
        ...,
        description="String containing formatted local time at scene center",
    )
    local_timezone: str = Field(
        ..., description="IANA or offset representation of local timezone"
    )

    platform: str = Field(..., description="Satellite identifier (e.g. 'capella-14')")
    mode: str = Field(..., description="Imaging mode (e.g. 'spotlight')")
    collect_id: str = Field(..., description="Unique collect identifier")

    image: ImageMetadata
    radar: Radar
    state: State
    pointing: list[PointingSample]
    transmit_antenna: Antenna
    receive_antenna: Antenna


---
src/capella_reader/enums.py
---
from enum import Enum


class LookSide(str, Enum):
    """Look side of the platform."""

    LEFT = "left"
    RIGHT = "right"

    def __int__(self) -> int:
        return 1 if self is LookSide.LEFT else -1

    @classmethod
    def _missing_(cls, value):
        if isinstance(value, str):
            value = value.lower()
            if value == cls.LEFT.value:
                return cls.LEFT
            if value == cls.RIGHT.value:
                return cls.RIGHT
        elif value == 1:
            return cls.LEFT
        elif value == -1:
            return cls.RIGHT
        return None


---
src/capella_reader/geometry.py
---
"""Geometric types: ECEF position, velocity, and attitude quaternions."""

from __future__ import annotations

import numpy as np
from pydantic import BaseModel, Field


class ECEFPosition(BaseModel):
    """Earth-Centered Earth-Fixed (ECEF) position in meters."""

    x: float = Field(..., description="ECEF X coordinate [m]")
    y: float = Field(..., description="ECEF Y coordinate [m]")
    z: float = Field(..., description="ECEF Z coordinate [m]")

    @classmethod
    def from_list(cls, v: list[float]) -> ECEFPosition:
        """Create from a list of [x, y, z]."""
        if len(v) != 3:
            msg = "ECEFPosition expects 3 elements [x, y, z]"
            raise ValueError(msg)
        return cls(x=v[0], y=v[1], z=v[2])

    def as_array(self) -> np.ndarray:
        """Return as numpy array."""
        return np.array([self.x, self.y, self.z], dtype=float)

    def __sub__(self, other: ECEFPosition) -> ECEFPosition:
        """Subtract another ECEFPosition from this one."""
        return ECEFPosition(x=self.x - other.x, y=self.y - other.y, z=self.z - other.z)

    def __add__(self, other: ECEFPosition) -> ECEFPosition:
        """Add another ECEFPosition to this one."""
        return ECEFPosition(x=self.x + other.x, y=self.y + other.y, z=self.z + other.z)

    def norm(self) -> float:
        """Return the Euclidean norm of the position vector."""
        return float(np.linalg.norm(self.as_array()))


class ECEFVelocity(BaseModel):
    """ECEF velocity in meters per second."""

    vx: float = Field(..., description="ECEF X velocity [m/s]")
    vy: float = Field(..., description="ECEF Y velocity [m/s]")
    vz: float = Field(..., description="ECEF Z velocity [m/s]")

    @classmethod
    def from_list(cls, v: list[float]) -> ECEFVelocity:
        """Create from a list of [vx, vy, vz]."""
        if len(v) != 3:
            msg = "ECEFVelocity expects 3 elements [vx, vy, vz]"
            raise ValueError(msg)
        return cls(vx=v[0], vy=v[1], vz=v[2])

    def as_array(self) -> np.ndarray:
        """Return as numpy array."""
        return np.array([self.vx, self.vy, self.vz], dtype=float)

    def __add__(self, other: ECEFVelocity) -> ECEFVelocity:
        """Add another ECEFVelocity to this one."""
        return ECEFVelocity(
            vx=self.vx + other.vx, vy=self.vy + other.vy, vz=self.vz + other.vz
        )

    def __sub__(self, other: ECEFVelocity) -> ECEFVelocity:
        """Subtract another ECEFVelocity from this one."""
        return ECEFVelocity(
            vx=self.vx - other.vx, vy=self.vy - other.vy, vz=self.vz - other.vz
        )

    def norm(self) -> float:
        """Return the Euclidean norm of the velocity vector."""
        return float(np.linalg.norm(self.as_array()))


class AttitudeQuaternion(BaseModel):
    """Spacecraft attitude quaternion components (order as given in metadata)."""

    q0: float = Field(..., description="Quaternion component q0")
    q1: float = Field(..., description="Quaternion component q1")
    q2: float = Field(..., description="Quaternion component q2")
    q3: float = Field(..., description="Quaternion component q3")

    @classmethod
    def from_list(cls, v: list[float]) -> AttitudeQuaternion:
        """Create from a list of [q0, q1, q2, q3]."""
        if len(v) != 4:
            msg = "AttitudeQuaternion expects 4 elements"
            raise ValueError(msg)
        return cls(q0=v[0], q1=v[1], q2=v[2], q3=v[3])

    def as_array(self) -> np.ndarray:
        """Return as numpy array."""
        return np.array([self.q0, self.q1, self.q2, self.q3], dtype=float)


---
src/capella_reader/image.py
---
"""Image metadata: geometry, windows, quantization, terrain models."""

from __future__ import annotations

from collections.abc import Mapping
from typing import Annotated, Any, Literal

import numpy as np
from pydantic import BaseModel, ConfigDict, Field, field_validator

from capella_reader._time import Time
from capella_reader.geometry import ECEFPosition
from capella_reader.polynomials import Poly1D, Poly2D


class Window(BaseModel):
    """Window function parameters."""

    name: str = Field(..., description="Window function name (e.g. 'rectangular')")
    parameters: Mapping[str, float] = Field(
        default_factory=dict,
        description="Window parameters (implementation-specific numeric values)",
    )
    broadening_factor: float = Field(
        ...,
        description="Main-lobe broadening factor due to windowing",
    )


class Quantization(BaseModel):
    """Quantization scheme parameters."""

    type: str = Field(
        ..., description="Quantization scheme, e.g. 'block_adaptive_quantization'"
    )
    block_sample_size: int = Field(..., description="Samples per block")
    mean_bits: int = Field(..., description="Bits used for block mean")
    std_bits: int = Field(..., description="Bits used for block std-dev")
    sample_bits: int = Field(..., description="Bits per quantized sample")


class TerrainModelRef(BaseModel):
    """Reference to a terrain model."""

    link: str = Field(..., description="Reference or URL describing terrain model")
    name: str = Field(..., description="Terrain model name / identifier")


class TerrainModels(BaseModel):
    """Collection of terrain model references."""

    focusing: TerrainModelRef | None = Field(
        None,
        description="Terrain model used for focusing / geolocation",
    )


class SlantPlaneGeometry(BaseModel):
    """Geometry parameters for slant-plane focused products."""

    type: Literal["slant_plane"] = Field(
        ..., description="Image geometry type (slant_plane)"
    )
    doppler_centroid_polynomial: Poly2D = Field(
        ...,
        description=(
            "A 2D polynomial mapping range and azimuth time to doppler centroid "
            "frequency in Hz used to compute the image geometry. Notice that the "
            "range dependence of the DC polynomial uses range distance. The azimuth "
            "variable is seconds since first_line_time."
        ),
    )
    first_line_time: Time = Field(
        ...,
        description="The timestamp of the first line",
    )
    delta_line_time: float = Field(
        ...,
        description="The time difference between successive lines in seconds",
    )
    range_to_first_sample: float = Field(
        ...,
        description="The slant range distance to the first sample in meters",
    )
    delta_range_sample: float = Field(
        ...,
        description="The slant range delta distance between each sample in meters",
    )


class PFAGeometry(BaseModel):
    """Geometry parameters for Polar Format Algorithm (PFA) products.

    PFA products use a different coordinate system and have many additional
    fields that are preserved via extra='allow'.
    """

    model_config = ConfigDict(extra="allow")

    type: Literal["pfa"] = Field(..., description="Image geometry type (pfa)")
    scene_reference_point_row_col: tuple[float, float] = Field(
        ...,
        description=(
            "The row and column at the scene reference point in units of pixels"
        ),
    )
    scene_reference_point_ecef: ECEFPosition = Field(
        ..., description="The scene reference point in ECEF meters"
    )
    row_sample_spacing: float = Field(
        ..., description="The sample spacing in meters in the row direction"
    )
    col_sample_spacing: float = Field(
        ..., description="The samples spacing in meters in the column direction"
    )
    row_direction: tuple[float, float, float] = Field(
        ..., description="A unit vector in ECEF indicating the row direction"
    )
    col_direction: tuple[float, float, float] = Field(
        ..., description="A unit vector in ECEF indicating the column direction"
    )
    slant_plane_normal: tuple[float, float, float] = Field(
        ...,
        description=(
            "A 3D unit vector in ECEF describing the direction normal to the slant"
            " plane"
        ),
    )
    ground_plane_normal: tuple[float, float, float] = Field(
        ...,
        description=(
            "A 3D unit vector in ECEF describing the direction normal to the ground"
            " plane"
        ),
    )
    polar_angle_polynomial: Poly1D = Field(
        ...,
        description=(
            "A 1D polynomial mapping seconds since collect_start to polar angle in"
            " radians"
        ),
    )
    spatial_frequency_scale_factor_polynomial: Poly1D = Field(
        ...,
        description=(
            "A 1D polynomial mapping polar angle in radians to Spatial Frequency Scale"
            " Factor. Used to scale RF frequency to aperture spatial frequency."
        ),
    )

    @field_validator("scene_reference_point_ecef", mode="before")
    @classmethod
    def _parse_ecef(cls, v: Any) -> ECEFPosition:
        if isinstance(v, list):
            return ECEFPosition.from_list(v)
        return v


ImageGeometry = Annotated[
    SlantPlaneGeometry | PFAGeometry,
    Field(discriminator="type"),
]


class CenterPixel(BaseModel):
    """Scene center pixel metadata."""

    incidence_angle: float = Field(
        ..., description="Incidence angle at scene center [deg]"
    )
    look_angle: float = Field(..., description="Look angle at scene center [deg]")
    squint_angle: float = Field(..., description="Squint angle at scene center [deg]")
    layover_angle: float | None = Field(
        None,
        description="Layover angle at scene center [deg]; null if not provided",
    )
    target_position: ECEFPosition = Field(
        ...,
        description="ECEF coordinates of scene center target [m]",
    )
    center_time: Time = Field(
        ...,
        description="Acquisition time of scene center (UTC)",
    )

    @field_validator("target_position", mode="before")
    @classmethod
    def _parse_target_position(cls, v: Any) -> ECEFPosition:
        if isinstance(v, list):
            return ECEFPosition.from_list(v)
        return v


class ImageMetadata(BaseModel):
    """Complete image metadata."""

    data_type: str = Field(
        ..., description="Underlying sample data type (e.g. 'CInt16')"
    )
    length: float = Field(..., description="Approximate scene length on ground [m]")
    width: float = Field(..., description="Approximate scene width on ground [m]")
    rows: int = Field(..., description="Number of image lines (azimuth dimension)")
    columns: int = Field(
        ..., description="Number of samples per line (range dimension)"
    )
    pixel_spacing_row: float = Field(..., description="Pixel spacing in azimuth [m]")
    pixel_spacing_column: float = Field(
        ..., description="Pixel spacing in ground-range or slant-range [m]"
    )
    algorithm: str = Field(
        ..., description="Imaging algorithm used (e.g. 'backprojection')"
    )
    scale_factor: float = Field(
        ..., description="Radiometric scale factor to convert stored DN"
    )

    range_autofocus: bool | str | None = Field(
        None,
        description=(
            "Whether range autofocus was applied (if known); "
            "may be bool, string like 'global', or null if unknown"
        ),
    )
    azimuth_autofocus: bool | str | None = Field(
        None,
        description=(
            "Whether azimuth autofocus was applied (if known); "
            "may be bool, string like 'global', or null if unknown"
        ),
    )

    range_window: Window
    processed_range_bandwidth: float = Field(
        ..., description="Processed range bandwidth [Hz]"
    )
    azimuth_window: Window
    processed_azimuth_bandwidth: float = Field(
        ..., description="Processed azimuth bandwidth [Hz]"
    )

    image_geometry: ImageGeometry
    center_pixel: CenterPixel

    range_resolution: float = Field(
        ..., description="Nominal slant-range resolution [m]"
    )
    ground_range_resolution: float = Field(
        ..., description="Nominal ground-range resolution [m]"
    )
    azimuth_resolution: float = Field(..., description="Nominal azimuth resolution [m]")
    ground_azimuth_resolution: float = Field(
        ..., description="Nominal ground azimuth resolution [m]"
    )

    azimuth_looks: float = Field(..., description="Number of looks in azimuth")
    range_looks: float = Field(..., description="Number of looks in range")
    enl: float = Field(..., description="Equivalent number of looks (ENL)")

    reference_antenna_position: ECEFPosition | None = Field(
        None,
        description="Reference antenna phase center position in ECEF [m], if provided",
    )
    reference_target_position: ECEFPosition | None = Field(
        None,
        description="Reference target position in ECEF [m], if provided",
    )

    azimuth_beam_pattern_corrected: bool = Field(
        ..., description="True if azimuth beam pattern was corrected"
    )
    elevation_beam_pattern_corrected: bool = Field(
        ..., description="True if elevation beam pattern was corrected"
    )

    radiometry: Literal["beta_nought", "sigma_nought", "gamma_nought", "beta"] = Field(
        ..., description="Radiometric convention"
    )
    calibration: Literal["full", "partial", "limited", "none"] = Field(
        ..., description="Calibration level"
    )
    calibration_id: str = Field(
        ..., description="Identifier for calibration bundle used"
    )

    nesz_polynomial: Poly1D = Field(
        ..., description="Noise Equivalent Sigma Zero (NESZ) as 1D polynomial"
    )
    nesz_peak: float = Field(..., description="Peak NESZ [dB]")
    terrain_models: TerrainModels | None = Field(
        None,
        description="Terrain models used for focusing/geolocation, if provided",
    )

    reference_doppler_centroid: float = Field(
        ..., description="Reference Doppler centroid at scene center [Hz]"
    )
    frequency_doppler_centroid_polynomial: Poly2D = Field(
        ...,
        description=(
            "Doppler centroid frequency as 2D polynomial of image coordinates [Hz]"
        ),
    )

    quantization: Quantization

    @property
    def shape(self) -> tuple[int, int]:
        """Image shape as (rows, columns)."""
        return self.rows, self.columns

    @property
    def dtype(self) -> np.dtype:
        """Numpy dtype corresponding to the on-disk pixel type."""
        if self.data_type == "CInt16":
            return np.dtype("complex64")  # decoded from int16 pairs
        if self.data_type == "UInt16":
            return np.dtype("uint16")
        return np.dtype(self.data_type)

    @property
    def is_slant_plane(self) -> bool:
        """Check if image uses slant-plane geometry."""
        return self.image_geometry.type == "slant_plane"

    @property
    def is_pfa(self) -> bool:
        """Check if image uses PFA geometry."""
        return self.image_geometry.type == "pfa"

    @field_validator(
        "reference_antenna_position", "reference_target_position", mode="before"
    )
    @classmethod
    def _parse_ecef(cls, v: Any) -> ECEFPosition:
        if isinstance(v, list):
            return ECEFPosition.from_list(v)
        return v


---
src/capella_reader/metadata.py
---
"""Top-level metadata parsed from TIFFTAG_IMAGEDESCRIPTION."""

from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel, Field

from capella_reader.collect import Collect


class CapellaSLCMetadata(BaseModel):
    """Top-level metadata parsed from TIFFTAG_IMAGEDESCRIPTION."""

    software_version: str = Field(
        ..., description="Capella processing software version"
    )
    software_revision: str = Field(..., description="Git revision or internal build ID")
    processing_time: datetime = Field(
        ..., description="Time this product was processed (UTC)"
    )
    processing_deployment: str = Field(
        ..., description="Deployment environment (e.g. 'production')"
    )
    copyright: str = Field(..., description="Copyright notice")
    license: str = Field(..., description="License information")
    product_version: str = Field(..., description="Internal Capella product version")
    product_type: str = Field(..., description="Product type (e.g. 'SLC')")

    collect: Collect


---
src/capella_reader/orbit.py
---
"""State vectors, pointing, orbit, and antenna models."""

from collections import Counter
from collections.abc import Sequence
from typing import Any

import numpy as np
from numpy.typing import NDArray
from pydantic import BaseModel, Field, field_validator

from capella_reader._time import Time
from capella_reader.geometry import AttitudeQuaternion, ECEFPosition, ECEFVelocity
from capella_reader.polynomials import Poly2D


class CoordinateSystem(BaseModel):
    """Coordinate system specification."""

    type: str = Field(..., description="Coordinate system name (e.g. 'ecef')")
    wkt: str | None = Field(
        None,
        description=(
            "Well-Known Text (WKT) representation of coordinate system, if using"
            " geographic projection"
        ),
    )
    # https://github.com/pydantic/pydantic/discussions/9800#discussioncomment-9916979
    # TODO: exclude serializing wkt if none


class StateVector(BaseModel):
    """Platform state vector at a specific time."""

    time: Time = Field(..., description="Time of state vector (UTC)")
    position: ECEFPosition = Field(..., description="Platform position in ECEF [m]")
    velocity: ECEFVelocity = Field(..., description="Platform velocity in ECEF [m/s]")

    @field_validator("position", mode="before")
    @classmethod
    def _parse_position(cls, v: Any) -> ECEFPosition:
        if isinstance(v, list):
            return ECEFPosition.from_list(v)
        return v

    @field_validator("velocity", mode="before")
    @classmethod
    def _parse_velocity(cls, v: Any) -> ECEFVelocity:
        if isinstance(v, list):
            return ECEFVelocity.from_list(v)
        return v


class State(BaseModel):
    """Platform state ephemeris."""

    coordinate_system: CoordinateSystem = Field(
        ...,
        description="Coordinate system for state vectors (ECEF, etc.)",
    )
    direction: str = Field(
        ..., description="Orbit direction (e.g. 'ascending', 'descending')"
    )
    state_vectors: list[StateVector] = Field(
        ...,
        description="Platform ephemeris sampled over the collect",
    )
    source: str = Field(
        ..., description="Source of orbit solution (e.g. 'precise_determination')"
    )

    def _to_seconds(
        self, t: NDArray[np.datetime64], epoch: np.datetime64
    ) -> np.ndarray:
        """Convert a time-like object to seconds since epoch."""
        return (t - epoch) / np.timedelta64(1, "s")

    def get_state(
        self, time_as_float: bool = True
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Return the (time, position, velocity) for all StateVectors as arrays.

        Parameters
        ----------
        time_as_float : bool, optional
            If True, return times as floating-point seconds since first epoch.
            If False, return times as numpy.datetime64 objects.

        Returns
        -------
        tuple[np.ndarray, np.ndarray, np.ndarray]
            Tuple of (times, positions, velocities).

        """
        times = np.array([sv.time.as_numpy() for sv in self.state_vectors])
        if time_as_float:
            times = self._to_seconds(times, times[0])
        positions = np.array([sv.position.as_array() for sv in self.state_vectors])
        velocities = np.array([sv.velocity.as_array() for sv in self.state_vectors])
        return times, positions, velocities


class PointingSample(BaseModel):
    """Platform attitude at a specific time."""

    time: Time = Field(..., description="Time of attitude sample (UTC)")
    attitude: AttitudeQuaternion = Field(
        ...,
        description="Platform attitude quaternion at this time",
    )

    @field_validator("attitude", mode="before")
    @classmethod
    def _parse_attitude(cls, v: Any) -> AttitudeQuaternion:
        if isinstance(v, list):
            return AttitudeQuaternion.from_list(v)
        return v


class Antenna(BaseModel):
    """Antenna parameters."""

    azimuth_beamwidth: float = Field(..., description="Azimuth 3-dB beamwidth [rad]")
    elevation_beamwidth: float = Field(
        ..., description="Elevation 3-dB beamwidth [rad]"
    )
    gain: float = Field(..., description="Antenna boresight gain [dB]")
    beam_pattern: Poly2D = Field(
        ...,
        description="2D polynomial approximation of antenna gain vs. angle",
    )


def is_uniformly_sampled(state_vectors: Sequence[StateVector]) -> bool:
    """Return True if the StateVectors are uniformly sampled."""
    times = [sv.time.as_numpy() for sv in state_vectors]
    dt_count = set(np.diff(times))  # type: ignore[arg-type]
    return len(dt_count) == 1


def interpolate_orbit(state_vectors: Sequence[StateVector]) -> list[StateVector]:
    """Ensure the `state_vectors` are uniformly sampled, interpolating if necessary.

    Parameters
    ----------
    state_vectors : Sequence[StateVector]
        Sequence of StateVector objects to interpolate.

    Returns
    -------
    list[StateVector]
        Uniformly sampled state vectors.

    """
    if is_uniformly_sampled(state_vectors):
        return list(state_vectors)
    times = [sv.time.as_numpy() for sv in state_vectors]
    positions = np.array([sv.position.as_array() for sv in state_vectors])
    velocities = np.array([sv.velocity.as_array() for sv in state_vectors])
    dt_count = Counter(np.diff(times))  # type: ignore[arg-type]
    # example: dt_count.most_common(1)
    #  [(datetime.timedelta(microseconds=599999), 102)]
    dt, _count = dt_count.most_common(1)[0]

    t2, p2, v2 = resample_orbit_data_linear(
        times, positions, velocities, dt_seconds=dt / np.timedelta64(1, "s")
    )
    return [
        StateVector(
            time=tt,
            position=ECEFPosition(x=pp[0], y=pp[1], z=pp[2]),
            velocity=ECEFVelocity(vx=vv[0], vy=vv[1], vz=vv[2]),
        )
        for tt, pp, vv in zip(t2, p2, v2, strict=True)
    ]


def resample_orbit_data_linear(
    t: Sequence[np.datetime64], p: NDArray, v: NDArray, dt_seconds: float = 10
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Resample orbit data to have uniform time spacing.

    Parameters
    ----------
    t : Sequence[np.datetime64]
        Sequence of datetime objects.
    p : np.ndarray
        Array of position vectors, shape (n, 3).
    v : np.ndarray
        Array of velocity vectors, shape (n, 3).
    dt_seconds : float, optional
        Desired time step in seconds, by default 10.

    Returns
    -------
    tuple[np.ndarray, np.ndarray, np.ndarray]
        Resampled t, p, and v arrays.

    Notes
    -----
    This function uses linear interpolation for resampling.

    """
    t_seconds = np.array([(ti - t[0]) / np.timedelta64(1, "s") for ti in t])

    t_new = np.arange(t_seconds[0], t_seconds[-1] + dt_seconds, dt_seconds)

    p_new = np.array([np.interp(t_new, t_seconds, p[:, i]) for i in range(3)]).T
    v_new = np.array([np.interp(t_new, t_seconds, v[:, i]) for i in range(3)]).T

    # Convert to integer nanoseconds to avoid floating point precision issues
    dt_ns = round(dt_seconds * 1e9)
    n_steps = len(t_new)
    t_new_datetime = np.array(
        [t[0] + np.timedelta64(i * dt_ns, "ns") for i in range(n_steps)]
    )

    return t_new_datetime, p_new, v_new


---
src/capella_reader/polynomials.py
---
"""Polynomial wrappers using numpy.polynomial.Polynomial."""

from __future__ import annotations

from typing import Any, Self

import numpy as np
from numpy.polynomial import Polynomial
from numpy.polynomial.polynomial import polyval2d, polyvander2d
from pydantic import BaseModel, Field, field_serializer, field_validator


class Poly1D(BaseModel, arbitrary_types_allowed=True):
    """1D polynomial p(x) = sum c[i] x^i using numpy.polynomial.Polynomial."""

    degree: int = Field(..., description="Polynomial degree (order)")
    coefficients: np.ndarray = Field(
        ...,
        description="1D array of coefficients [c0, c1, ..., cN]",
    )

    @field_validator("coefficients", mode="before")
    @classmethod
    def _coeffs_to_1d_array(cls, v: Any) -> np.ndarray:
        arr = np.asanyarray(v, dtype=float)
        if arr.ndim != 1:
            msg = f"Poly1D coefficients must be 1D, got shape {arr.shape}"
            raise ValueError(msg)
        return arr

    @field_serializer("coefficients")
    def _serialize_coefficients(self, coefficients: np.ndarray) -> list[float]:
        """Serialize numpy array to list for JSON output."""
        return coefficients.tolist()

    def as_numpy_polynomial(self) -> Polynomial:
        """Convert to numpy.polynomial.Polynomial."""
        return Polynomial(self.coefficients)

    def __call__(self, x: float | np.ndarray) -> float | np.ndarray:
        """Evaluate the polynomial at x."""
        return self.as_numpy_polynomial()(x)


class Poly2D(BaseModel, arbitrary_types_allowed=True):
    """2D polynomial p(x,y) = sum c[i,j] x^i y^j."""

    degree: tuple[int, int] = Field(..., description="Polynomial degree for (x, y)")
    coefficients: np.ndarray = Field(
        ...,
        description="2D array of coefficients [i, j] -> c_ij",
    )

    # let user pass int as degree:
    @field_validator("degree", mode="before")
    @classmethod
    def _degree_to_tuple(cls, v: int | tuple[int, int]) -> tuple[int, int]:
        if isinstance(v, int):
            return v, v
        return v

    @field_validator("coefficients", mode="before")
    @classmethod
    def _coeffs_to_2d_array(cls, v: Any) -> np.ndarray:
        arr = np.asanyarray(v, dtype=float)
        if arr.ndim != 2:
            msg = f"Poly2D coefficients must be 2D, got shape {arr.shape}"
            raise ValueError(msg)
        return arr

    @field_serializer("coefficients")
    def _serialize_coefficients(self, coefficients: np.ndarray) -> list[list[float]]:
        """Serialize numpy array to nested list for JSON output."""
        return coefficients.tolist()

    def __call__(
        self, x: float | np.ndarray, y: float | np.ndarray
    ) -> float | np.ndarray:
        """Evaluate the polynomial at (x, y).

        p(x,y) = sum_{i,j} c[i,j] * x^i * y^j
        """
        x_arr = np.asanyarray(x)
        y_arr = np.asanyarray(y)
        return polyval2d(x_arr, y_arr, self.coefficients)

    @classmethod
    def from_fit(
        cls,
        x: np.ndarray,
        y: np.ndarray,
        data: np.ndarray,
        degree: int | tuple[int, int] = 1,
        # TODO: this seems hacky, not sure how to organize this
        # for the coreg. polynomial
        cross_terms: bool = True,
        weights: np.ndarray | None = None,
        robust: bool = True,
    ) -> Self:
        """Fit a 2D polynomial to gridded data.

        Parameters
        ----------
        x : np.ndarray
            1D array of x coordinates.
        y : np.ndarray
            1D array of y coordinates.
        data : np.ndarray
            2D array of data values where data[i, j] = f(x[i], y[j]).
        degree : int or tuple[int, int]
            Degree of the polynomial to fit.
        cross_terms : bool
            Whether to include cross terms in the fit.
            Default is True.
        weights : np.ndarray | None
            Weights for each data point.
            If None, uniform weights are used.
        robust : bool
            Whether to perform a robust fitting iteration by reweighting
            data based on the MAD of the residuals.
            Default is True

        Returns
        -------
        Poly2D
            Fitted 2D polynomial.

        Examples
        --------
        Fit a linear surface to a 3x3 grid:

        >>> x = np.array([0., 1., 2.])
        >>> y = np.array([0., 1., 2.])
        >>> data = np.array([[1., 2., 3.],
        ...                  [2., 3., 4.],
        ...                  [3., 4., 5.]])
        >>> poly = Poly2D.from_fit(x, y, data, degree=1)

        """
        if isinstance(degree, int):
            degree = (degree, degree)

        # Flatten for fitting
        if data.shape == (len(y), len(x)):
            # Create meshgrid for all combinations of x and y
            x_grid, y_grid = np.meshgrid(x, y, indexing="ij")
            x_flat = x_grid.ravel()
            y_flat = y_grid.ravel()
            data_flat = data.ravel()
        elif data.shape == x.shape == y.shape:
            x_flat, y_flat, data_flat = x, y, data
        else:
            msg = (
                "data must be 2D array with shape (len(y), len(x)),"
                " or 1D arrays with same length as x and y"
            )
            raise ValueError(msg)

        if weights is None:
            weights = np.ones_like(data_flat)
        for _ in range(1 + int(robust)):
            # Create Vandermonde matrix and solve
            vander = polyvander2d(x_flat, y_flat, degree)
            if not cross_terms:
                # TODO: this isn't right for degree above 1
                vander = vander[:, :3]
            coeffs_flat = np.linalg.lstsq(
                vander * weights[:, None], data_flat * weights, rcond=None
            )[0]
            if not cross_terms:
                coeffs = np.zeros((degree[0] + 1, degree[1] + 1))
                coeffs.ravel()[:3] = coeffs_flat
            else:
                coeffs = coeffs_flat.reshape(degree[0] + 1, degree[1] + 1)

            # Add robust weighting from the residuals
            r = polyval2d(x_flat, y_flat, coeffs) - data_flat
            med = np.nanmedian(np.abs(r))
            mad = 1.4286 * np.nanmedian(np.abs(r - med))
            # Weight by Tukey's biweight
            if mad > 0:
                u = (r - med) / (3 * mad)
                w_robust = (1 - u**2) ** 2
                w_robust[np.abs(u) >= 1] = 0.0
                weights *= w_robust

        return cls(degree=degree, coefficients=coeffs)


---
src/capella_reader/py.typed
---


---
src/capella_reader/radar.py
---
"""Radar configuration and time-varying parameters."""

from __future__ import annotations

from typing import Literal

from pydantic import BaseModel, Field

from ._time import Time
from .enums import LookSide


class RadarTimeVaryingParams(BaseModel):
    """Time-varying radar parameters."""

    start_timestamps: list[Time] = Field(
        ...,
        description="Start times when this PRF / waveform setting becomes active",
    )
    prf: float = Field(..., description="Pulse repetition frequency [Hz]")
    pulse_bandwidth: float = Field(..., description="Transmit pulse bandwidth [Hz]")
    pulse_duration: float = Field(..., description="Transmit pulse duration [s]")
    rank: int = Field(
        ..., description="Internal rank / priority for this configuration"
    )


class PRFEntry(BaseModel):
    """Pulse repetition frequency entry."""

    start_timestamps: list[Time] = Field(
        ...,
        description="Start times when this PRF is active",
    )
    prf: float = Field(..., description="Pulse repetition frequency [Hz]")


class Radar(BaseModel):
    """Radar configuration."""

    rank: int = Field(..., description="Internal radar configuration rank")
    center_frequency: float = Field(..., description="Radar carrier frequency [Hz]")
    pointing: LookSide = Field(..., description="Look side of platform")
    sampling_frequency: float = Field(
        ..., description="Receive sampling frequency [Hz]"
    )
    transmit_polarization: Literal["H", "V"] = Field(
        ..., description="Transmit polarization"
    )
    receive_polarization: Literal["H", "V"] = Field(
        ..., description="Receive polarization"
    )

    time_varying_parameters: list[RadarTimeVaryingParams] = Field(
        ...,
        description="Full time-varying radar configuration over the collect",
    )
    prf: list[PRFEntry] = Field(
        ...,
        description="Simplified time-varying PRF sequence",
    )


---
src/capella_reader/slc.py
---
"""Capella SLC wrapper with convenient data access."""

from __future__ import annotations

import json
from functools import cached_property
from pathlib import Path
from typing import NamedTuple

import numpy as np
import pydantic
import tifffile
from pydantic import BaseModel, Field
from typing_extensions import Self

from capella_reader._time import Time
from capella_reader.collect import Collect
from capella_reader.image import PFAGeometry, SlantPlaneGeometry
from capella_reader.metadata import CapellaSLCMetadata
from capella_reader.polynomials import Poly2D

C_LIGHT = 299792458.0


class CapellaParseError(ValueError):
    """Exception thrown from incorrectly parsing a Capella GeoTiff file."""


class CapellaImageGeometryError(ValueError):
    """Exception thrown when image geometry is not supported."""


class GroundControlPoint(NamedTuple):
    """A mapping of row, col image coordinates to x, y, z."""

    row: float
    col: float
    x: float
    y: float
    z: float


class CapellaSLC(BaseModel):
    """Convenience wrapper for Capella SLC GeoTIFF files.

    Holds the path to the TIFF and the parsed metadata,
    and exposes image-like helpers (shape, dtype, slicing with __getitem__).
    """

    path: Path = Field(..., description="Path to Capella SLC GeoTIFF or JSON file")
    meta: CapellaSLCMetadata = Field(..., description="Full parsed Capella metadata")

    @property
    def shape(self) -> tuple[int, int]:
        """Alias to the underlying image shape (rows, columns)."""
        return self.meta.collect.image.shape

    @property
    def dtype(self) -> np.dtype:
        """Numpy dtype of the pixel data."""
        return self.meta.collect.image.dtype

    @classmethod
    def from_file(cls, path: str | Path) -> CapellaSLC:
        """Read TIFFTAG_IMAGEDESCRIPTION and parse metadata.

        Parameters
        ----------
        path
            Path to Capella SLC GeoTIFF file

        Returns
        -------
        CapellaSLC
            Parsed SLC object with metadata

        """
        path = Path(path)
        if path.suffix == ".tiff" or path.suffix == ".tif":
            try:
                with tifffile.TiffFile(path) as tif:
                    image_description_tag: str = (
                        tif.pages[0].tags["ImageDescription"].value  # type: ignore[union-attr]
                    )
            except KeyError as e:
                msg = f"Failed to parse Capella ImageDescription tags in {path}"
                raise CapellaParseError(msg) from e
                # Continue with TIFF parsing
            d = json.loads(image_description_tag)
        elif path.suffix == ".json":
            # JSON metadata file
            d = json.loads(path.read_text())
        else:
            msg = f"Unsupported file type: {path.suffix}"
            raise CapellaParseError(msg)

        try:
            meta = CapellaSLCMetadata.model_validate(d)
        except pydantic.ValidationError as e:
            msg = "Failed to validate Capella metadata"
            raise CapellaParseError(msg) from e
        return cls(path=path, meta=meta)

    @property
    def collect(self) -> Collect:
        """Alias to the underlying collect metadata."""
        return self.meta.collect

    # Geometry attributes
    @property
    def range_to_first_sample(self: Self) -> float:
        geom = self.meta.collect.image.image_geometry
        if geom.type != "slant_plane":
            msg = "Only supported for slant_plane geometry"
            raise CapellaImageGeometryError(msg)
        return geom.range_to_first_sample

    @property
    def delta_range_sample(self: Self) -> float:
        """Alias to the underlying range pixel spacing."""
        image = self.meta.collect.image
        geom = image.image_geometry
        if image.is_pfa:
            # appease mypy
            assert isinstance(geom, PFAGeometry)
            return geom.row_sample_spacing
        else:
            assert isinstance(geom, SlantPlaneGeometry)
            return geom.delta_range_sample

    @property
    def first_line_time(self: Self) -> Time:
        geom = self.meta.collect.image.image_geometry
        if geom.type != "slant_plane":
            msg = "Only supported for slant_plane geometry"
            raise CapellaImageGeometryError(msg)
        return geom.first_line_time

    @property
    def center_time(self: Self) -> Time:
        return self.meta.collect.image.center_pixel.center_time

    @property
    def ref_epoch(self: Self) -> Time:
        """Azimuth Time of center pixel, aliased to be a scene reference epoch."""
        # Center time is available for both slant_plane/pfa geometry
        return self.center_time

    @property
    def delta_line_time(self: Self) -> float:
        """Alias to the underlying azimuth time interval."""
        geom = self.meta.collect.image.image_geometry
        if geom.type != "slant_plane":
            msg = "Only supported for slant_plane geometry"
            raise CapellaImageGeometryError(msg)
        return geom.delta_line_time

    @property
    def sensing_start(self: Self) -> Time:
        return self.first_line_time

    @property
    def starting_range(self: Self) -> float:
        return self.range_to_first_sample

    @property
    def prf_average(self: Self) -> float:
        prf_values = [entry.prf for entry in self.meta.collect.radar.prf]
        return float(np.mean(prf_values))

    @property
    def center_frequency(self: Self) -> float:
        return self.meta.collect.radar.center_frequency

    @property
    def wavelength(self: Self) -> float:
        return C_LIGHT / self.center_frequency

    @property
    def polarization(self: Self) -> str:
        pol_transmit = self.meta.collect.radar.transmit_polarization
        pol_receive = self.meta.collect.radar.receive_polarization
        return f"{pol_transmit}{pol_receive}"

    @cached_property
    def gcps(self: Self) -> list[GroundControlPoint]:
        """Get the Ground Control Points in the tiff file."""
        if self.path.suffix == ".json":
            msg = "No GCPs available in JSON metadata files"
            raise ValueError(msg)

        with tifffile.TiffFile(self.path) as tif:
            gcp_arr: np.ndarray = tif.pages[0].tags["ModelTiepointTag"].value  # type: ignore[union-attr]
        # Delete the 3rd column ("3rd" dim of the 2D image)
        out: list[GroundControlPoint] = []
        gcp_arr = np.asarray(gcp_arr, dtype=float)
        gcp_arr = np.delete(gcp_arr.reshape(-1, 6), 2, axis=1)
        for row in gcp_arr:
            out.append(GroundControlPoint(*row.tolist()))
        return out

    @property
    def bounds(self: Self) -> tuple[float, float, float, float]:
        """Return the GCP bounding box as (min_lon, min_lat, max_lon, max_lat)."""
        if not self.gcps:
            msg = "No GCPs available to compute bounds"
            raise ValueError(msg)

        x = [gcp.x for gcp in self.gcps]
        y = [gcp.y for gcp in self.gcps]
        return (min(x), min(y), max(x), max(y))

    @property
    def frequency_doppler_centroid_polynomial(self: Self) -> Poly2D:
        return self.meta.collect.image.frequency_doppler_centroid_polynomial


---
src/capella_reader/adapters/__init__.py
---
"""Adapters for Capella reader to other processing libraries.

This subpackage contains optional adapters for working with Capella SLC data
using external processing libraries. These adapters are not required for basic metadata
parsing and are only available if the corresponding libraries are installed.

Modules
-------
isce3
    ISCE3 conversion utilities (requires isce3 to be installed)

"""

__all__ = ["isce3"]


---
src/capella_reader/adapters/isce3.py
---
"""ISCE3 conversion utilities for Capella SLC data.

This module provides functions to convert Capella SLC metadata and data
into ISCE3 data structures for use with the ISCE3 processing library.

Functions
---------
get_radar_grid
    Create ISCE3 `RadarGridParameters` from Capella SLC
get_orbit
    Create ISCE3 `Orbit` from Capella SLC state vectors
get_doppler_poly
    Create ISCE3 `Poly2d` for Doppler centroid frequency
get_doppler_lut2d
    Create ISCE3 `LUT2d` for Doppler centroid frequency
get_attitude
    Create ISCE3 `Attitude` from Capella pointing samples

"""

from __future__ import annotations

import warnings
from collections import Counter
from collections.abc import Sequence
from typing import TYPE_CHECKING

import numpy as np

from capella_reader._time import Time
from capella_reader.enums import LookSide
from capella_reader.orbit import PointingSample, StateVector

if TYPE_CHECKING:
    import isce3

    from capella_reader.slc import CapellaSLC


C_LIGHT = 299792458.0


def get_radar_grid(slc: CapellaSLC) -> isce3.product.RadarGridParameters:
    """Create ISCE3 RadarGridParameters from Capella SLC.

    Parameters
    ----------
    slc : CapellaSLC
        Capella SLC object with metadata

    Returns
    -------
    isce3.product.RadarGridParameters
        ISCE3 radar grid parameters

    """
    import isce3

    assert slc.sensing_start is not None
    # Using logic similar to s1-reader to create the radar grid
    sensing_start_seconds = (slc.sensing_start - slc.ref_epoch).total_seconds()
    ref_epoch_isce = isce3.core.DateTime(str(slc.ref_epoch.as_numpy()))

    return isce3.product.RadarGridParameters(
        sensing_start_seconds,
        slc.wavelength,
        # NB: "PRF" is used in isce to make the radar grid's azimuth time spacing.
        #  it is *not* the actual radar PRF.
        # Capella already specifies delta_line_time as the azimuth spacing
        1 / slc.delta_line_time,
        slc.starting_range,
        slc.delta_range_sample,
        isce3.core.LookSide(
            1 if slc.meta.collect.radar.pointing == LookSide.LEFT else -1
        ),
        slc.shape[0],
        slc.shape[1],
        ref_epoch_isce,
    )


def get_orbit(
    state_vectors: Sequence[StateVector], ref_epoch: Time | None = None
) -> isce3.core.Orbit:
    """Create ISCE3 orbit from Capella state vectors.

    Parameters
    ----------
    state_vectors : Sequence[StateVector]
        Sequence of StateVector objects from Capella metadata
    ref_epoch : Time, optional
        Reference epoch to use for the orbit.
        If None, uses the first state vector's time.

    Returns
    -------
    isce3.core.Orbit
        ISCE3 orbit object

    """
    import isce3

    from capella_reader.orbit import interpolate_orbit, is_uniformly_sampled

    if not state_vectors:
        msg = "No state vectors found in Capella metadata"
        raise ValueError(msg)

    # isce3 will throw the following for default collects:
    # ValueError: non-uniform spacing between state vectors encountered ...
    if not is_uniformly_sampled(state_vectors):
        sampled_dts = np.diff([sv.time.as_numpy() for sv in state_vectors])  # type: ignore[arg-type]
        dt_counter = Counter(sampled_dts)
        warnings.warn(
            f"State vectors are not uniformly sampled. Found {dt_counter}."
            " Interpolating to most common spacing.",
            stacklevel=2,
        )
        state_vectors = interpolate_orbit(state_vectors)

    # Use provided reference epoch or default to first state vector time
    if ref_epoch is None:
        ref_epoch = state_vectors[0].time
    ref_epoch_isce = isce3.core.DateTime(str(ref_epoch).strip("Z"))

    orbit_svs: list[isce3.core.StateVector] = [
        isce3.core.StateVector(
            isce3.core.DateTime(str(sv.time).strip("Z")),
            sv.position.as_array(),
            sv.velocity.as_array(),
        )
        for sv in state_vectors
    ]

    return isce3.core.Orbit(orbit_svs, ref_epoch_isce)


def get_attitude(
    pointing_samples: Sequence[PointingSample], ref_epoch: Time | None = None
) -> isce3.core.Attitude:
    """Create ISCE3 attitude from Capella pointing samples.

    Parameters
    ----------
    pointing_samples : Sequence[PointingSample]
        Sequence of PointingSample objects from Capella metadata
    ref_epoch : Time, optional
        Reference epoch to use for the attitude.
        If None, uses the first pointing sample's time.

    Returns
    -------
    isce3.core.Attitude
        ISCE3 attitude object

    Notes
    -----
    Capella quaternions are stored as (q0, q1, q2, q3) = (w, x, y, z) in the
    Hamilton convention (scalar-first), and represent a rotation from the
    coordinate_system frame to the Capella antenna frame.

    ISCE3 `Attitude` expects quaternions that rotate from the antenna frame
    to ECEF.

    """
    import isce3

    if not pointing_samples:
        msg = "No pointing samples found in Capella metadata"
        raise ValueError(msg)

    if ref_epoch is None:
        ref_epoch = pointing_samples[0].time

    # ISCE3 DateTime doesn't like trailing 'Z'
    ref_epoch_isce = isce3.core.DateTime(str(ref_epoch).strip("Z"))

    times = [
        (ps.time.as_numpy() - ref_epoch.as_numpy()) / np.timedelta64(1, "s")
        for ps in pointing_samples
    ]
    # Constant +90 deg rotation about +Z: RCS -> Capella antenna frame
    half_angle = 0.5 * np.deg2rad(90.0)
    q_R2A = np.array([np.cos(half_angle), 0.0, 0.0, np.sin(half_angle)], dtype=float)

    def quat_mul(q2: np.ndarray, q1: np.ndarray):
        """Hamilton product q = q2 ⊗ q1 (apply q1, then q2)."""
        w2, x2, y2, z2 = q2
        w1, x1, y1, z1 = q1
        return np.array(
            [
                w2 * w1 - x2 * x1 - y2 * y1 - z2 * z1,
                w2 * x1 + x2 * w1 + y2 * z1 - z2 * y1,
                w2 * y1 - x2 * z1 + y2 * w1 + z2 * x1,
                w2 * z1 + x2 * y1 - y2 * x1 + z2 * w1,
            ]
        )

    quaternions = []
    for ps in pointing_samples:
        # Capella quaternion: ECEF -> Antenna
        # Flip to be Antenna -> ECEF
        q_A2E = ps.attitude.as_array() * np.array([1, -1, -1, -1])

        # Radar coords -> ECEF = (Antenna -> ECEF) ⨂ (Radar coords -> Antenna)
        q_R2E = quat_mul(q_A2E, q_R2A)
        quaternions.append(isce3.core.Quaternion(*q_R2E))

    return isce3.core.Attitude(times, quaternions, ref_epoch_isce)


def get_doppler_poly(slc: CapellaSLC) -> isce3.core.Poly2d:
    """Build an ISCE3 Poly2D for Doppler centroid frequency (Hz).

    Capella polynomial convention:
        f_dc = poly(az_s, range_m)
    where az_s is seconds since first_line_time, and range_m is slant range (m).

    Parameters
    ----------
    slc : CapellaSLC
        Capella SLC object with metadata

    Returns
    -------
    isce3.core.Poly2d
        2D polynomial converted to isce3 object

    """
    import isce3

    poly = slc.frequency_doppler_centroid_polynomial
    return isce3.core.Poly2d(poly.coefficients)


def get_doppler_lut2d(
    slc: CapellaSLC,
    n_az: int = 10,
    method: str = "bilinear",
) -> isce3.core.LUT2d:
    """Build an ISCE3 LUT2d for Doppler centroid frequency (Hz).

    Capella polynomial convention:
        f_dc = poly(az_rel_s, range_m)
    where az_rel_s is seconds since first_line_time.

    ISCE3 LUT2d convention:
        f_dc = doppler_lut.eval(az_s_since_ref_epoch, range_m)

    This method constructs y-coordinates in "seconds since ref_epoch"
    (i.e., the same time basis used by RadarGridParameters.ref_epoch) and
    evaluates the Capella poly using az_rel = y - sensing_start_seconds.

    Parameters
    ----------
    slc : CapellaSLC
        Capella SLC object with metadata
    n_az : int
        Number of knots in azimuth (time) for the LUT.
        Default is 10.
    method : str
        Interpolation method for LUT2d (e.g. 'bilinear', 'bicubic').

    Returns
    -------
    isce3.core.LUT2d
        Doppler centroid LUT2d in Hz.

    """
    import isce3

    n_lines, n_samples = slc.shape

    # x axis: slant range (meters)
    r0 = slc.range_to_first_sample
    dr = slc.delta_range_sample

    # Pad one before and after to avoid OOB errors
    rg_idx = np.arange(-1, n_samples + 1, dtype=int)
    slant_ranges = r0 + rg_idx * dr
    slant_ranges = np.asarray(slant_ranges, dtype=np.float64)

    # y axis: az time in seconds since radar-grid reference epoch
    sensing_start_seconds = (slc.sensing_start - slc.ref_epoch).total_seconds()
    az_idx = np.linspace(-1, n_lines + 1, int(n_az))
    az_rel = az_idx * slc.delta_line_time  # seconds since first line time

    az_times = sensing_start_seconds + az_rel  # seconds since ref_epoch
    az_times = np.asarray(az_times, dtype=np.float64)

    # evaluate Capella poly: f_dc(az_rel, range)
    Y, X = np.meshgrid(az_times, slant_ranges, indexing="ij")
    data = slc.frequency_doppler_centroid_polynomial(Y, X)  # (len(az), len(rg))
    data = np.ascontiguousarray(data, dtype=np.float64)

    # LUT2d wants (xcoord, ycoord, data[y, x])
    return isce3.core.LUT2d(slant_ranges, az_times, data, method=method, b_error=True)


---
tests/conftest.py
---
"""Pytest configuration and fixtures."""

from pathlib import Path

import pytest


# Make a parametrized fixture with all the metadata files
def pytest_generate_tests(metafunc):
    """Generate parametrized tests for metadata files."""
    if "metadata_file" in metafunc.fixturenames:
        test_data_dir = Path(__file__).parent / "data"
        files = [
            test_data_dir / f
            for f in [
                "CAPELLA_C11_SM_SLC_VV_20251031191104_20251031191109_extended.json",
                "CAPELLA_C13_SP_SLC_HH_20241126045307_20241126045346_extended.json",
                "CAPELLA_C13_SP_SLC_HH_20250826023518_20250826023527_extended.json",
                "CAPELLA_C13_SP_SLC_HH_20251102104909_20251102104943_extended.json",
                "CAPELLA_C17_SM_SLC_HH_20251103180619_20251103180628_extended.json",
            ]
        ]
        metafunc.parametrize("metadata_file", files, ids=lambda p: p.name)


@pytest.fixture(scope="session")
def capella_test_files():
    """Validate and return paths to Capella SLC test data files.

    Raises
    ------
    FileNotFoundError
        If test data files are missing.

    Returns
    -------
    tuple[Path, Path]
        Paths to the two Capella SLC test files.

    """
    test_data_dir = Path(__file__).parent / "data"
    file1 = test_data_dir / "CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif"
    file2 = test_data_dir / "CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif"

    missing_files = []
    if not file1.exists():
        missing_files.append(file1.name)
    if not file2.exists():
        missing_files.append(file2.name)

    if missing_files:
        msg = (
            f"Test data files not found: {', '.join(missing_files)}\n\n"
            "Download test data by running:\n"
            "    pixi run download-test-data\n\n"
            "Or download manually from:\n"
            "    https://capella-open-data.s3.amazonaws.com/data/2024/6/26/"
            "CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055/"
            "CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif\n"
            "    https://capella-open-data.s3.amazonaws.com/data/2024/6/29/"
            "CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915/"
            "CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif"
        )
        raise FileNotFoundError(msg)

    return file1, file2


@pytest.fixture
def sample_metadata_dict():
    """Sample metadata dictionary matching Capella SLC structure."""
    return {
        "software_version": "1.0.0",
        "software_revision": "abc123",
        "processing_time": "2024-07-09T04:30:00Z",
        "processing_deployment": "production",
        "copyright": "Copyright 2025 Capella Space. All Rights Reserved.",
        "license": "https://www.capellaspace.com/data-licensing/",
        "product_version": "1.0",
        "product_type": "SLC",
        "collect": {
            "start_timestamp": "2024-07-09T04:03:29Z",
            "stop_timestamp": "2024-07-09T04:03:58Z",
            "local_datetime": "2024-07-09T12:03:43+0800",
            "local_timezone": "Asia/Shanghai",
            "platform": "capella-14",
            "mode": "spotlight",
            "collect_id": "CAPELLA_C14_SP_SLC_HH_20240709040329_20240709040358",
            "image": {
                "data_type": "CInt16",
                "length": 5.0,
                "width": 5.0,
                "rows": 1000,
                "columns": 1000,
                "pixel_spacing_row": 0.5,
                "pixel_spacing_column": 0.5,
                "algorithm": "backprojection",
                "scale_factor": 1.0,
                "range_autofocus": True,
                "azimuth_autofocus": True,
                "range_window": {
                    "name": "rectangular",
                    "parameters": {},
                    "broadening_factor": 1.0,
                },
                "processed_range_bandwidth": 1e9,
                "azimuth_window": {
                    "name": "rectangular",
                    "parameters": {},
                    "broadening_factor": 1.0,
                },
                "processed_azimuth_bandwidth": 1e9,
                "image_geometry": {
                    "type": "slant_plane",
                    "doppler_centroid_polynomial": {
                        "degree": 1,
                        "coefficients": [[0.0, 0.1], [0.2, 0.3]],
                    },
                    "first_line_time": "2024-07-09T04:03:29Z",
                    "delta_line_time": 0.001,
                    "range_to_first_sample": 800000.0,
                    "delta_range_sample": 0.5,
                },
                "center_pixel": {
                    "incidence_angle": 30.0,
                    "look_angle": 45.0,
                    "squint_angle": 0.1,
                    "layover_angle": 10.0,
                    "target_position": [1000000.0, 2000000.0, 3000000.0],
                    "center_time": "2024-07-09T04:03:43Z",
                },
                "range_resolution": 0.5,
                "ground_range_resolution": 0.5,
                "azimuth_resolution": 0.5,
                "ground_azimuth_resolution": 0.5,
                "azimuth_looks": 1.0,
                "range_looks": 1.0,
                "enl": 1.0,
                "reference_antenna_position": [5000000.0, 6000000.0, 7000000.0],
                "reference_target_position": [1000000.0, 2000000.0, 3000000.0],
                "azimuth_beam_pattern_corrected": True,
                "elevation_beam_pattern_corrected": True,
                "radiometry": "beta_nought",
                "calibration": "full",
                "calibration_id": "CAL123",
                "nesz_polynomial": {
                    "degree": 2,
                    "coefficients": [-30.0, 0.1, 0.01],
                },
                "nesz_peak": -25.0,
                "terrain_models": {"focusing": None},
                "reference_doppler_centroid": 0.0,
                "frequency_doppler_centroid_polynomial": {
                    "degree": 1,
                    "coefficients": [[0.0, 0.1], [0.2, 0.3]],
                },
                "quantization": {
                    "type": "block_adaptive_quantization",
                    "block_sample_size": 256,
                    "mean_bits": 8,
                    "std_bits": 8,
                    "sample_bits": 4,
                },
            },
            "radar": {
                "rank": 1,
                "center_frequency": 9.65e9,
                "pointing": "left",
                "sampling_frequency": 1.2e9,
                "transmit_polarization": "H",
                "receive_polarization": "H",
                "time_varying_parameters": [
                    {
                        "start_timestamps": ["2024-07-09T04:03:29Z"],
                        "prf": 5000.0,
                        "pulse_bandwidth": 1e9,
                        "pulse_duration": 1e-6,
                        "rank": 1,
                    }
                ],
                "prf": [
                    {
                        "start_timestamps": ["2024-07-09T04:03:29Z"],
                        "prf": 5000.0,
                    }
                ],
            },
            "state": {
                "coordinate_system": {"type": "ecef"},
                "direction": "ascending",
                "state_vectors": [
                    {
                        "time": "2024-07-09T04:03:29Z",
                        "position": [5000000.0, 6000000.0, 7000000.0],
                        "velocity": [1000.0, 2000.0, 3000.0],
                    }
                ],
                "source": "precise_determination",
            },
            "pointing": [
                {
                    "time": "2024-07-09T04:03:29Z",
                    "attitude": [1.0, 0.0, 0.0, 0.0],
                }
            ],
            "transmit_antenna": {
                "azimuth_beamwidth": 0.01,
                "elevation_beamwidth": 0.01,
                "gain": 40.0,
                "beam_pattern": {
                    "degree": 1,
                    "coefficients": [[1.0, 0.0], [0.0, 0.0]],
                },
            },
            "receive_antenna": {
                "azimuth_beamwidth": 0.01,
                "elevation_beamwidth": 0.01,
                "gain": 40.0,
                "beam_pattern": {
                    "degree": 1,
                    "coefficients": [[1.0, 0.0], [0.0, 0.0]],
                },
            },
        },
    }


---
tests/download-test-data.sh
---
set -e
mkdir -p tests/data
curl -L -o tests/data/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif https://capella-open-data.s3.amazonaws.com/data/2024/6/26/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055/CAPELLA_C14_SM_SLC_HH_20240626150051_20240626150055.tif
curl -L -o tests/data/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif https://capella-open-data.s3.amazonaws.com/data/2024/6/29/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915/CAPELLA_C14_SM_SLC_HH_20240629134910_20240629134915.tif


---
tests/test_adapters_isce3.py
---
"""Tests for orbit module."""

from __future__ import annotations

import numpy as np
import pytest

from capella_reader import Time
from capella_reader.adapters import isce3 as isce3_adapter
from capella_reader.orbit import StateVector
from capella_reader.slc import CapellaSLC

try:
    isce3 = pytest.importorskip("isce3", reason="isce3 not installed")
    from capella_reader.adapters.isce3 import get_orbit
except ImportError:
    isce3 = None


@pytest.mark.skipif(not isce3, reason="isce3 not installed")
class TestGetOrbit:
    """Tests for get_orbit function."""

    def test_basic_creation(self):
        """Test creating ISCE3 orbit from uniform state vectors."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:20.000000000"),
                position=[6378137.0, 150000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        orbit = get_orbit(svs)

        assert isinstance(orbit, isce3.core.Orbit)
        assert orbit.size == len(svs)

    def test_non_uniform_triggers_warning(self):
        """Test that non-uniform state vectors trigger interpolation warning."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:25.000000000"),
                position=[6378137.0, 187500.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        with pytest.warns(UserWarning, match="not uniformly sampled"):
            orbit = get_orbit(svs)

        assert isinstance(orbit, isce3.core.Orbit)

    def test_empty_state_vectors(self):
        """Test that empty state vectors raise ValueError."""
        with pytest.raises(ValueError, match="No state vectors"):
            get_orbit([])

    def test_reference_epoch(self):
        """Test that reference epoch is set to first state vector time."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        orbit = get_orbit(svs)

        ref_epoch_str = str(orbit.reference_epoch)
        assert "2024-01-01" in ref_epoch_str
        assert "12:00:00" in ref_epoch_str

    def test_non_uniform_spacing_with_fractional_seconds(self):
        """Test that fractional second spacing (like 0.6s) is handled correctly.

        This test reproduces the bug where floating point errors in nanosecond
        conversion caused ISCE3 to reject orbits with spacing like 0.6 seconds.
        The error was: "non-uniform spacing between state vectors encountered -
        interval between state vector at position 5 and state vector at position 6
        is 0.599999 s, expected 0.599999 s"

        The bug occurred because converting float seconds to nanoseconds accumulated
        floating point errors, creating variations at the nanosecond level that
        ISCE3's strict validation detected.
        """
        # Create state vectors with non-uniform spacing similar to real Capella data
        # The spacing is approximately 0.6 seconds but varies slightly
        base_time = Time("2024-01-01T12:00:00.000000000")
        svs = []
        for i in range(108):  # Similar to the error case which had 107 state vectors
            # Add slight variations at nanosecond level to mimic real data
            ns_offset = 599999455 if i % 2 == 0 else 599999454
            time_offset_s = i * ns_offset / 1e9

            t = Time(
                base_time.as_numpy() + np.timedelta64(int(time_offset_s * 1e9), "ns")
            )
            svs.append(
                StateVector(
                    time=t,
                    position=[6378137.0 + i * 100, i * 100, 0.0],
                    velocity=[0.0, 7500.0, 0.0],
                )
            )

        # This should not raise ValueError from ISCE3
        # The interpolate_orbit should fix the spacing to be truly uniform
        with pytest.warns(UserWarning, match="not uniformly sampled"):
            orbit = get_orbit(svs)

        # Verify the orbit was created successfully
        assert isinstance(orbit, isce3.core.Orbit)

        # Verify that ISCE3 accepts it (no ValueError about non-uniform spacing)
        # If the bug exists, ISCE3 constructor would raise:
        # ValueError: non-uniform spacing between state vectors encountered
        assert orbit.size > 0


def test_get_radar_grid(metadata_file) -> None:
    slc = CapellaSLC.from_file(metadata_file)
    if slc.meta.collect.image.is_pfa:
        pytest.skip("PFA mode not supported for isce")

    grid = isce3_adapter.get_radar_grid(slc)
    assert grid.length == slc.shape[0]
    assert grid.width == slc.shape[1]
    assert grid.wavelength == slc.wavelength
    assert grid.prf == (1 / slc.delta_line_time)
    assert grid.starting_range == slc.starting_range


def test_get_doppler_lut2d(metadata_file) -> None:
    slc = CapellaSLC.from_file(metadata_file)
    if slc.meta.collect.image.is_pfa:
        pytest.skip("PFA mode not supported for isce")
    lut = isce3_adapter.get_doppler_lut2d(slc)
    # Check that out sample files have a doppler lookup +/- 500 Hz
    assert -500.0 < lut.eval(0.0, slc.range_to_first_sample) < 500
    # TODO: better verification here by comparing with usage?


def test_get_doppler_poly(metadata_file) -> None:
    slc = CapellaSLC.from_file(metadata_file)
    if slc.meta.collect.image.is_pfa:
        pytest.skip("PFA mode not supported for isce")

    poly = isce3_adapter.get_doppler_poly(slc)
    np.testing.assert_array_equal(
        poly.coeffs, slc.frequency_doppler_centroid_polynomial.coefficients
    )

    lut = isce3_adapter.get_doppler_lut2d(slc, n_az=4, method="bilinear")
    assert lut.data.shape == (4, slc.shape[1] + 2)
    assert lut.interp_method.name == "BILINEAR"
    assert lut.bounds_error is True


def test_get_attitude(metadata_file) -> None:
    slc = CapellaSLC.from_file(metadata_file)
    pointing_samples = slc.collect.pointing
    attitude = isce3_adapter.get_attitude(pointing_samples=pointing_samples)
    assert attitude is not None
    # TODO: better verifying


---
tests/test_collect.py
---
"""Tests for collect module."""

from __future__ import annotations

import pytest
from pydantic import ValidationError

from capella_reader import Time
from capella_reader.collect import Collect
from capella_reader.enums import LookSide
from capella_reader.image import (
    CenterPixel,
    ImageMetadata,
    Quantization,
    SlantPlaneGeometry,
    Window,
)
from capella_reader.orbit import (
    Antenna,
    CoordinateSystem,
    PointingSample,
    State,
    StateVector,
)
from capella_reader.polynomials import Poly1D, Poly2D
from capella_reader.radar import PRFEntry, Radar, RadarTimeVaryingParams


class TestCollect:
    """Tests for Collect."""

    @pytest.fixture
    def sample_image_metadata(self):
        """Create sample ImageMetadata for testing."""
        nesz = Poly1D(degree=2, coefficients=[1.0, 0.5, 0.1])
        doppler_poly = Poly2D(degree=(1, 1), coefficients=[[100.0, 0.5], [0.1, 0.0]])

        geometry = SlantPlaneGeometry(
            type="slant_plane",
            doppler_centroid_polynomial=doppler_poly,
            first_line_time=Time("2024-01-01T12:00:00.000000000"),
            delta_line_time=0.001,
            range_to_first_sample=800000.0,
            delta_range_sample=1.5,
        )

        return ImageMetadata(
            data_type="CInt16",
            length=5000.0,
            width=5000.0,
            rows=1024,
            columns=2048,
            pixel_spacing_row=3.0,
            pixel_spacing_column=3.0,
            algorithm="backprojection",
            scale_factor=1.0,
            range_window=Window(name="rectangular", broadening_factor=1.0),
            processed_range_bandwidth=300e6,
            azimuth_window=Window(name="rectangular", broadening_factor=1.0),
            processed_azimuth_bandwidth=500.0,
            image_geometry=geometry,
            center_pixel=CenterPixel(
                incidence_angle=35.5,
                look_angle=30.0,
                squint_angle=2.5,
                target_position=[6378137.0, 0.0, 0.0],
                center_time=Time("2024-01-01T12:00:00.000000000"),
            ),
            range_resolution=1.0,
            ground_range_resolution=1.5,
            azimuth_resolution=1.0,
            ground_azimuth_resolution=1.5,
            azimuth_looks=1.0,
            range_looks=1.0,
            enl=1.0,
            azimuth_beam_pattern_corrected=True,
            elevation_beam_pattern_corrected=True,
            radiometry="beta_nought",
            calibration="full",
            calibration_id="cal_v1",
            nesz_polynomial=nesz,
            nesz_peak=-25.0,
            reference_doppler_centroid=100.0,
            frequency_doppler_centroid_polynomial=doppler_poly,
            quantization=Quantization(
                type="block_adaptive_quantization",
                block_sample_size=32,
                mean_bits=5,
                std_bits=3,
                sample_bits=4,
            ),
        )

    @pytest.fixture
    def sample_radar(self):
        """Create sample Radar for testing."""
        return Radar(
            rank=1,
            center_frequency=9.65e9,
            pointing=LookSide.RIGHT,
            sampling_frequency=600e6,
            transmit_polarization="H",
            receive_polarization="H",
            time_varying_parameters=[
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                    pulse_bandwidth=300e6,
                    pulse_duration=1e-6,
                    rank=1,
                )
            ],
            prf=[
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                )
            ],
        )

    @pytest.fixture
    def sample_state(self):
        """Create sample State for testing."""
        return State(
            coordinate_system=CoordinateSystem(type="ecef"),
            direction="ascending",
            state_vectors=[
                StateVector(
                    time=Time("2024-01-01T12:00:00.000000000"),
                    position=[6378137.0, 0.0, 0.0],
                    velocity=[0.0, 7500.0, 0.0],
                ),
                StateVector(
                    time=Time("2024-01-01T12:00:10.000000000"),
                    position=[6378137.0, 75000.0, 0.0],
                    velocity=[0.0, 7500.0, 0.0],
                ),
            ],
            source="precise_determination",
        )

    @pytest.fixture
    def sample_antenna(self):
        """Create sample Antenna for testing."""
        beam_pattern = Poly2D(
            degree=(1, 1),
            coefficients=[[1.0, 0.0], [0.0, 0.0]],
        )
        return Antenna(
            azimuth_beamwidth=0.01,
            elevation_beamwidth=0.02,
            gain=30.0,
            beam_pattern=beam_pattern,
        )

    def test_creation(
        self, sample_image_metadata, sample_radar, sample_state, sample_antenna
    ):
        """Test creating a Collect."""
        collect = Collect(
            start_timestamp=Time("2024-01-01T12:00:00.000000000"),
            stop_timestamp=Time("2024-01-01T12:01:00.000000000"),
            local_datetime="2024-01-01 05:00:00",
            local_timezone="America/Los_Angeles",
            platform="capella-14",
            mode="spotlight",
            collect_id="CAPELLA_C14_SP_GEO_HH_20240101T120000_20240101T120100",
            image=sample_image_metadata,
            radar=sample_radar,
            state=sample_state,
            pointing=[
                PointingSample(
                    time=Time("2024-01-01T12:00:00.000000000"),
                    attitude=[1.0, 0.0, 0.0, 0.0],
                )
            ],
            transmit_antenna=sample_antenna,
            receive_antenna=sample_antenna,
        )

        assert collect.start_timestamp.as_numpy() < collect.stop_timestamp.as_numpy()
        assert collect.platform == "capella-14"
        assert collect.mode == "spotlight"
        assert collect.local_timezone == "America/Los_Angeles"
        assert len(collect.pointing) == 1
        assert collect.image.rows == 1024
        assert collect.radar.center_frequency == 9.65e9

    def test_time_ordering(
        self, sample_image_metadata, sample_radar, sample_state, sample_antenna
    ):
        """Test that start_timestamp is before stop_timestamp."""
        collect = Collect(
            start_timestamp=Time("2024-01-01T12:00:00.000000000"),
            stop_timestamp=Time("2024-01-01T12:01:00.000000000"),
            local_datetime="2024-01-01 05:00:00",
            local_timezone="America/Los_Angeles",
            platform="capella-14",
            mode="spotlight",
            collect_id="CAPELLA_C14_SP_GEO_HH_20240101T120000_20240101T120100",
            image=sample_image_metadata,
            radar=sample_radar,
            state=sample_state,
            pointing=[],
            transmit_antenna=sample_antenna,
            receive_antenna=sample_antenna,
        )

        delta = collect.stop_timestamp - collect.start_timestamp
        assert delta.total_seconds() == 60.0

    def test_multiple_pointing_samples(
        self, sample_image_metadata, sample_radar, sample_state, sample_antenna
    ):
        """Test Collect with multiple pointing samples."""
        collect = Collect(
            start_timestamp=Time("2024-01-01T12:00:00.000000000"),
            stop_timestamp=Time("2024-01-01T12:01:00.000000000"),
            local_datetime="2024-01-01 05:00:00",
            local_timezone="America/Los_Angeles",
            platform="capella-14",
            mode="spotlight",
            collect_id="CAPELLA_C14_SP_GEO_HH_20240101T120000_20240101T120100",
            image=sample_image_metadata,
            radar=sample_radar,
            state=sample_state,
            pointing=[
                PointingSample(
                    time=Time("2024-01-01T12:00:00.000000000"),
                    attitude=[1.0, 0.0, 0.0, 0.0],
                ),
                PointingSample(
                    time=Time("2024-01-01T12:00:30.000000000"),
                    attitude=[0.999, 0.01, 0.01, 0.01],
                ),
                PointingSample(
                    time=Time("2024-01-01T12:01:00.000000000"),
                    attitude=[0.998, 0.02, 0.02, 0.02],
                ),
            ],
            transmit_antenna=sample_antenna,
            receive_antenna=sample_antenna,
        )

        assert len(collect.pointing) == 3
        assert collect.pointing[0].time.as_numpy() < collect.pointing[1].time.as_numpy()
        assert collect.pointing[1].time.as_numpy() < collect.pointing[2].time.as_numpy()

    def test_different_transmit_receive_antennas(
        self, sample_image_metadata, sample_radar, sample_state
    ):
        """Test Collect with different transmit and receive antennas."""
        tx_antenna = Antenna(
            azimuth_beamwidth=0.01,
            elevation_beamwidth=0.02,
            gain=30.0,
            beam_pattern=Poly2D(degree=(1, 1), coefficients=[[1.0, 0.0], [0.0, 0.0]]),
        )
        rx_antenna = Antenna(
            azimuth_beamwidth=0.015,
            elevation_beamwidth=0.025,
            gain=32.0,
            beam_pattern=Poly2D(degree=(1, 1), coefficients=[[1.2, 0.0], [0.0, 0.0]]),
        )

        collect = Collect(
            start_timestamp=Time("2024-01-01T12:00:00.000000000"),
            stop_timestamp=Time("2024-01-01T12:01:00.000000000"),
            local_datetime="2024-01-01 05:00:00",
            local_timezone="America/Los_Angeles",
            platform="capella-14",
            mode="spotlight",
            collect_id="CAPELLA_C14_SP_GEO_HH_20240101T120000_20240101T120100",
            image=sample_image_metadata,
            radar=sample_radar,
            state=sample_state,
            pointing=[],
            transmit_antenna=tx_antenna,
            receive_antenna=rx_antenna,
        )

        assert collect.transmit_antenna.gain == 30.0
        assert collect.receive_antenna.gain == 32.0
        assert (
            collect.transmit_antenna.azimuth_beamwidth
            != collect.receive_antenna.azimuth_beamwidth
        )

    def test_validation_requires_fields(self):
        """Test that required fields are validated."""
        with pytest.raises(ValidationError):
            Collect(
                start_timestamp=Time("2024-01-01T12:00:00.000000000"),
                platform="capella-14",
            )


---
tests/test_enums.py
---
"""Tests for small utility modules."""

from __future__ import annotations

import pytest

from capella_reader.enums import LookSide


def test_lookside_int_and_missing():
    """Test LookSide int conversion and missing value handling."""
    assert int(LookSide.LEFT) == 1
    assert int(LookSide.RIGHT) == -1
    assert LookSide("LEFT") is LookSide.LEFT
    assert LookSide("right") is LookSide.RIGHT
    assert LookSide(1) is LookSide.LEFT
    assert LookSide(-1) is LookSide.RIGHT
    with pytest.raises(ValueError, match="unknown"):
        LookSide("unknown")


---
tests/test_geometry.py
---
"""Tests for geometry types."""

import numpy as np
import pytest

from capella_reader.geometry import AttitudeQuaternion, ECEFPosition, ECEFVelocity


class TestECEFPosition:
    """Tests for ECEFPosition."""

    def test_creation(self):
        """Test creating an ECEFPosition."""
        pos = ECEFPosition(x=1000.0, y=2000.0, z=3000.0)

        assert pos.x == 1000.0
        assert pos.y == 2000.0
        assert pos.z == 3000.0

    def test_from_list(self):
        """Test creating from a list."""
        coords = [1000.0, 2000.0, 3000.0]
        pos = ECEFPosition.from_list(coords)

        assert pos.x == 1000.0
        assert pos.y == 2000.0
        assert pos.z == 3000.0

    def test_from_list_wrong_length(self):
        """Test that wrong length list raises a ValueError."""
        with pytest.raises(ValueError, match="expects 3 elements"):
            ECEFPosition.from_list([1000.0, 2000.0])

    def test_as_array(self):
        """Test conversion to numpy array."""
        pos = ECEFPosition(x=1000.0, y=2000.0, z=3000.0)
        arr = pos.as_array()

        expected = np.array([1000.0, 2000.0, 3000.0])
        np.testing.assert_array_equal(arr, expected)

    def test_add_subtract_and_norm(self):
        """Test vector arithmetic and norms."""
        pos1 = ECEFPosition(x=1000.0, y=2000.0, z=3000.0)
        pos2 = ECEFPosition(x=100.0, y=200.0, z=300.0)

        added = pos1 + pos2
        subtracted = pos1 - pos2

        assert added == ECEFPosition(x=1100.0, y=2200.0, z=3300.0)
        assert subtracted == ECEFPosition(x=900.0, y=1800.0, z=2700.0)
        assert pos2.norm() == pytest.approx(np.linalg.norm([100.0, 200.0, 300.0]))


class TestECEFVelocity:
    """Tests for ECEFVelocity."""

    def test_creation(self):
        """Test creating an ECEFVelocity."""
        vel = ECEFVelocity(vx=100.0, vy=200.0, vz=300.0)

        assert vel.vx == 100.0
        assert vel.vy == 200.0
        assert vel.vz == 300.0

    def test_from_list(self):
        """Test creating from a list."""
        coords = [100.0, 200.0, 300.0]
        vel = ECEFVelocity.from_list(coords)

        assert vel.vx == 100.0
        assert vel.vy == 200.0
        assert vel.vz == 300.0

    def test_from_list_wrong_length(self):
        """Test that wrong length list raises a ValueError."""
        with pytest.raises(ValueError, match="expects 3 elements"):
            ECEFVelocity.from_list([100.0, 200.0])

    def test_as_array(self):
        """Test conversion to numpy array."""
        vel = ECEFVelocity(vx=100.0, vy=200.0, vz=300.0)
        arr = vel.as_array()

        expected = np.array([100.0, 200.0, 300.0])
        np.testing.assert_array_equal(arr, expected)

    def test_add_subtract_and_norm(self):
        """Test vector arithmetic and norms."""
        vel1 = ECEFVelocity(vx=100.0, vy=200.0, vz=300.0)
        vel2 = ECEFVelocity(vx=10.0, vy=20.0, vz=30.0)

        added = vel1 + vel2
        subtracted = vel1 - vel2

        assert added == ECEFVelocity(vx=110.0, vy=220.0, vz=330.0)
        assert subtracted == ECEFVelocity(vx=90.0, vy=180.0, vz=270.0)
        assert vel2.norm() == pytest.approx(np.linalg.norm([10.0, 20.0, 30.0]))


class TestAttitudeQuaternion:
    """Tests for AttitudeQuaternion."""

    def test_creation(self):
        """Test creating an AttitudeQuaternion."""
        quat = AttitudeQuaternion(q0=1.0, q1=0.0, q2=0.0, q3=0.0)

        assert quat.q0 == 1.0
        assert quat.q1 == 0.0
        assert quat.q2 == 0.0
        assert quat.q3 == 0.0

    def test_from_list(self):
        """Test creating from a list."""
        coords = [1.0, 0.0, 0.0, 0.0]
        quat = AttitudeQuaternion.from_list(coords)

        assert quat.q0 == 1.0
        assert quat.q1 == 0.0
        assert quat.q2 == 0.0
        assert quat.q3 == 0.0

    def test_from_list_wrong_length(self):
        """Test that wrong length list raises a ValueError."""
        with pytest.raises(ValueError, match="expects 4 elements"):
            AttitudeQuaternion.from_list([1.0, 0.0, 0.0])

    def test_as_array(self):
        """Test conversion to numpy array."""
        quat = AttitudeQuaternion(q0=0.707, q1=0.707, q2=0.0, q3=0.0)
        arr = quat.as_array()

        expected = np.array([0.707, 0.707, 0.0, 0.0])
        np.testing.assert_array_almost_equal(arr, expected)


---
tests/test_image.py
---
"""Tests for image module."""

from __future__ import annotations

import numpy as np
import pytest
from pydantic import ValidationError

from capella_reader import Time
from capella_reader.geometry import ECEFPosition
from capella_reader.image import (
    CenterPixel,
    ImageMetadata,
    Quantization,
    SlantPlaneGeometry,
    TerrainModelRef,
    TerrainModels,
    Window,
)
from capella_reader.polynomials import Poly1D, Poly2D


@pytest.fixture
def minimal_slant_plane_geometry():
    """Create a minimal SlantPlaneGeometry for testing."""
    doppler_poly = Poly2D(degree=(1, 1), coefficients=[[100.0, 0.5], [0.1, 0.0]])
    return SlantPlaneGeometry(
        type="slant_plane",
        doppler_centroid_polynomial=doppler_poly,
        first_line_time=Time("2024-01-01T12:00:00.000000000"),
        delta_line_time=0.001,
        range_to_first_sample=800000.0,
        delta_range_sample=1.5,
    )


@pytest.fixture
def image_metadata_factory(minimal_slant_plane_geometry):
    """Create ImageMetadata objects with sensible defaults."""
    nesz = Poly1D(degree=2, coefficients=[1.0, 0.5, 0.1])
    doppler_poly = Poly2D(degree=(1, 1), coefficients=[[100.0, 0.5], [0.1, 0.0]])

    def create(**overrides):
        data = {
            "data_type": "CInt16",
            "length": 5000.0,
            "width": 5000.0,
            "rows": 1024,
            "columns": 2048,
            "pixel_spacing_row": 3.0,
            "pixel_spacing_column": 3.0,
            "algorithm": "backprojection",
            "scale_factor": 1.0,
            "range_window": Window(name="rectangular", broadening_factor=1.0),
            "processed_range_bandwidth": 300e6,
            "azimuth_window": Window(name="rectangular", broadening_factor=1.0),
            "processed_azimuth_bandwidth": 500.0,
            "image_geometry": minimal_slant_plane_geometry,
            "center_pixel": CenterPixel(
                incidence_angle=35.5,
                look_angle=30.0,
                squint_angle=2.5,
                target_position=[6378137.0, 0.0, 0.0],
                center_time=Time("2024-01-01T12:00:00.000000000"),
            ),
            "range_resolution": 1.0,
            "ground_range_resolution": 1.5,
            "azimuth_resolution": 1.0,
            "ground_azimuth_resolution": 1.5,
            "azimuth_looks": 1.0,
            "range_looks": 1.0,
            "enl": 1.0,
            "azimuth_beam_pattern_corrected": True,
            "elevation_beam_pattern_corrected": True,
            "radiometry": "beta_nought",
            "calibration": "full",
            "calibration_id": "cal_v1",
            "nesz_polynomial": nesz,
            "nesz_peak": -25.0,
            "reference_doppler_centroid": 100.0,
            "frequency_doppler_centroid_polynomial": doppler_poly,
            "quantization": Quantization(
                type="block_adaptive_quantization",
                block_sample_size=32,
                mean_bits=5,
                std_bits=3,
                sample_bits=4,
            ),
        }
        data.update(overrides)
        return ImageMetadata(**data)

    return create


class TestWindow:
    """Tests for Window."""

    def test_creation_basic(self):
        """Test creating a basic Window."""
        window = Window(
            name="rectangular",
            parameters={},
            broadening_factor=1.0,
        )

        assert window.name == "rectangular"
        assert window.parameters == {}
        assert window.broadening_factor == 1.0

    def test_creation_with_parameters(self):
        """Test creating Window with parameters."""
        window = Window(
            name="hamming",
            parameters={"alpha": 0.54, "beta": 0.46},
            broadening_factor=1.3,
        )

        assert window.name == "hamming"
        assert window.parameters["alpha"] == 0.54
        assert window.parameters["beta"] == 0.46
        assert window.broadening_factor == 1.3

    def test_validation_requires_fields(self):
        """Test that required fields are validated."""
        with pytest.raises(ValidationError):
            Window(name="rectangular")


class TestQuantization:
    """Tests for Quantization."""

    def test_creation(self):
        """Test creating a Quantization."""
        quant = Quantization(
            type="block_adaptive_quantization",
            block_sample_size=32,
            mean_bits=5,
            std_bits=3,
            sample_bits=4,
        )

        assert quant.type == "block_adaptive_quantization"
        assert quant.block_sample_size == 32
        assert quant.mean_bits == 5
        assert quant.std_bits == 3
        assert quant.sample_bits == 4

    def test_validation_requires_fields(self):
        """Test that all fields are required."""
        with pytest.raises(ValidationError):
            Quantization(type="block_adaptive_quantization")


class TestTerrainModelRef:
    """Tests for TerrainModelRef."""

    def test_creation(self):
        """Test creating a TerrainModelRef."""
        tm = TerrainModelRef(
            link="https://example.com/srtm",
            name="SRTM",
        )

        assert tm.link == "https://example.com/srtm"
        assert tm.name == "SRTM"


class TestTerrainModels:
    """Tests for TerrainModels."""

    def test_creation_empty(self):
        """Test creating empty TerrainModels."""
        tm = TerrainModels()

        assert tm.focusing is None

    def test_creation_with_focusing(self):
        """Test creating TerrainModels with focusing model."""
        ref = TerrainModelRef(link="https://example.com/srtm", name="SRTM")
        tm = TerrainModels(focusing=ref)

        assert tm.focusing is not None
        assert tm.focusing.name == "SRTM"


class TestImageGeometry:
    """Tests for ImageGeometry union types."""

    def test_creation_slant_plane(self):
        """Test creating SlantPlaneGeometry."""
        from capella_reader.image import SlantPlaneGeometry

        doppler_poly = Poly2D(
            degree=(1, 1),
            coefficients=[[100.0, 0.5], [0.1, 0.0]],
        )

        geom = SlantPlaneGeometry(
            type="slant_plane",
            doppler_centroid_polynomial=doppler_poly,
            first_line_time=Time("2024-01-01T12:00:00.000000000"),
            delta_line_time=0.001,
            range_to_first_sample=800000.0,
            delta_range_sample=1.5,
        )

        assert geom.type == "slant_plane"
        assert geom.doppler_centroid_polynomial is not None
        assert geom.first_line_time is not None
        assert geom.delta_line_time == 0.001
        assert geom.range_to_first_sample == 800000.0
        assert geom.delta_range_sample == 1.5

    def test_creation_pfa(self):
        """Test creating PFAGeometry with required and extra fields."""
        from capella_reader.image import PFAGeometry

        geom = PFAGeometry(
            type="pfa",
            scene_reference_point_row_col=(100.0, 200.0),
            scene_reference_point_ecef=[1000000.0, 2000000.0, 3000000.0],
            row_sample_spacing=1.0,
            col_sample_spacing=1.0,
            row_direction=(1.0, 0.0, 0.0),
            col_direction=(0.0, 1.0, 0.0),
            slant_plane_normal=(0.0, 0.0, 1.0),
            ground_plane_normal=(0.0, 0.0, 1.0),
            polar_angle_polynomial=Poly1D(degree=1, coefficients=[0.0, 1.0]),
            spatial_frequency_scale_factor_polynomial=Poly1D(
                degree=1, coefficients=[1.0, 0.0]
            ),
            custom_field="custom_value",
        )

        assert geom.type == "pfa"
        assert not hasattr(geom, "doppler_centroid_polynomial")
        assert hasattr(geom, "custom_field")

    def test_allows_extra_fields(self):
        """Test that extra fields are allowed for PFA geometry."""
        from capella_reader.image import PFAGeometry

        geom = PFAGeometry(
            type="pfa",
            scene_reference_point_row_col=(100.0, 200.0),
            scene_reference_point_ecef=[1000000.0, 2000000.0, 3000000.0],
            row_sample_spacing=1.0,
            col_sample_spacing=1.0,
            row_direction=(1.0, 0.0, 0.0),
            col_direction=(0.0, 1.0, 0.0),
            slant_plane_normal=(0.0, 0.0, 1.0),
            ground_plane_normal=(0.0, 0.0, 1.0),
            polar_angle_polynomial=Poly1D(degree=1, coefficients=[0.0, 1.0]),
            spatial_frequency_scale_factor_polynomial=Poly1D(
                degree=1, coefficients=[1.0, 0.0]
            ),
            pfa_specific_param=123.456,
            another_param="value",
        )

        assert geom.type == "pfa"
        assert geom.model_extra["pfa_specific_param"] == 123.456
        assert geom.model_extra["another_param"] == "value"


class TestCenterPixel:
    """Tests for CenterPixel."""

    def test_creation(self):
        """Test creating a CenterPixel."""
        cp = CenterPixel(
            incidence_angle=35.5,
            look_angle=30.0,
            squint_angle=2.5,
            layover_angle=45.0,
            target_position=ECEFPosition(x=6378137.0, y=0.0, z=0.0),
            center_time=Time("2024-01-01T12:00:00.000000000"),
        )

        assert cp.incidence_angle == 35.5
        assert cp.look_angle == 30.0
        assert cp.squint_angle == 2.5
        assert cp.layover_angle == 45.0
        assert cp.target_position.x == 6378137.0
        assert cp.center_time is not None

    def test_creation_with_list_position(self):
        """Test creating CenterPixel with position as list."""
        cp = CenterPixel(
            incidence_angle=35.5,
            look_angle=30.0,
            squint_angle=2.5,
            target_position=[6378137.0, 0.0, 0.0],
            center_time=Time("2024-01-01T12:00:00.000000000"),
        )

        assert cp.target_position.x == 6378137.0
        assert cp.target_position.y == 0.0
        assert cp.target_position.z == 0.0

    def test_layover_angle_optional(self):
        """Test that layover_angle can be None."""
        cp = CenterPixel(
            incidence_angle=35.5,
            look_angle=30.0,
            squint_angle=2.5,
            layover_angle=None,
            target_position=[6378137.0, 0.0, 0.0],
            center_time=Time("2024-01-01T12:00:00.000000000"),
        )

        assert cp.layover_angle is None


class TestImageMetadata:
    """Tests for ImageMetadata."""

    def test_shape_property(self, image_metadata_factory):
        """Test that shape property returns (rows, columns)."""
        img = image_metadata_factory()

        assert img.shape == (1024, 2048)

    def test_dtype_cint16(self, image_metadata_factory):
        """Test that dtype returns complex64 for CInt16."""
        img = image_metadata_factory()

        assert img.dtype == np.dtype("complex64")

    def test_dtype_uint16(self, image_metadata_factory):
        """Test that dtype returns uint16 for UInt16."""
        img = image_metadata_factory(data_type="UInt16")

        assert img.dtype == np.dtype("uint16")

    def test_dtype_fallback(self, image_metadata_factory):
        """Test dtype fallback for unexpected data types."""
        img = image_metadata_factory(data_type="float32")

        assert img.dtype == np.dtype("float32")

    def test_autofocus_bool(self, image_metadata_factory):
        """Test that autofocus fields accept boolean values."""
        img = image_metadata_factory(range_autofocus=True, azimuth_autofocus=False)

        assert img.range_autofocus is True
        assert img.azimuth_autofocus is False

    def test_autofocus_string(self, image_metadata_factory):
        """Test that autofocus fields accept string values."""
        img = image_metadata_factory(
            range_autofocus="global", azimuth_autofocus="local"
        )

        assert img.range_autofocus == "global"
        assert img.azimuth_autofocus == "local"

    def test_reference_positions_with_lists(self, image_metadata_factory):
        """Test that reference positions can be parsed from lists."""
        img = image_metadata_factory(
            reference_antenna_position=[6400000.0, 100.0, 200.0],
            reference_target_position=[6378137.0, 0.0, 0.0],
        )

        assert img.reference_antenna_position is not None
        assert img.reference_antenna_position.x == 6400000.0
        assert img.reference_target_position is not None
        assert img.reference_target_position.x == 6378137.0


---
tests/test_metadata.py
---
"""Tests for metadata parsing and models."""

from datetime import datetime
from pathlib import Path

import numpy as np

from capella_reader._time import Time
from capella_reader.collect import Collect
from capella_reader.geometry import AttitudeQuaternion, ECEFPosition, ECEFVelocity
from capella_reader.image import ImageMetadata
from capella_reader.metadata import CapellaSLCMetadata
from capella_reader.orbit import Antenna, State, StateVector
from capella_reader.polynomials import Poly1D, Poly2D
from capella_reader.radar import Radar


class TestCapellaSLCMetadata:
    """Tests for CapellaSLCMetadata parsing."""

    def test_parse_full_metadata(self, sample_metadata_dict):
        """Test parsing a complete metadata dictionary."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert meta.software_version == "1.0.0"
        assert meta.software_revision == "abc123"
        assert meta.product_type == "SLC"
        assert meta.processing_deployment == "production"

    def test_collect_parsing(self, sample_metadata_dict):
        """Test that collect metadata is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.collect, Collect)
        assert meta.collect.platform == "capella-14"
        assert meta.collect.mode == "spotlight"

    def test_image_metadata_parsing(self, sample_metadata_dict):
        """Test that image metadata is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.collect.image, ImageMetadata)
        assert meta.collect.image.rows == 1000
        assert meta.collect.image.columns == 1000
        assert meta.collect.image.data_type == "CInt16"

    def test_image_shape_property(self, sample_metadata_dict):
        """Test the shape property of ImageMetadata."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert meta.collect.image.shape == (1000, 1000)

    def test_image_dtype_property(self, sample_metadata_dict):
        """Test the numpy dtype property of ImageMetadata."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert meta.collect.image.dtype == np.dtype(np.complex64)

    def test_radar_parsing(self, sample_metadata_dict):
        """Test that radar metadata is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.collect.radar, Radar)
        assert meta.collect.radar.center_frequency == 9.65e9
        assert meta.collect.radar.pointing == "left"

    def test_state_parsing(self, sample_metadata_dict):
        """Test that state metadata is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.collect.state, State)
        assert meta.collect.state.direction == "ascending"
        assert len(meta.collect.state.state_vectors) == 1

    def test_state_vector_parsing(self, sample_metadata_dict):
        """Test that state vectors are parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        sv = meta.collect.state.state_vectors[0]
        assert isinstance(sv, StateVector)
        assert isinstance(sv.position, ECEFPosition)
        assert isinstance(sv.velocity, ECEFVelocity)
        assert sv.position.x == 5000000.0

    def test_pointing_parsing(self, sample_metadata_dict):
        """Test that pointing data is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert len(meta.collect.pointing) == 1
        assert isinstance(meta.collect.pointing[0].attitude, AttitudeQuaternion)
        assert meta.collect.pointing[0].attitude.q0 == 1.0

    def test_antenna_parsing(self, sample_metadata_dict):
        """Test that antenna metadata is parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.collect.transmit_antenna, Antenna)
        assert isinstance(meta.collect.receive_antenna, Antenna)
        assert meta.collect.transmit_antenna.gain == 40.0

    def test_polynomial_parsing(self, sample_metadata_dict):
        """Test that polynomials are parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        doppler_poly = meta.collect.image.image_geometry.doppler_centroid_polynomial
        assert isinstance(doppler_poly, Poly2D)

        nesz_poly = meta.collect.image.nesz_polynomial
        assert isinstance(nesz_poly, Poly1D)

    def test_datetime_parsing(self, sample_metadata_dict):
        """Test that datetimes are parsed correctly."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        assert isinstance(meta.processing_time, datetime)
        assert isinstance(meta.collect.start_timestamp, Time)
        assert isinstance(meta.collect.local_datetime, str)

    def test_ecef_position_from_list(self, sample_metadata_dict):
        """Test that ECEF positions are converted from lists."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        target_pos = meta.collect.image.center_pixel.target_position
        assert isinstance(target_pos, ECEFPosition)
        assert target_pos.x == 1000000.0
        assert target_pos.y == 2000000.0
        assert target_pos.z == 3000000.0

    def test_extra_fields_ignored(self, sample_metadata_dict):
        """Test that extra unknown fields are ignored."""
        data = sample_metadata_dict.copy()
        data["unknown_field"] = "should be ignored"
        data["collect"]["unknown_collect_field"] = "also ignored"

        meta = CapellaSLCMetadata.model_validate(data)
        assert meta.software_version == "1.0.0"

    def test_load_from_json_file(self):
        """Test loading metadata from a JSON file."""
        json_file = (
            Path(__file__).parent
            / "data"
            / "CAPELLA_C13_SP_SLC_HH_20241126045307_20241126045346_extended.json"
        )

        with open(json_file) as f:
            json_str = f.read()

        meta = CapellaSLCMetadata.model_validate_json(json_str)

        assert meta.software_version == "2.64.3"
        assert meta.product_type == "SLC"
        assert meta.collect.platform == "capella-13"
        assert meta.collect.mode == "spotlight"

    def test_round_trip_json(self, sample_metadata_dict):
        """Test that metadata can be dumped to JSON and loaded back identically."""
        import json

        meta_original = CapellaSLCMetadata.model_validate(sample_metadata_dict)

        json_str = meta_original.model_dump_json()
        meta_reloaded = CapellaSLCMetadata.model_validate_json(json_str)

        assert meta_original.model_dump() == meta_reloaded.model_dump()

        json_dict = json.loads(json_str)
        meta_from_dict = CapellaSLCMetadata.model_validate(json_dict)
        assert meta_original.model_dump() == meta_from_dict.model_dump()

    def test_round_trip_real_json_file(self):
        """Test round-trip with the actual extended JSON file."""
        import json
        from pathlib import Path

        json_file = (
            Path(__file__).parent
            / "data"
            / "CAPELLA_C13_SP_SLC_HH_20241126045307_20241126045346_extended.json"
        )

        with open(json_file) as f:
            original_str = f.read()
        original_dict = json.loads(original_str)

        meta = CapellaSLCMetadata.model_validate_json(original_str)

        dumped_dict = meta.model_dump(mode="json")

        keys_to_check = [
            "software_version",
            "product_type",
            "processing_deployment",
        ]
        for key in keys_to_check:
            assert dumped_dict[key] == original_dict[key]

        assert (
            dumped_dict["collect"]["platform"] == original_dict["collect"]["platform"]
        )
        assert dumped_dict["collect"]["mode"] == original_dict["collect"]["mode"]

    def test_load_all_metadata_files(self, metadata_file):
        """Test loading all metadata files without errors.

        This test is parameterized to run separately for each metadata file
        in the test data directory, ensuring they can all be successfully
        loaded and parsed without errors.
        """
        with open(metadata_file) as f:
            json_str = f.read()

        meta = CapellaSLCMetadata.model_validate_json(json_str)

        assert meta.software_version is not None
        assert meta.product_type == "SLC"
        assert meta.collect.platform.startswith("capella-")
        assert meta.collect.mode in ["spotlight", "stripmap"]


---
tests/test_orbit.py
---
"""Tests for orbit module."""

from __future__ import annotations

import numpy as np
import pytest
from pydantic import ValidationError

from capella_reader import Time
from capella_reader.geometry import AttitudeQuaternion, ECEFPosition, ECEFVelocity
from capella_reader.orbit import (
    Antenna,
    CoordinateSystem,
    PointingSample,
    State,
    StateVector,
    interpolate_orbit,
    is_uniformly_sampled,
    resample_orbit_data_linear,
)
from capella_reader.polynomials import Poly2D


class TestCoordinateSystem:
    """Tests for CoordinateSystem."""

    def test_creation_basic(self):
        """Test creating a basic CoordinateSystem."""
        cs = CoordinateSystem(type="ecef")

        assert cs.type == "ecef"
        assert cs.wkt is None

    def test_creation_with_wkt(self):
        """Test creating a CoordinateSystem with WKT."""
        wkt_str = 'GEOGCS["WGS 84",DATUM["WGS_1984"]]'
        cs = CoordinateSystem(type="geographic", wkt=wkt_str)

        assert cs.type == "geographic"
        assert cs.wkt == wkt_str

    def test_validation_requires_type(self):
        """Test that type field is required."""
        with pytest.raises(ValidationError):
            CoordinateSystem()


class TestStateVector:
    """Tests for StateVector."""

    def test_creation(self):
        """Test creating a StateVector."""
        t = Time("2024-01-01T12:00:00.000000000")
        pos = ECEFPosition(x=6378137.0, y=0.0, z=0.0)
        vel = ECEFVelocity(vx=0.0, vy=7500.0, vz=0.0)

        sv = StateVector(time=t, position=pos, velocity=vel)

        assert sv.time == t
        assert sv.position == pos
        assert sv.velocity == vel

    def test_creation_with_list_position(self):
        """Test creating StateVector with position as list."""
        sv = StateVector(
            time="2024-01-01T12:00:00.000000000",
            position=[6378137.0, 0.0, 0.0],
            velocity=ECEFVelocity(vx=0.0, vy=7500.0, vz=0.0),
        )

        assert sv.position.x == 6378137.0
        assert sv.position.y == 0.0
        assert sv.position.z == 0.0

    def test_creation_with_list_velocity(self):
        """Test creating StateVector with velocity as list."""
        sv = StateVector(
            time="2024-01-01T12:00:00.000000000",
            position=ECEFPosition(x=6378137.0, y=0.0, z=0.0),
            velocity=[0.0, 7500.0, 0.0],
        )

        assert sv.velocity.vx == 0.0
        assert sv.velocity.vy == 7500.0
        assert sv.velocity.vz == 0.0

    def test_creation_with_both_lists(self):
        """Test creating StateVector with both position and velocity as lists."""
        sv = StateVector(
            time="2024-01-01T12:00:00.000000000",
            position=[6378137.0, 0.0, 0.0],
            velocity=[0.0, 7500.0, 0.0],
        )

        assert sv.position.x == 6378137.0
        assert sv.velocity.vy == 7500.0


class TestState:
    """Tests for State."""

    def test_creation(self):
        """Test creating a State object."""
        cs = CoordinateSystem(type="ecef")
        sv1 = StateVector(
            time="2024-01-01T12:00:00.000000000",
            position=[6378137.0, 0.0, 0.0],
            velocity=[0.0, 7500.0, 0.0],
        )
        sv2 = StateVector(
            time="2024-01-01T12:00:10.000000000",
            position=[6378137.0, 75000.0, 0.0],
            velocity=[0.0, 7500.0, 0.0],
        )

        state = State(
            coordinate_system=cs,
            direction="ascending",
            state_vectors=[sv1, sv2],
            source="precise_determination",
        )

        assert state.coordinate_system == cs
        assert state.direction == "ascending"
        assert len(state.state_vectors) == 2
        assert state.source == "precise_determination"

    def test_get_state_outputs(self):
        """Test state arrays for both float and datetime time outputs."""
        cs = CoordinateSystem(type="ecef")
        sv1 = StateVector(
            time="2024-01-01T12:00:00.000000000",
            position=[6378137.0, 0.0, 0.0],
            velocity=[0.0, 7500.0, 0.0],
        )
        sv2 = StateVector(
            time="2024-01-01T12:00:10.000000000",
            position=[6378137.0, 75000.0, 0.0],
            velocity=[0.0, 7500.0, 0.0],
        )
        state = State(
            coordinate_system=cs,
            direction="ascending",
            state_vectors=[sv1, sv2],
            source="precise_determination",
        )

        times_float, positions, velocities = state.get_state(time_as_float=True)
        assert times_float.dtype.kind == "f"
        assert positions.shape == (2, 3)
        assert velocities.shape == (2, 3)

        times_dt, _positions, _velocities = state.get_state(time_as_float=False)
        assert np.issubdtype(times_dt.dtype, np.datetime64)


class TestPointingSample:
    """Tests for PointingSample."""

    def test_creation(self):
        """Test creating a PointingSample."""
        t = Time("2024-01-01T12:00:00.000000000")
        att = AttitudeQuaternion(q0=1.0, q1=0.0, q2=0.0, q3=0.0)

        ps = PointingSample(time=t, attitude=att)

        assert ps.time == t
        assert ps.attitude == att

    def test_creation_with_list_attitude(self):
        """Test creating PointingSample with attitude as list."""
        ps = PointingSample(
            time="2024-01-01T12:00:00.000000000",
            attitude=[1.0, 0.0, 0.0, 0.0],
        )

        assert ps.attitude.q0 == 1.0
        assert ps.attitude.q1 == 0.0
        assert ps.attitude.q2 == 0.0
        assert ps.attitude.q3 == 0.0


class TestAntenna:
    """Tests for Antenna."""

    def test_creation(self):
        """Test creating an Antenna."""
        beam_pattern = Poly2D(
            degree=(1, 1),
            coefficients=[[1.0, 0.0], [0.0, 0.0]],
        )

        ant = Antenna(
            azimuth_beamwidth=0.01,
            elevation_beamwidth=0.02,
            gain=30.0,
            beam_pattern=beam_pattern,
        )

        assert ant.azimuth_beamwidth == 0.01
        assert ant.elevation_beamwidth == 0.02
        assert ant.gain == 30.0
        assert ant.beam_pattern == beam_pattern


class TestIsUniformlySampled:
    """Tests for is_uniformly_sampled function."""

    def test_uniform_spacing(self):
        """Test that uniformly spaced state vectors return True."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:20.000000000"),
                position=[6378137.0, 150000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        assert is_uniformly_sampled(svs)

    def test_non_uniform_spacing(self):
        """Test that non-uniformly spaced state vectors return False."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:25.000000000"),
                position=[6378137.0, 187500.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        assert not is_uniformly_sampled(svs)

    def test_single_state_vector(self):
        """Test that a single state vector is not considered uniformly sampled."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        assert not is_uniformly_sampled(svs)


class TestResampleOrbitDataLinear:
    """Tests for resample_orbit_data_linear function."""

    def test_basic_resampling(self):
        """Test basic linear resampling."""
        t = [
            np.datetime64("2024-01-01T12:00:00", "ns"),
            np.datetime64("2024-01-01T12:00:20", "ns"),
            np.datetime64("2024-01-01T12:00:40", "ns"),
        ]
        p = np.array([[1000.0, 0.0, 0.0], [2000.0, 0.0, 0.0], [3000.0, 0.0, 0.0]])
        v = np.array([[100.0, 0.0, 0.0], [100.0, 0.0, 0.0], [100.0, 0.0, 0.0]])

        t_new, p_new, v_new = resample_orbit_data_linear(t, p, v, dt_seconds=10.0)

        assert len(t_new) == 5
        assert t_new[0] == t[0]
        assert t_new[-1] >= t[-1]

        np.testing.assert_array_almost_equal(p_new[0], p[0])
        np.testing.assert_array_almost_equal(v_new[0], v[0])

    def test_preserves_endpoints(self):
        """Test that linear resampling preserves start point."""
        t = [
            np.datetime64("2024-01-01T12:00:00", "ns"),
            np.datetime64("2024-01-01T12:00:10", "ns"),
        ]
        p = np.array([[1000.0, 2000.0, 3000.0], [1100.0, 2100.0, 3100.0]])
        v = np.array([[100.0, 200.0, 300.0], [100.0, 200.0, 300.0]])

        _t_new, p_new, v_new = resample_orbit_data_linear(t, p, v, dt_seconds=5.0)

        np.testing.assert_array_almost_equal(p_new[0], p[0])
        np.testing.assert_array_almost_equal(v_new[0], v[0])

    def test_uniform_spacing_at_nanosecond_level(self):
        """Test that resampled times are uniformly spaced at nanosecond precision.

        This test verifies the fix for a bug where floating point errors in
        nanosecond conversion caused non-uniform spacing. When using fractional
        seconds like 0.6, the old code would produce timestamps that varied by
        1-2 nanoseconds due to floating point precision issues.

        The fix converts dt_seconds to integer nanoseconds once, then uses
        integer arithmetic to avoid accumulation errors.
        """
        t = [
            np.datetime64("2024-01-01T12:00:00.000000000", "ns"),
            np.datetime64("2024-01-01T12:01:00.000000000", "ns"),
        ]
        p = np.array([[1000.0, 0.0, 0.0], [2000.0, 0.0, 0.0]])
        v = np.array([[100.0, 0.0, 0.0], [100.0, 0.0, 0.0]])

        # Use 0.6 seconds spacing - a case that triggered the bug
        t_new, _p_new, _v_new = resample_orbit_data_linear(t, p, v, dt_seconds=0.6)

        # Verify all time differences are exactly the same at nanosecond level
        time_diffs = np.diff(t_new)
        unique_diffs = np.unique(time_diffs)

        # Should have only ONE unique time difference (uniformly sampled)
        assert len(unique_diffs) == 1, (
            f"Expected uniform spacing, but got {len(unique_diffs)} different "
            f"intervals: {unique_diffs}"
        )

        # The difference should be exactly 600000000 nanoseconds (0.6 seconds)
        expected_diff = np.timedelta64(600000000, "ns")
        assert (
            unique_diffs[0] == expected_diff
        ), f"Expected {expected_diff} spacing, got {unique_diffs[0]}"


class TestInterpolateOrbit:
    """Tests for interpolate_orbit function."""

    def test_already_uniform(self):
        """Test that uniformly sampled orbits are returned unchanged."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[6378137.0, 0.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[6378137.0, 75000.0, 0.0],
                velocity=[0.0, 7500.0, 0.0],
            ),
        ]

        result = interpolate_orbit(svs)

        assert len(result) == len(svs)
        assert result[0].time == svs[0].time
        assert result[1].time == svs[1].time

    def test_linear_interpolation(self):
        """Test linear interpolation of non-uniform orbit."""
        svs = [
            StateVector(
                time=Time("2024-01-01T12:00:00.000000000"),
                position=[1000.0, 0.0, 0.0],
                velocity=[100.0, 0.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:10.000000000"),
                position=[2000.0, 0.0, 0.0],
                velocity=[100.0, 0.0, 0.0],
            ),
            StateVector(
                time=Time("2024-01-01T12:00:25.000000000"),
                position=[3500.0, 0.0, 0.0],
                velocity=[100.0, 0.0, 0.0],
            ),
        ]

        result = interpolate_orbit(svs)

        assert len(result) >= len(svs)
        assert is_uniformly_sampled(result)


---
tests/test_package.py
---
from __future__ import annotations

import capella_reader


def test_version():
    """Test that the package version is accessible."""
    assert hasattr(capella_reader, "__version__")


---
tests/test_polynomials.py
---
"""Tests for polynomial wrappers."""

import numpy as np
import pytest
from numpy.polynomial import Polynomial

from capella_reader.polynomials import Poly1D, Poly2D


class TestPoly1D:
    """Tests for Poly1D polynomial wrapper."""

    def test_creation_from_list(self):
        """Test creating a Poly1D from a list of coefficients."""
        coeffs = [1.0, 2.0, 3.0]
        poly = Poly1D(degree=2, coefficients=coeffs)

        assert poly.degree == 2
        np.testing.assert_array_equal(poly.coefficients, coeffs)

    def test_creation_from_array(self):
        """Test creating a Poly1D from a numpy array."""
        coeffs = np.array([1.0, 2.0, 3.0])
        poly = Poly1D(degree=2, coefficients=coeffs)

        np.testing.assert_array_equal(poly.coefficients, coeffs)

    def test_as_polynomial(self):
        """Test conversion to numpy.polynomial.Polynomial."""
        coeffs = [1.0, 2.0, 3.0]
        poly = Poly1D(degree=2, coefficients=coeffs)

        np_poly = poly.as_numpy_polynomial()
        assert isinstance(np_poly, Polynomial)
        np.testing.assert_array_equal(np_poly.coef, coeffs)

    def test_evaluation_scalar(self):
        """Test evaluating the polynomial at a scalar value."""
        coeffs = [1.0, 2.0, 3.0]  # 1 + 2x + 3x^2
        poly = Poly1D(degree=2, coefficients=coeffs)

        result = poly(2.0)
        expected = 1.0 + 2.0 * 2.0 + 3.0 * 2.0**2
        assert result == pytest.approx(expected)

    def test_evaluation_array(self):
        """Test evaluating the polynomial at array values."""
        coeffs = [1.0, 2.0, 3.0]
        poly = Poly1D(degree=2, coefficients=coeffs)

        x = np.array([0.0, 1.0, 2.0])
        result = poly(x)
        expected = np.array([1.0, 6.0, 17.0])
        np.testing.assert_array_almost_equal(result, expected)

    def test_invalid_coefficients_shape(self):
        """Test that tells 2D coefficients are rejected."""
        with pytest.raises(ValueError, match="coefficients must be 1D"):
            Poly1D(degree=1, coefficients=[[1.0, 2.0], [3.0, 4.0]])


class TestPoly2D:
    """Tests for Poly2D polynomial wrapper."""

    def test_creation(self):
        """Test creating a Poly2D from a 2D array."""
        coeffs = [[1.0, 2.0], [3.0, 4.0]]
        poly = Poly2D(degree=(1, 1), coefficients=coeffs)
        assert poly.degree == (1, 1)
        np.testing.assert_array_equal(poly.coefficients, coeffs)

        poly = Poly2D(degree=1, coefficients=coeffs)
        assert poly.degree == (1, 1)
        np.testing.assert_array_equal(poly.coefficients, coeffs)

    def test_evaluation_scalar(self):
        """Test evaluating 2D polynomial at scalar values."""
        coeffs = [[1.0, 2.0], [3.0, 4.0]]  # 1 + 2y + 3x + 4xy
        poly = Poly2D(degree=1, coefficients=coeffs)

        result = poly(2.0, 3.0)
        expected = 1.0 + 2.0 * 3.0 + 3.0 * 2.0 + 4.0 * 2.0 * 3.0
        assert result == pytest.approx(expected)

    def test_evaluation_array(self):
        """Test evaluating 2D polynomial at array values."""
        coeffs = [[1.0, 2.0], [3.0, 0.0]]  # 1 + 2y + 3x
        poly = Poly2D(degree=1, coefficients=coeffs)

        x = np.array([0.0, 1.0])
        y = np.array([0.0, 1.0])
        result = poly(x, y)
        expected = np.array([1.0, 6.0])
        np.testing.assert_array_almost_equal(result, expected)

    def test_higher_order(self):
        """Test higher-order polynomial evaluation."""
        coeffs = [[1.0, 0.0, 0.5], [0.0, 1.0, 0.0], [0.25, 0.0, 0.0]]
        poly = Poly2D(degree=2, coefficients=coeffs)

        result = poly(2.0, 2.0)
        expected = 1.0 + 0.5 * 2.0**2 + 1.0 * 2.0 * 2.0 + 0.25 * 2.0**2
        assert result == pytest.approx(expected)

    def test_from_fit(self):
        """Test fitting a 2D polynomial to grid data."""
        # Fit to a linear surface: p(x, y) = 1 + 2*x + 3*y
        x = np.array([0.0, 1.0, 2.0])
        y = np.array([0.0, 1.0, 2.0])
        data = np.array(
            [
                [1.0, 4.0, 7.0],  # x=0: 1+0+3*y
                [3.0, 6.0, 9.0],  # x=1: 1+2+3*y
                [5.0, 8.0, 11.0],  # x=2: 1+4+3*y
            ]
        )
        poly = Poly2D.from_fit(x, y, data, degree=1)

        # Expected coefficients: c[i,j] for x^i * y^j
        # p(x,y) = 1 + 2*x + 3*y -> c[0,0]=1, c[1,0]=2, c[0,1]=3, c[1,1]=0
        np.testing.assert_allclose(
            poly.coefficients, [[1.0, 3.0], [2.0, 0.0]], atol=1e-10
        )

        # Verify the fit by evaluating at grid points
        for i, xi in enumerate(x):
            for j, yj in enumerate(y):
                assert poly(xi, yj) == pytest.approx(data[i, j])

    def test_invalid_coefficients_shape(self):
        """Test that tells 1D coefficients are rejected."""
        with pytest.raises(ValueError, match="coefficients must be 2D"):
            Poly2D(degree=1, coefficients=[1.0, 2.0, 3.0])

    def test_from_fit_with_1d_samples_no_cross_terms(self):
        """Test 1D sample fitting without cross terms."""
        x = np.array([0.0, 1.0, 2.0, 3.0])
        y = np.array([1.0, 0.0, 1.0, 0.0])
        data = 1.0 + 2.0 * x + 3.0 * y

        poly = Poly2D.from_fit(x, y, data, degree=1, cross_terms=False, robust=False)

        np.testing.assert_allclose(poly.coefficients[0, 0], 1.0, atol=1e-10)
        np.testing.assert_allclose(poly.coefficients[1, 0], 2.0, atol=1e-10)
        np.testing.assert_allclose(poly.coefficients[0, 1], 3.0, atol=1e-10)
        np.testing.assert_allclose(poly.coefficients[1, 1], 0.0, atol=1e-10)


---
tests/test_radar.py
---
"""Tests for radar module."""

from __future__ import annotations

import pytest
from pydantic import ValidationError

from capella_reader import Time
from capella_reader.enums import LookSide
from capella_reader.radar import PRFEntry, Radar, RadarTimeVaryingParams


class TestRadarTimeVaryingParams:
    """Tests for RadarTimeVaryingParams."""

    def test_creation(self):
        """Test creating RadarTimeVaryingParams."""
        params = RadarTimeVaryingParams(
            start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
            prf=3000.0,
            pulse_bandwidth=300e6,
            pulse_duration=1e-6,
            rank=1,
        )

        assert len(params.start_timestamps) == 1
        assert params.prf == 3000.0
        assert params.pulse_bandwidth == 300e6
        assert params.pulse_duration == 1e-6
        assert params.rank == 1

    def test_multiple_timestamps(self):
        """Test creating RadarTimeVaryingParams with multiple timestamps."""
        params = RadarTimeVaryingParams(
            start_timestamps=[
                Time("2024-01-01T12:00:00.000000000"),
                Time("2024-01-01T12:00:10.000000000"),
            ],
            prf=3000.0,
            pulse_bandwidth=300e6,
            pulse_duration=1e-6,
            rank=1,
        )

        assert len(params.start_timestamps) == 2
        assert (
            params.start_timestamps[0].as_numpy()
            < params.start_timestamps[1].as_numpy()
        )

    def test_validation_requires_fields(self):
        """Test that required fields are validated."""
        with pytest.raises(ValidationError):
            RadarTimeVaryingParams(prf=3000.0)


class TestPRFEntry:
    """Tests for PRFEntry."""

    def test_creation(self):
        """Test creating a PRFEntry."""
        entry = PRFEntry(
            start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
            prf=3000.0,
        )

        assert len(entry.start_timestamps) == 1
        assert entry.prf == 3000.0

    def test_multiple_timestamps(self):
        """Test creating PRFEntry with multiple timestamps."""
        entry = PRFEntry(
            start_timestamps=[
                Time("2024-01-01T12:00:00.000000000"),
                Time("2024-01-01T12:00:10.000000000"),
                Time("2024-01-01T12:00:20.000000000"),
            ],
            prf=3000.0,
        )

        assert len(entry.start_timestamps) == 3


class TestRadar:
    """Tests for Radar."""

    def test_creation_basic(self):
        """Test creating a basic Radar configuration."""
        radar = Radar(
            rank=1,
            center_frequency=9.65e9,
            pointing=LookSide.RIGHT,
            sampling_frequency=600e6,
            transmit_polarization="H",
            receive_polarization="H",
            time_varying_parameters=[
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                    pulse_bandwidth=300e6,
                    pulse_duration=1e-6,
                    rank=1,
                )
            ],
            prf=[
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                )
            ],
        )

        assert radar.rank == 1
        assert radar.center_frequency == 9.65e9
        assert radar.pointing == LookSide.RIGHT
        assert radar.sampling_frequency == 600e6
        assert radar.transmit_polarization == "H"
        assert radar.receive_polarization == "H"
        assert len(radar.time_varying_parameters) == 1
        assert len(radar.prf) == 1

    def test_creation_vertical_polarization(self):
        """Test creating Radar with vertical polarization."""
        radar = Radar(
            rank=1,
            center_frequency=9.65e9,
            pointing=LookSide.LEFT,
            sampling_frequency=600e6,
            transmit_polarization="V",
            receive_polarization="V",
            time_varying_parameters=[
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                    pulse_bandwidth=300e6,
                    pulse_duration=1e-6,
                    rank=1,
                )
            ],
            prf=[
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                )
            ],
        )

        assert radar.transmit_polarization == "V"
        assert radar.receive_polarization == "V"
        assert radar.pointing == LookSide.LEFT

    def test_creation_cross_polarization(self):
        """Test creating Radar with cross-polarization."""
        radar = Radar(
            rank=1,
            center_frequency=9.65e9,
            pointing=LookSide.RIGHT,
            sampling_frequency=600e6,
            transmit_polarization="H",
            receive_polarization="V",
            time_varying_parameters=[
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                    pulse_bandwidth=300e6,
                    pulse_duration=1e-6,
                    rank=1,
                )
            ],
            prf=[
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                )
            ],
        )

        assert radar.transmit_polarization == "H"
        assert radar.receive_polarization == "V"

    def test_multiple_time_varying_params(self):
        """Test creating Radar with multiple time-varying parameter sets."""
        radar = Radar(
            rank=1,
            center_frequency=9.65e9,
            pointing=LookSide.RIGHT,
            sampling_frequency=600e6,
            transmit_polarization="H",
            receive_polarization="H",
            time_varying_parameters=[
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                    pulse_bandwidth=300e6,
                    pulse_duration=1e-6,
                    rank=1,
                ),
                RadarTimeVaryingParams(
                    start_timestamps=[Time("2024-01-01T12:00:10.000000000")],
                    prf=3500.0,
                    pulse_bandwidth=350e6,
                    pulse_duration=1.2e-6,
                    rank=2,
                ),
            ],
            prf=[
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:00.000000000")],
                    prf=3000.0,
                ),
                PRFEntry(
                    start_timestamps=[Time("2024-01-01T12:00:10.000000000")],
                    prf=3500.0,
                ),
            ],
        )

        assert len(radar.time_varying_parameters) == 2
        assert radar.time_varying_parameters[0].prf == 3000.0
        assert radar.time_varying_parameters[1].prf == 3500.0
        assert len(radar.prf) == 2

    def test_validation_invalid_polarization(self):
        """Test that invalid polarization values are rejected."""
        with pytest.raises(ValidationError):
            Radar(
                rank=1,
                center_frequency=9.65e9,
                pointing=LookSide.RIGHT,
                sampling_frequency=600e6,
                transmit_polarization="X",
                receive_polarization="H",
                time_varying_parameters=[],
                prf=[],
            )


---
tests/test_slc.py
---
"""Tests for CapellaSLC wrapper."""

import json
from pathlib import Path

import numpy as np
import pytest
import tifffile

from capella_reader.metadata import CapellaSLCMetadata
from capella_reader.slc import (
    C_LIGHT,
    CapellaImageGeometryError,
    CapellaParseError,
    CapellaSLC,
)


class TestCapellaSLC:
    """Tests for CapellaSLC wrapper."""

    def test_creation(self, sample_metadata_dict):
        """Test creating a CapellaSLC object."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)
        slc = CapellaSLC(path=Path("/fake/path.tif"), meta=meta)

        assert slc.path == Path("/fake/path.tif")
        assert isinstance(slc.meta, CapellaSLCMetadata)

    def test_shape_property(self, sample_metadata_dict):
        """Test the shape property."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)
        slc = CapellaSLC(path=Path("/fake/path.tif"), meta=meta)

        assert slc.shape == (1000, 1000)

    def test_dtype_property(self, sample_metadata_dict):
        """Test the dtype property."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)
        slc = CapellaSLC(path=Path("/fake/path.tif"), meta=meta)

        assert slc.dtype == np.dtype(np.complex64)

    def test_metadata_properties(self, sample_metadata_dict):
        """Test derived properties from metadata."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)
        slc = CapellaSLC(path=Path("/fake/path.tif"), meta=meta)

        assert slc.range_to_first_sample == 800000.0
        assert slc.delta_range_sample == 0.5
        assert str(slc.first_line_time) == "2024-07-09T04:03:29.000000000Z"
        assert str(slc.center_time) == "2024-07-09T04:03:43.000000000Z"
        assert slc.ref_epoch == slc.center_time
        assert slc.delta_line_time == 0.001
        assert slc.sensing_start == slc.first_line_time
        assert slc.starting_range == slc.range_to_first_sample

        assert slc.prf_average == 5000.0
        assert slc.center_frequency == 9.65e9
        assert slc.wavelength == pytest.approx(C_LIGHT / 9.65e9)
        assert slc.polarization == "HH"

        doppler_poly = slc.frequency_doppler_centroid_polynomial
        np.testing.assert_array_equal(
            doppler_poly.coefficients,
            [[0.0, 0.1], [0.2, 0.3]],
        )

    def test_from_file(self, tmp_path, sample_metadata_dict):
        """Test loading from a TIFF file."""
        # Create a test TIFF file with metadata
        test_file = tmp_path / "test.tif"
        test_data = np.random.randn(100, 100).astype(np.complex64)

        # Write TIFF with metadata in ImageDescription tag
        with tifffile.TiffWriter(test_file) as tif:
            tif.write(
                test_data,
                description=json.dumps(sample_metadata_dict),
            )

        slc = CapellaSLC.from_file(test_file)

        assert slc.path == test_file
        assert isinstance(slc.meta, CapellaSLCMetadata)
        assert slc.meta.software_version == sample_metadata_dict["software_version"]

    def test_from_file_json(self, tmp_path, sample_metadata_dict):
        """Test loading from a JSON metadata file."""
        test_file = tmp_path / "test.json"
        test_file.write_text(json.dumps(sample_metadata_dict))

        slc = CapellaSLC.from_file(test_file)

        assert slc.path == test_file
        assert slc.meta.software_version == sample_metadata_dict["software_version"]

    def test_from_real_metadata_files(self, metadata_file):
        slc = CapellaSLC.from_file(metadata_file)
        assert slc.path == metadata_file
        assert slc.delta_range_sample > 0.0
        if slc.meta.collect.image.is_slant_plane:
            # Does not exist in pfa
            assert slc.delta_line_time > 0.0
            assert slc.range_to_first_sample > 0.0
            assert slc.sensing_start == slc.first_line_time
            assert slc.starting_range == slc.range_to_first_sample

    def test_pfa_properties_raise(self, metadata_file):
        """Test slant-plane-only properties raise for PFA metadata."""
        slc = CapellaSLC.from_file(metadata_file)
        # Only test for pfa metadata files
        if not slc.meta.collect.image.is_pfa:
            return

        # These do not:
        with pytest.raises(CapellaImageGeometryError, match="slant_plane"):
            _ = slc.range_to_first_sample
        with pytest.raises(CapellaImageGeometryError, match="slant_plane"):
            _ = slc.first_line_time
        with pytest.raises(CapellaImageGeometryError, match="slant_plane"):
            _ = slc.delta_line_time

    def test_from_file_unsupported_extension(self, tmp_path, sample_metadata_dict):
        """Test unsupported file extension raises CapellaParseError."""
        test_file = tmp_path / "test.txt"
        test_file.write_text(json.dumps(sample_metadata_dict))

        with pytest.raises(CapellaParseError, match="Unsupported file type"):
            CapellaSLC.from_file(test_file)

    def test_gcps_and_bounds(self, tmp_path, sample_metadata_dict):
        """Test reading GCPs and computing bounds."""
        test_file = tmp_path / "test_gcps.tif"
        test_data = np.random.randn(10, 10).astype(np.complex64)
        gcp_values = np.array(
            [
                0.0,
                0.0,
                0.0,
                10.0,
                20.0,
                30.0,
                5.0,
                10.0,
                0.0,
                40.0,
                50.0,
                60.0,
            ],
            dtype=np.float64,
        )
        extratags = [(33922, "d", len(gcp_values), gcp_values, False)]

        with tifffile.TiffWriter(test_file) as tif:
            tif.write(
                test_data,
                description=json.dumps(sample_metadata_dict),
                extratags=extratags,
            )

        slc = CapellaSLC.from_file(test_file)
        gcps = slc.gcps

        assert len(gcps) == 2
        assert gcps[0].row == 0.0
        assert gcps[0].col == 0.0
        assert gcps[0].x == 10.0
        assert gcps[0].y == 20.0
        assert gcps[0].z == 30.0

        assert gcps[1].row == 5.0
        assert gcps[1].col == 10.0
        assert gcps[1].x == 40.0
        assert gcps[1].y == 50.0
        assert gcps[1].z == 60.0

        assert slc.bounds == (10.0, 20.0, 40.0, 50.0)

    def test_gcps_unavailable_for_json(self, sample_metadata_dict):
        """Test that JSON-backed SLCs do not expose GCPs."""
        meta = CapellaSLCMetadata.model_validate(sample_metadata_dict)
        slc = CapellaSLC(path=Path("/fake/path.json"), meta=meta)

        with pytest.raises(ValueError, match="No GCPs available"):
            _ = slc.gcps


---
tests/test_time.py
---
"""Tests for Time and TimeDelta classes."""

from __future__ import annotations

import numpy as np
import pytest
from pydantic import BaseModel, ValidationError

from capella_reader import Time, TimeDelta


class TestTime:
    """Test Time class."""

    def test_creation_from_string(self):
        """Test creating Time from ISO string."""
        t = Time("2024-01-01T12:00:00.123456789")
        assert str(t) == "2024-01-01T12:00:00.123456789Z"

    def test_creation_from_string_with_timezone(self):
        """Test creating Time from ISO string with Z suffix."""
        t = Time("2024-01-01T12:00:00.123456789Z")
        assert str(t) == "2024-01-01T12:00:00.123456789Z"

    def test_creation_from_datetime64(self):
        """Test creating Time from numpy datetime64."""
        dt = np.datetime64("2024-01-01T12:00:00.123456789", "ns")
        t = Time(dt)
        assert t.as_numpy() == dt

    def test_nanosecond_precision(self):
        """Test that nanosecond precision is preserved."""
        t = Time("2024-01-01T12:00:00.123456789")
        assert str(t) == "2024-01-01T12:00:00.123456789Z"
        assert str(t.as_numpy()) == "2024-01-01T12:00:00.123456789"

    def test_subtraction(self):
        """Test Time subtraction returns TimeDelta."""
        t1 = Time("2024-01-01T12:00:00.000000000")
        t2 = Time("2024-01-01T12:00:01.000000000")

        delta = t2 - t1
        assert isinstance(delta, TimeDelta)
        assert delta.total_seconds() == 1.0

    def test_subtraction_with_nanoseconds(self):
        """Test Time subtraction preserves nanoseconds."""
        t1 = Time("2024-01-01T12:00:00.000000000")
        t2 = Time("2024-01-01T12:00:00.123456789")

        delta = t2 - t1
        assert delta.total_seconds() == pytest.approx(0.123456789, rel=1e-9)

    def test_addition(self):
        """Test Time addition with timedelta64."""
        t = Time("2024-01-01T12:00:00.000000000")
        t2 = t + np.timedelta64(10, "s")

        assert str(t2) == "2024-01-01T12:00:10.000000000Z"

    def test_addition_with_nanoseconds(self):
        """Test Time addition preserves nanoseconds."""
        t = Time("2024-01-01T12:00:00.123456789")
        t2 = t + np.timedelta64(1_000_000_000, "ns")

        assert str(t2) == "2024-01-01T12:00:01.123456789Z"

    def test_repr(self):
        """Test Time repr."""
        t = Time("2024-01-01T12:00:00.123456789")
        assert repr(t) == "Time(2024-01-01T12:00:00.123456789)"

    def test_as_datetime(self):
        """Test converting to Python datetime."""
        import datetime

        t = Time("2024-01-01T12:00:00.000000000")
        dt = t.as_datetime()

        assert isinstance(dt, datetime.datetime)
        assert dt.year == 2024
        assert dt.month == 1
        assert dt.day == 1
        assert dt.hour == 12
        assert dt.minute == 0
        assert dt.second == 0
        assert dt.tzinfo == datetime.timezone.utc

    def test_as_datetime_preserves_microseconds(self):
        """Test that as_datetime preserves microsecond precision."""

        t = Time("2024-01-01T12:00:00.123456")
        dt = t.as_datetime()

        assert dt.microsecond == 123456

    def test_hash_and_comparison(self):
        """Test hashing and ordering comparisons."""
        t1 = Time("2024-01-01T12:00:00.000000000")
        t2 = Time("2024-01-01T12:00:01.000000000")
        t1_dup = Time("2024-01-01T12:00:00.000000000")

        assert t1 == t1_dup
        assert t1 < t2
        assert len({t1, t1_dup, t2}) == 2


class TestTimeDelta:
    """Test TimeDelta class."""

    def test_total_seconds_integer(self):
        """Test total_seconds with integer seconds."""
        td = TimeDelta(np.timedelta64(10, "s"))
        assert td.total_seconds() == 10.0

    def test_total_seconds_fractional(self):
        """Test total_seconds with fractional seconds."""
        td = TimeDelta(np.timedelta64(1_500_000_000, "ns"))
        assert td.total_seconds() == 1.5

    def test_total_seconds_nanosecond_precision(self):
        """Test total_seconds preserves nanosecond precision."""
        td = TimeDelta(np.timedelta64(123_456_789, "ns"))
        assert td.total_seconds() == pytest.approx(0.123456789, rel=1e-9)

    def test_as_numpy(self):
        """Test accessing underlying numpy timedelta64."""
        td = TimeDelta(np.timedelta64(10, "s"))
        assert isinstance(td.as_numpy(), np.timedelta64)

    def test_repr(self):
        """Test TimeDelta repr."""
        td = TimeDelta(np.timedelta64(1, "s"))
        assert "TimeDelta" in repr(td)
        assert "nanoseconds" in repr(td)


class TestPydanticIntegration:
    """Test Pydantic integration with Time."""

    def test_model_with_time_field(self):
        """Test creating a Pydantic model with Time field."""

        class TestModel(BaseModel):
            timestamp: Time

        m = TestModel(timestamp="2024-01-01T12:00:00.123456789")
        assert isinstance(m.timestamp, Time)
        assert str(m.timestamp) == "2024-01-01T12:00:00.123456789Z"

    def test_model_with_time_from_datetime64(self):
        """Test Pydantic model accepts numpy datetime64."""

        class TestModel(BaseModel):
            timestamp: Time

        dt = np.datetime64("2024-01-01T12:00:00.123456789", "ns")
        m = TestModel(timestamp=dt)
        assert isinstance(m.timestamp, Time)

    def test_model_json_serialization(self):
        """Test JSON serialization of Time in Pydantic model."""

        class TestModel(BaseModel):
            timestamp: Time

        m = TestModel(timestamp="2024-01-01T12:00:00.123456789")
        json_str = m.model_dump_json()

        assert "2024-01-01T12:00:00.123456789Z" in json_str

    def test_model_json_deserialization(self):
        """Test JSON deserialization of Time in Pydantic model."""

        class TestModel(BaseModel):
            timestamp: Time

        m1 = TestModel(timestamp="2024-01-01T12:00:00.123456789")
        json_str = m1.model_dump_json()

        m2 = TestModel.model_validate_json(json_str)
        assert isinstance(m2.timestamp, Time)
        assert str(m2.timestamp) == str(m1.timestamp)

    def test_model_round_trip(self):
        """Test round-trip serialization preserves nanoseconds."""

        class TestModel(BaseModel):
            timestamp: Time

        m1 = TestModel(timestamp="2024-01-01T12:00:00.123456789")
        json_str = m1.model_dump_json()
        m2 = TestModel.model_validate_json(json_str)

        assert str(m1.timestamp) == str(m2.timestamp)
        assert m1.model_dump() == m2.model_dump()

    def test_model_with_timezone_suffix(self):
        """Test Pydantic model handles timezone suffix."""

        class TestModel(BaseModel):
            timestamp: Time

        m = TestModel(timestamp="2024-01-01T12:00:00.123456789Z")
        assert str(m.timestamp) == "2024-01-01T12:00:00.123456789Z"

    def test_model_optional_time(self):
        """Test Pydantic model with optional Time field."""

        class TestModel(BaseModel):
            timestamp: Time | None = None

        m1 = TestModel()
        assert m1.timestamp is None

        m2 = TestModel(timestamp="2024-01-01T12:00:00.123456789")
        assert isinstance(m2.timestamp, Time)

    def test_model_time_list(self):
        """Test Pydantic model with list of Time objects."""

        class TestModel(BaseModel):
            timestamps: list[Time]

        m = TestModel(
            timestamps=[
                "2024-01-01T12:00:00.000000000",
                "2024-01-01T12:00:01.000000000",
                "2024-01-01T12:00:02.000000000",
            ]
        )

        assert len(m.timestamps) == 3
        assert all(isinstance(t, Time) for t in m.timestamps)

    def test_model_validation_error(self):
        """Test that invalid time strings raise validation errors."""

        class TestModel(BaseModel):
            timestamp: Time

        with pytest.raises(ValidationError):
            TestModel(timestamp="not-a-datetime")


class TestEdgeCases:
    """Test edge cases and special values."""

    def test_negative_timedelta(self):
        """Test negative time differences."""
        t1 = Time("2024-01-01T12:00:01.000000000")
        t2 = Time("2024-01-01T12:00:00.000000000")

        delta = t2 - t1
        assert delta.total_seconds() == -1.0

    def test_very_small_timedelta(self):
        """Test very small nanosecond differences."""
        t1 = Time("2024-01-01T12:00:00.000000000")
        t2 = Time("2024-01-01T12:00:00.000000001")

        delta = t2 - t1
        assert delta.total_seconds() == pytest.approx(1e-9, rel=1e-9)

    def test_large_timedelta(self):
        """Test large time differences."""
        t1 = Time("2024-01-01T00:00:00.000000000")
        t2 = Time("2024-01-02T00:00:00.000000000")

        delta = t2 - t1
        assert delta.total_seconds() == 86400.0  # 24 hours

    def test_microsecond_precision(self):
        """Test that microsecond precision is preserved."""
        t = Time("2024-01-01T12:00:00.123456")
        assert "123456" in str(t)

    def test_millisecond_precision(self):
        """Test that millisecond precision is preserved."""
        t = Time("2024-01-01T12:00:00.123")
        assert "123" in str(t)


---
tests/test_types.py
---
"""Tests for small utility modules."""

from __future__ import annotations

from types import EllipsisType
from typing import get_args

from capella_reader._types import Index


def test_index_type_alias():
    """Test Index type alias expands to the expected union."""
    args = get_args(Index)
    assert args[0] is int
    assert args[1] is slice
    assert args[2] is EllipsisType


---
